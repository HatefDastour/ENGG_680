{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5308af3",
   "metadata": {
    "id": "c5308af3"
   },
   "source": [
    "# Regression Trees\n",
    "\n",
    "Regression trees are a type of decision tree that is specifically designed for solving **regression problems**. Unlike classification trees, which predict discrete class labels, regression trees predict **continuous numerical values**. Regression trees are a powerful tool for modeling and predicting numerical outcomes, making them a valuable asset in various fields such as finance, economics, engineering, and any domain where the target variable is continuous.\n",
    "\n",
    "Here's how regression trees work [Breiman, 2017]:\n",
    "\n",
    "1. **Structure:** A regression tree has a tree-like structure, with a **root node**, **internal nodes**, **branches**, and **leaf nodes**. Each internal node represents a splitting decision based on a feature, and each leaf node contains a predicted numerical value.\n",
    "\n",
    "2. **Splitting Criteria:** The main goal of a regression tree is to minimize the **variance** of the target variable within each leaf node. The algorithm chooses the feature and the split point that result in the most significant reduction in variance after the split. This reduction in variance is often measured using **mean squared error (MSE)**, which calculates the average squared difference between the predicted values and the actual values within each leaf node.\n",
    "\n",
    "3. **Recursive Splitting:** The construction of a regression tree involves a recursive process. The algorithm starts at the root node, selects the best feature and split point to minimize variance, and divides the data into subsets based on the chosen split. The process continues for each subset, creating child nodes and further splitting the data until a **stopping criterion** is met (e.g., a maximum tree depth or a minimum number of samples in a leaf node).\n",
    "\n",
    "4. **Leaf Nodes and Predictions:** Once the tree construction process is complete, the leaf nodes contain the predicted numerical values. When you have a new data point, you traverse the tree from the root node to a specific leaf node based on the values of the input features. The predicted value in that leaf node becomes the model's prediction for the new data point.\n",
    "\n",
    "5. **Pruning:** Like other decision trees, regression trees can be prone to **overfitting**, where the model captures noise in the training data rather than the underlying patterns. Pruning is a technique used to prevent overfitting by removing or merging unnecessary branches from the tree. This leads to a more generalized and less complex model that performs better on new, unseen data.\n",
    "\n",
    "Regression trees are a valuable tool when you need to predict a continuous outcome based on a set of input features. However, it's essential to keep in mind that decision trees, including regression trees, can be sensitive to the specific training data and may not always generalize well to new data. Techniques like pruning and using **ensemble methods** (e.g., Random Forests) can help improve the predictive performance of regression trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11387ed",
   "metadata": {
    "id": "f11387ed"
   },
   "source": [
    "---\n",
    "\n",
    "**Regression Tree Algorithm**\n",
    "\n",
    "1. **Start**\n",
    "2. Define the problem as a **regression task**, where the goal is to predict a continuous numerical value based on a set of input features.\n",
    "3. Collect and preprocess the dataset, such as handling missing values, outliers, or scaling the features.\n",
    "4. **Construct a Regression Tree:**\n",
    "    - Define the **root node**, which contains the entire dataset.\n",
    "    - Choose a feature to split the data based on **variance reduction**, which is the difference between the variance of the parent node and the weighted average variance of the child nodes.\n",
    "    - Calculate **mean squared error (MSE)** for the split, which is the average squared difference between the predicted values and the actual values in each node.\n",
    "    - Check if the **stopping criterion** is met (e.g., max depth or min samples). This criterion helps to prevent overfitting by limiting the growth of the tree.\n",
    "        - If yes, create a **leaf node** with the predicted value, which is the mean of the actual values in that node.\n",
    "        - If no, create **child nodes** and repeat the process for each subset.\n",
    "5. **Pruning (Optional):**\n",
    "    - Evaluate the tree for overfitting, which means that the tree captures noise in the training data rather than the underlying patterns.\n",
    "    - Identify unnecessary branches for removal, which are the branches that do not improve the performance of the tree on a validation set.\n",
    "    - Remove or merge branches to create a **simpler model**, which is more generalizable and less complex.\n",
    "6. **End**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307bb5f",
   "metadata": {
    "id": "b307bb5f"
   },
   "source": [
    "## Example: Auto MPG dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa649b4",
   "metadata": {
    "id": "2fa649b4"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>: Recall Auto MPG dataset from from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eFkJKEvgq-X1",
   "metadata": {
    "id": "eFkJKEvgq-X1"
   },
   "outputs": [],
   "source": [
    "# Download the zip file using wget\n",
    "!wget -N \"http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\"\n",
    "\n",
    "# Unzip the downloaded zip file\n",
    "!unzip -o auto+mpg.zip auto-mpg.data\n",
    "\n",
    "# Remove the downloaded zip file after extraction\n",
    "!rm -r auto+mpg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e0dcd",
   "metadata": {
    "id": "826e0dcd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n",
    "\n",
    "# Define column names based on the dataset's description\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model_Year', 'Origin', 'Car_Name']\n",
    "\n",
    "# Read the dataset with column names, treating '?' as missing values, and remove rows with missing values\n",
    "auto_mpg_df = pd.read_csv('auto-mpg.data', names=column_names,\n",
    "                          na_values='?',\n",
    "                          delim_whitespace=True).dropna().reset_index(drop = True)\n",
    "\n",
    "# Remove the 'Car_Name' column from the DataFrame\n",
    "auto_mpg_df = auto_mpg_df.drop(columns=['Car_Name'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(auto_mpg_df[['MPG', 'Horsepower', 'Cylinders']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ece04ab",
   "metadata": {
    "id": "6ece04ab"
   },
   "source": [
    "### Modeling ln(MGP) as a function of Horsepower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af20fc",
   "metadata": {
    "id": "21af20fc"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Prepare the input features and target variable\n",
    "X = auto_mpg_df[['Horsepower']]\n",
    "y = np.log(auto_mpg_df.MPG.values)  # Take the natural logarithm of MPG\n",
    "\n",
    "# Create a Decision Tree Regressor with specific settings\n",
    "reg = DecisionTreeRegressor(criterion='squared_error',\n",
    "                                 splitter='best', max_leaf_nodes= 2)\n",
    "\n",
    "# Fit the Decision Tree Regressor to the data\n",
    "_ = reg.fit(X, y)\n",
    "\n",
    "# Create a figure and axis for plotting the decision tree\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 2.75))\n",
    "\n",
    "# Visualize the decision tree\n",
    "_ = tree.plot_tree(reg, ax=ax,\n",
    "                   impurity=True,         # Show impurity in nodes\n",
    "                   node_ids=True,         # Show node IDs\n",
    "                   filled=True,           # Fill nodes with colors\n",
    "                   feature_names=X.columns.tolist(),  # Names of the features\n",
    "                   proportion=True)       # Display class proportion\n",
    "\n",
    "# Adjust the layout for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46531adf",
   "metadata": {
    "id": "46531adf"
   },
   "source": [
    "***\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "- **square_error**: Square error is a criterion used to measure the quality of a split in a decision tree regressor. It is equivalent to the mean squared error (MSE), which is the average squared difference between the predicted and actual target values in each node. It also represents the variance reduction achieved by the split, which is the goal of feature selection. Square error minimizes the L2 loss by using the mean target value as the prediction in each terminal node.\n",
    "\n",
    "- **samples**: In a decision tree, \"samples\" refers to the number of training data points in a node. It indicates how many data points are involved in a split or a prediction at that node.\n",
    "\n",
    "- **value**: The \"value\" is the predicted output for a new data point that falls into a node. It depends on the regression criterion used to build the tree. For example, if the criterion is square error, the \"value\" is the mean target value of the samples in that node. If the criterion is absolute error, the \"value\" is the median target value of the samples in that node.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d4d45",
   "metadata": {
    "id": "549d4d45"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "print(tree.export_text(reg, feature_names = X.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cae94f",
   "metadata": {
    "id": "79cae94f"
   },
   "source": [
    "The split of the regression tree can be interpreted as follows:\n",
    "\n",
    "0. The tree is divided based on the feature \"Horsepower.\"\n",
    "\n",
    "1. If the \"Horsepower\" is less than or equal to 97.50, the predicted value is 3.32.\n",
    "\n",
    "1. If the \"Horsepower\" is greater than 97.50, the predicted value is 2.81.\n",
    "\n",
    "This tree represents a binary decision structure where the regression model predicts different values depending on whether the \"Horsepower\" is above or below the threshold of 97.50."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2862ab3",
   "metadata": {
    "id": "e2862ab3",
    "scrolled": true
   },
   "source": [
    "To get `squared_error` and `value` for `node#1`, we have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40afba",
   "metadata": {
    "id": "2d40afba"
   },
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "_horsepower = 97.5\n",
    "\n",
    "# Node #01: Predict and Evaluate Subset\n",
    "print('Node #1: Predict and Evaluate Subset')\n",
    "# Import metrics from scikit-learn\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict the target variable for data points where 'Cylinders' is less than 5.5\n",
    "y_hat = reg.predict(X.loc[X['Horsepower'] < _horsepower])\n",
    "\n",
    "# Get the indices of the data points where 'Cylinders' is less than 5.5\n",
    "ind = X.loc[X['Horsepower'] < _horsepower].index.to_numpy()\n",
    "\n",
    "# Calculate the mean squared error (MSE) for the predicted values\n",
    "mse = metrics.mean_squared_error(y_hat, y[ind])\n",
    "\n",
    "# Print the squared error (MSE) with three decimal places\n",
    "print(f'squared_error = {mse:.3f}')\n",
    "\n",
    "# Calculate the percentage of samples in this node compared to the entire dataset\n",
    "sample_percentage = (len(y_hat) / len(y) * 100)\n",
    "\n",
    "# Print the percentage of samples in this node\n",
    "print(f'samples = {sample_percentage:.1f}%')\n",
    "\n",
    "# Calculate and print the mean value of the predicted target variable in this node\n",
    "mean_value_node1 = y_hat.mean()\n",
    "print(f'value = {mean_value_node1:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c51ed",
   "metadata": {
    "id": "b43c51ed"
   },
   "source": [
    "To get `squared_error` and `value` for `node#2`, we have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656e7364",
   "metadata": {
    "id": "656e7364"
   },
   "outputs": [],
   "source": [
    "# Node #02: Predict and Evaluate Subset\n",
    "print('Node #2: Predict and Evaluate Subset')\n",
    "# Import metrics from scikit-learn\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict the target variable for data points where 'Cylinders' is less than 5.5\n",
    "y_hat = reg.predict(X.loc[X['Horsepower'] >= _horsepower])\n",
    "\n",
    "# Get the indices of the data points where 'Cylinders' is less than 5.5\n",
    "ind = X.loc[X['Horsepower'] >= _horsepower].index.to_numpy()\n",
    "\n",
    "# Calculate the mean squared error (MSE) for the predicted values\n",
    "mse = metrics.mean_squared_error(y_hat, y[ind])\n",
    "\n",
    "# Print the squared error (MSE) with three decimal places\n",
    "print(f'squared_error = {mse:.3f}')\n",
    "\n",
    "# Calculate the percentage of samples in this node compared to the entire dataset\n",
    "sample_percentage = (len(y_hat) / len(y) * 100)\n",
    "\n",
    "# Print the percentage of samples in this node\n",
    "print(f'samples = {sample_percentage:.1f}%')\n",
    "\n",
    "# Calculate and print the mean value of the predicted target variable in this node\n",
    "mean_value_node2 = y_hat.mean()\n",
    "print(f'value = {mean_value_node2:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9330cd5",
   "metadata": {
    "id": "a9330cd5"
   },
   "outputs": [],
   "source": [
    "# Create a figure and axis for the plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Define tick positions and limits for the plot\n",
    "xticks = [0, _horsepower, 230]\n",
    "yticks = [2, 3, 4]\n",
    "xlim = [-5, 245]\n",
    "ylim = [1.8, 4.2]\n",
    "\n",
    "# Create a scatter plot of data points\n",
    "_ = ax.scatter(X['Horsepower'], y, marker='o',\n",
    "               facecolor='DodgerBlue', edgecolor='Navy', alpha=0.3)\n",
    "\n",
    "# Set labels, ticks, and limits for the axes\n",
    "_ = ax.set(xlabel='Horsepower', ylabel='ln(MPG)',\n",
    "           xticks=xticks, yticks=yticks, xlim=xlim, ylim=ylim)\n",
    "\n",
    "# Add a vertical dashed line at x = 4.5\n",
    "_ = ax.vlines(xticks[1], ylim[0], ylim[1],\n",
    "              linestyles='dashed', linewidth=2, colors='Black')\n",
    "\n",
    "# Add horizontal dashed lines for mean values\n",
    "_ = ax.hlines(mean_value_node1, xlim[0], xticks[1],\n",
    "              linestyles='dashed', linewidth=4, colors='Red',\n",
    "              label = f'ln(MPG) = {mean_value_node1:.3f}')\n",
    "_ = ax.hlines(mean_value_node2, xticks[1], xlim[-1],\n",
    "              linestyles='dashed', linewidth=4, colors='Green',\n",
    "              label = f'ln(MPG) = {mean_value_node2:.3f}')\n",
    "\n",
    "# Annotations for the regions\n",
    "_ = ax.annotate(r'$\\mathbf{R_1}$', xy=(20, 3), fontsize=30)\n",
    "_ = ax.annotate(r'$\\mathbf{R_2}$', xy=(160, 3), fontsize=30)\n",
    "\n",
    "# Fill regions with different colors\n",
    "_ = ax.fill_between([xlim[0], xticks[1]], ylim[0], ylim[1],\n",
    "                    color='LimeGreen', alpha=0.1)\n",
    "_ = ax.fill_between([xticks[1], xlim[-1]], ylim[0], ylim[1],\n",
    "                    color='purple', alpha=0.1)\n",
    "\n",
    "# Add a grid to the plot\n",
    "_ = ax.grid(True)\n",
    "_ = ax.legend(fontsize = 12)\n",
    "# Ensure a tight layout for the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336ae58",
   "metadata": {
    "id": "0336ae58"
   },
   "source": [
    "The decision tree effectively divides the data into two distinct regions within the predictor space:\n",
    "\n",
    "1. **Region 1 - $Hoursepower \\leq 97.5$ ($R_1$):**\n",
    "   - Mathematically expressed as: $R_1 = \\{X~|~\\text{Hoursepower} \\leq 97.5\\}$\n",
    "\n",
    "2. **Region 2 - $Hoursepower > 97.5$ ($R_2$):**\n",
    "   - Mathematically expressed as: $R_2 = \\{X~|~\\text{Hoursepower} > 97.5\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4876729b",
   "metadata": {
    "id": "4876729b"
   },
   "source": [
    "The following figure was generated utilizing  [dtreeviz](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/dtr_fig01.png\" alt=\"picture\" width=\"500\">\n",
    "<br>\n",
    "<b>Figure</b>: Visual representation of the above Decision Tree Regressor.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32726f",
   "metadata": {
    "id": "ab32726f"
   },
   "source": [
    "### Modeling ln(MGP) as a function of Horsepower and Cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54dbe81",
   "metadata": {
    "id": "e54dbe81",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Prepare the input features and target variable\n",
    "X = auto_mpg_df[['Horsepower', 'Cylinders']]\n",
    "y = np.log(auto_mpg_df.MPG.values)  # Take the natural logarithm of MPG\n",
    "\n",
    "# Create a Decision Tree Regressor with specific settings\n",
    "reg = DecisionTreeRegressor(criterion='squared_error', splitter='best', max_leaf_nodes=3)\n",
    "\n",
    "# Fit the Decision Tree Regressor to the data\n",
    "_ = reg.fit(X, y)\n",
    "\n",
    "# Create a figure and axis for plotting the decision tree\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4.5))\n",
    "\n",
    "# Visualize the decision tree\n",
    "_ = tree.plot_tree(reg, ax=ax,\n",
    "                   impurity=True,         # Show impurity in nodes\n",
    "                   node_ids=True,         # Show node IDs\n",
    "                   filled=True,           # Fill nodes with colors\n",
    "                   feature_names=X.columns.tolist(),  # Names of the features\n",
    "                   proportion=True)       # Display class proportion\n",
    "\n",
    "# Adjust the layout for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236123d9",
   "metadata": {
    "id": "236123d9"
   },
   "outputs": [],
   "source": [
    "print(tree.export_text(reg, feature_names = X.columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab14128",
   "metadata": {
    "id": "2ab14128"
   },
   "source": [
    "The tree is organized as follows:\n",
    "\n",
    "0. The top-level split is based on the feature \"Cylinders\" with a threshold of 5.50. This means that the dataset is initially divided into two groups: one where the number of cylinders is less than or equal to 5.50 and the other where it is greater than 5.50.\n",
    "\n",
    "1. For the first group (Cylinders <= 5.50), the model predicts a value of 3.35. This value represents the predicted outcome for instances in this subset.\n",
    "\n",
    "1. For the second group (Cylinders > 5.50), further splitting is performed based on the feature \"Horsepower\" with a threshold of 139.50. This results in two additional subsets.\n",
    "\n",
    "1. In the subset where Horsepower is less than or equal to 139.50, the model predicts a value of 2.96.\n",
    "\n",
    "1. In the subset where Horsepower is greater than 139.50, the model predicts a value of 2.65.\n",
    "\n",
    "This decision tree regressor is a structured representation of a predictive model used for regression tasks. It divides the data into different subsets based on specific feature values to make predictions about a target variable, typically a numeric value. In this case, the target variable is not explicitly mentioned, but it represents a numeric value that we are trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec89c4",
   "metadata": {
    "id": "23ec89c4"
   },
   "source": [
    "For instance to get `squared_error` and `value` for `node#1`, we have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81380a3d",
   "metadata": {
    "id": "81380a3d"
   },
   "outputs": [],
   "source": [
    "# Define Thresholds\n",
    "_cylinder = 5.5\n",
    "_horsepower = 139.5\n",
    "\n",
    "# Node #01: Predict and Evaluate Subset\n",
    "\n",
    "# Import metrics from scikit-learn\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict the target variable for data points where 'Cylinders' is less than 5.5\n",
    "y_hat = reg.predict(X.loc[X['Cylinders'] < _cylinder])\n",
    "\n",
    "# Get the indices of the data points where 'Cylinders' is less than 5.5\n",
    "ind = X.loc[X['Cylinders'] < _cylinder].index.to_numpy()\n",
    "\n",
    "# Calculate the mean squared error (MSE) for the predicted values\n",
    "mse = metrics.mean_squared_error(y_hat, y[ind])\n",
    "\n",
    "# Print the squared error (MSE) with three decimal places\n",
    "print(f'squared_error = {mse:.3f}')\n",
    "\n",
    "# Calculate the percentage of samples in this node compared to the entire dataset\n",
    "sample_percentage = (len(y_hat) / len(y) * 100)\n",
    "\n",
    "# Print the percentage of samples in this node\n",
    "print(f'samples = {sample_percentage:.1f}%')\n",
    "\n",
    "# Calculate and print the mean value of the predicted target variable in this node\n",
    "mean_value = y_hat.mean()\n",
    "print(f'value = {mean_value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1b7c1",
   "metadata": {
    "id": "d5d1b7c1"
   },
   "source": [
    "Observe that,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caef41a",
   "metadata": {
    "id": "3caef41a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create a figure with three subplots\n",
    "fig, ax = plt.subplots(3, 1, figsize=(6.5, 8))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Top subplot: Histogram of the natural logarithm of MPG\n",
    "ax[0].hist(np.log(auto_mpg_df.MPG), label='ln(MPG)',\n",
    "           color ='Violet', ec = 'k', bins = 12)\n",
    "ax[0].set(xlabel=r'$\\ln$(MPG)', ylabel='Frequency', ylim=[0, 60])\n",
    "\n",
    "# Middle subplot: Cylinders Histogram\n",
    "ax[1].hist(auto_mpg_df.Cylinders, label='Cylinders',\n",
    "           color='RoyalBlue', ec = 'k')\n",
    "ax[1].set(xlabel='Cylinders', ylabel='Frequency', ylim=[0, 250])\n",
    "\n",
    "# Add a vertical line at x = 5.5 in the middle subplot\n",
    "ax[1].axvline(x=5.5, color='red', linestyle='--', lw=2,\n",
    "              label= f'Cylinders = {_cylinder}')\n",
    "\n",
    "# Bottom subplot: Horsepower Histogram\n",
    "ax[2].hist(auto_mpg_df.Horsepower, label='Horsepower',\n",
    "           color ='lightYellow', ec = 'k', bins = 12)\n",
    "ax[2].set(xlabel='Horsepower', ylabel='Frequency', ylim=[0, 100])\n",
    "\n",
    "# Add a vertical line at x = 139.5 in the bottom subplot\n",
    "ax[2].axvline(x=139.5, color='red', linestyle='--', lw=2,\n",
    "              label = f'Horsepower = {_horsepower}')\n",
    "\n",
    "# Add legends to the middle and bottom subplots\n",
    "for a in ax[1:]:\n",
    "    a.legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9683dcb5",
   "metadata": {
    "id": "9683dcb5"
   },
   "source": [
    "The decision tree effectively divides the data into three distinct regions within the predictor space:\n",
    "\n",
    "1. **Region 1 - Cylinders with 5 or Fewer Cylinders ($R_1$):**\n",
    "   - This region, denoted as $R_1$, comprises vehicles with four or fewer cylinders.\n",
    "   - Mathematically expressed as: $R_1 = \\{X~|~\\text{Cylinders} \\leq 5.5\\}$\n",
    "\n",
    "2. **Region 2 - Horsepower with Fewer Than 139.5 Horsepower ($R_2$):**\n",
    "   - The second region, labeled as $R_2$, includes vehicles with 6 or more cylinders and less than 139.5 horsepower.\n",
    "   - Mathematically expressed as: $R_2 = \\{X~|~\\text{Cylinders} > 5.5,~\\text{Horsepower} \\leq 139.5\\}$\n",
    "\n",
    "3. **Region 3 - Horsepower with At Least 139.5 Horsepower ($R_3$):**\n",
    "   - The third region, referred to as $R_3$, encompasses vehicles with five or more cylinders and at least 139.5 horsepower.\n",
    "   - Mathematically expressed as: $R_3 = \\{X~|~\\text{Cylinders} > 5.5,~\\text{Horsepower} > 139.5\\}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd3fe9",
   "metadata": {
    "id": "50cd3fe9"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5))\n",
    "\n",
    "# Define tick positions and limits for the plot\n",
    "xticks = [1, _cylinder, 8]\n",
    "yticks = [0, _horsepower, 230]\n",
    "xlim = [0.5, 8.5]\n",
    "ylim = [-5, 245]\n",
    "\n",
    "# Scatter plot of data points\n",
    "_ = ax.scatter(auto_mpg_df['Cylinders'], auto_mpg_df['Horsepower'],\n",
    "               marker='o', facecolor='Red', edgecolor='DarkRed', alpha=0.3)\n",
    "\n",
    "# Set labels, ticks, and limits for the axes\n",
    "_ = ax.set(xlabel='Cylinders', ylabel='Horsepower',\n",
    "           xticks=xticks, yticks=yticks,\n",
    "           xlim=xlim, ylim=ylim)\n",
    "\n",
    "# Vertical dashed line at x = 4.5\n",
    "_ = ax.vlines(xticks[1], ylim[0], ylim[1], linestyles='dashed', linewidth=2, colors='Black')\n",
    "\n",
    "# Horizontal dashed line at y = 139.5\n",
    "_ = ax.hlines(yticks[1], xticks[1], xlim[1], linestyles='dashed', linewidth=2, colors='Black')\n",
    "\n",
    "# Annotations for the regions\n",
    "_ = ax.annotate(r'$\\mathbf{R_1}$', xy=(2, 117.5), fontsize=30)\n",
    "_ = ax.annotate(r'$\\mathbf{R_2}$', xy=(6, 40), fontsize=30)\n",
    "_ = ax.annotate(r'$\\mathbf{R_3}$', xy=(6, 180), fontsize=30)\n",
    "\n",
    "# Fill regions with different colors\n",
    "_ = ax.fill_between([xlim[0], xticks[1]], ylim[0], ylim[1], color='LimeGreen', alpha=0.1)\n",
    "_ = ax.fill_between([xticks[1], xlim[1]], yticks[1], ylim[1], color='aqua', alpha=0.1)\n",
    "_ = ax.fill_between([xticks[1], xlim[1]], ylim[0], yticks[1], color='purple', alpha=0.1)\n",
    "\n",
    "# Add a grid to the plot\n",
    "_ = ax.grid(True)\n",
    "\n",
    "# Ensure a tight layout for the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad682c6",
   "metadata": {
    "id": "0ad682c6"
   },
   "source": [
    "So, to recap, in the context of decision trees:\n",
    "\n",
    "- The terminal nodes or leaves ($R_1$, $R_2$, $R_3$) correspond to the distinct regions where the observations ultimately fall.\n",
    "- Decision trees are typically drawn in a manner where these leaves appear at the bottom of the tree.\n",
    "- The points along the tree where the predictor space splits into segments are referred to as internal nodes.\n",
    "- The segments connecting these nodes are termed branches, akin to the branches of an actual tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03793e9",
   "metadata": {
    "id": "a03793e9"
   },
   "source": [
    "The following figure was generated utilizing  [dtreeviz](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/dtr_fig02.png\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual representation of the above Decision Tree Regressor.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9811d3fb",
   "metadata": {
    "id": "9811d3fb"
   },
   "source": [
    "## DecisionTreeRegressor algorithm in scikit-learn\n",
    "\n",
    "The DecisionTreeRegressor algorithm in scikit-learn is a machine learning technique used for **regression tasks**, where the goal is to predict a continuous target variable based on input features. The algorithm constructs a **decision tree** to partition the feature space into regions and make predictions within each region. The partitioning is done by recursively splitting the data into subsets that optimize a certain **criterion**, which measures the quality of the splits. Three common criteria used for regression in DecisionTreeRegressor are **Mean Squared Error (MSE)**, **Half Poisson Deviance**, and **Mean Absolute Error (MAE)**.\n",
    "\n",
    "Let's go through the mathematical explanations of these criteria [scikit-learn Developers, 2023]:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**:\n",
    "    The Mean Squared Error is a commonly used criterion for regression tasks. It aims to minimize the average squared difference between the predicted values ($\\hat{y}$) and the actual target values ($y$) within each node of the decision tree [scikit-learn Developers, 2023].\n",
    "    - $\\bar{y}_m$: The mean target value of the samples in node $m$.\n",
    "    - $n_m$: The number of samples in node $m$.\n",
    "    - $Q_m$: The set of samples in node $m$.\n",
    "\n",
    "    \\begin{align*}\n",
    "    \\bar{y}_m &= \\frac{1}{n_m} \\sum_{y \\in Q_m} y &&\\text{(Mean target value)} \\\\\n",
    "    H(Q_m) &= \\frac{1}{n_m} \\sum_{y \\in Q_m} (\\hat{y} - y)^2 && \\text{(MSE)}\n",
    "    \\end{align*}\n",
    "\n",
    "2. **Half Poisson Deviance**:\n",
    "    The Half Poisson Deviance criterion is suitable for situations where the target variable represents counts or frequencies. It aims to minimize a measure of the difference between the observed ($y$) and predicted ($\\hat{y}$) values, scaled by a logarithmic term [scikit-learn Developers, 2023].\n",
    "\n",
    "    \\begin{align*}\n",
    "    H(Q_m) &= \\frac{1}{n_m} \\sum_{y \\in Q_m} \\left( y \\log\\frac{y}{\\hat{y}_m} - y + \\hat{y}_m \\right) \\quad \\text{(Half Poisson Deviance)}\n",
    "    \\end{align*}\n",
    "\n",
    "3. **Mean Absolute Error (MAE)**:\n",
    "    The Mean Absolute Error criterion aims to minimize the average absolute difference between the predicted values ($\\hat{y}$) and the actual target values ($y$) within each node [scikit-learn Developers, 2023].\n",
    "\n",
    "    - $median(y)_m$: The median target value of the samples in node $m$.\n",
    "\n",
    "    \\begin{align*}\n",
    "    median(y)_m &= \\underset{y \\in Q_m}{\\mathrm{median}}(y) && \\text{(Median target value)} \\\\\n",
    "    H(Q_m) &= \\frac{1}{n_m} \\sum_{y \\in Q_m} |\\hat{y} - y| && \\text{(MAE)}\n",
    "    \\end{align*}\n",
    "\n",
    "The DecisionTreeRegressor algorithm selects the split that minimizes the chosen criterion at each node during the tree-building process. This recursive partitioning continues until a **stopping criterion** is met, such as reaching a predefined maximum depth or the number of samples in a node falling below a certain threshold.\n",
    "\n",
    "It's worth noting that while these criteria provide guidelines for constructing the decision tree, the actual implementation may involve additional optimizations and techniques to handle overfitting and improve predictive performance. For more details, you can refer to the [scikit-learn Developers, 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682a37c7",
   "metadata": {
    "id": "682a37c7"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font>\n",
    "In this example, a decision tree is employed to model and fit a cosine curve amidst noisy observations. The decision tree captures local linear relationships, effectively approximating the underlying cosine curve while considering the noisy nature of the data. This scenario showcases how decision trees can adapt to data patterns and perform regression tasks. The presented example is an adapted version of an example available in scikit-learn's documentation\n",
    "[scikit-learn Developers, 2023]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5665f2",
   "metadata": {
    "id": "aa5665f2"
   },
   "outputs": [],
   "source": [
    "# This is an modified version of an example by Sklearn:\n",
    "# https://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html\n",
    "\n",
    "# Import the necessary modules and libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a random dataset\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.sort(5 * rng.rand(80, 1), axis=0)\n",
    "y = np.cos(X).ravel()\n",
    "y[::5] += 3 * (0.5 - rng.rand(16))\n",
    "\n",
    "# Predict\n",
    "X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n",
    "\n",
    "# Create decision tree regressors with different max depths\n",
    "max_depths = [1, 2, 3, 4]\n",
    "colors = [\"DodgerBlue\", \"Violet\", \"OrangeRed\", \"Green\"]\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (depth, color) in enumerate(zip(max_depths, colors)):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(X, y, s= 30, edgecolor=\"black\", c=\"darkorange\", label=\"data\")\n",
    "\n",
    "    regr = DecisionTreeRegressor(max_depth=depth).fit(X, y)\n",
    "    y_pred = regr.predict(X_test)\n",
    "    ax.plot(X_test, y_pred, color=color, linewidth=2)\n",
    "\n",
    "    _ = ax.set_title(f\"max_depth = {depth}\", weight = 'bold')\n",
    "    _ = ax.set(xlabel = \"x\", ylabel = \"y\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0942b24e",
   "metadata": {
    "id": "0942b24e"
   },
   "source": [
    "In this context, the `max_depths` list specifies different values for the `max_depth` parameter of the DecisionTreeRegressor. Each value determines the maximum depth to which the decision tree can grow during its construction.\n",
    "\n",
    "1. **max_depth = 1**:\n",
    "   With a maximum depth of 1, the decision tree will only have a single split. This means that the tree will create one node that best splits the data into two regions based on some feature threshold. The decision boundary will be a horizontal or vertical line, which will not capture the complexity of the underlying cosine curve. This model will likely underfit the data.\n",
    "\n",
    "2. **max_depth = 2**:\n",
    "   Increasing the maximum depth to 2 allows the decision tree to make one additional split. The tree will now have a root node and two child nodes. This might allow the decision tree to capture some basic curvature of the cosine curve, but it's still limited in terms of complexity.\n",
    "\n",
    "3. **max_depth = 3**:\n",
    "   With a maximum depth of 3, the decision tree can create three levels of nodes. This extra depth allows the tree to capture more of the curvature in the cosine curve, but it might still miss some finer details. The model might start to capture the initial rise and the subsequent fall of the cosine curve.\n",
    "\n",
    "4. **max_depth = 4**:\n",
    "   Increasing the maximum depth to 4 adds another level of complexity to the decision tree. The tree can now create four levels of nodes. This could allow the tree to better approximate the shape of the cosine curve, capturing more of its oscillations and details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ffea1",
   "metadata": {
    "id": "9c6ffea1"
   },
   "source": [
    "<!-- gridspec_kw={'height_ratios': [1, 2, 3]} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67ab762",
   "metadata": {
    "id": "e67ab762",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def _DTR_TreePlot(max_depth=1, figsize=(6.5, 3.5)):\n",
    "    \"\"\"\n",
    "    Generates a decision tree plot based on the specified parameters.\n",
    "\n",
    "    Parameters:\n",
    "    - max_depth (int): Maximum depth of the decision tree.\n",
    "    - figsize (tuple): Figure size (width, height).\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "    regr = DecisionTreeRegressor(max_depth=max_depth).fit(X, y)\n",
    "    _ = tree.plot_tree(regr, ax=ax,\n",
    "                       filled=True,\n",
    "                       impurity=True,\n",
    "                       node_ids=True,\n",
    "                       feature_names = ['x'],\n",
    "                       proportion=True,\n",
    "                       fontsize=12)\n",
    "    _ = ax.set_title(f\"Decision Tree with max_depth = {max_depth}\", weight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    print(tree.export_text(regr, feature_names = ['x']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db90e7d",
   "metadata": {
    "id": "5db90e7d"
   },
   "source": [
    "**max_depth = 1**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0a782",
   "metadata": {
    "id": "70b0a782"
   },
   "outputs": [],
   "source": [
    "_DTR_TreePlot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d17c58a",
   "metadata": {
    "id": "5d17c58a"
   },
   "source": [
    "The following figure was generated utilizing  [dtreeviz](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/dtr_fig03.png\" alt=\"picture\" width=\"450\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual representation of the above Decision Tree Regressor.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc15284",
   "metadata": {
    "id": "7fc15284"
   },
   "source": [
    "Here is how to read the tree:\n",
    "\n",
    "- **node #0** shows the root node, which is the starting point of the tree. It splits the data based on the value of x. If x is less than or equal to 1.36, then the data goes to the left branch. If x is greater than 1.36, then the data goes to the right branch.\n",
    "- **node #1** shows the left terminal node, which is the end point of the tree. It has a predicted value of 0.74, which means that the average value of the target variable for the data that reaches this node is 0.74.\n",
    "- **node #2** shows the right terminal node, which has a predicted value of -0.57, which means that the average value of the target variable for the data that reaches this node is -0.57."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b25a1",
   "metadata": {
    "id": "d56b25a1"
   },
   "source": [
    "**max_depth = 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a16529",
   "metadata": {
    "id": "11a16529"
   },
   "outputs": [],
   "source": [
    "_DTR_TreePlot(max_depth = 2, figsize = (9.5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b392d5c",
   "metadata": {
    "id": "3b392d5c"
   },
   "source": [
    "The following figure was generated utilizing  [dtreeviz](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/dtr_fig04.png\" alt=\"picture\" width=\"900\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual representation of the above Decision Tree Regressor.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57e7e1",
   "metadata": {
    "id": "1c57e7e1"
   },
   "source": [
    "Here is how to read the tree:\n",
    "\n",
    "- **node #0** shows the root node, which is the starting point of the tree. It splits the data based on the value of x. If x is less than or equal to 1.36, then the data goes to the left branch. If x is greater than 1.36, then the data goes to the right branch.\n",
    "- **node #1** shows the left internal node, which is a node that splits the data further based on the value of x. If x is less than or equal to 0.05, then the data goes to the left branch. If x is greater than 0.05, then the data goes to the right branch.\n",
    "- **node #2** shows the left terminal node, which has a predicted value of -0.15, which means that the average value of the target variable for the data that reaches this node is -0.15.\n",
    "- **node #3** shows the right terminal node, which has a predicted value of 0.78, which means that the average value of the target variable for the data that reaches this node is 0.78.\n",
    "- **node #4** shows the right internal node, which is a node that splits the data further based on the value of x. If x is less than or equal to 4.28, then the data goes to the left branch. If x is greater than 4.28, then the data goes to the right branch.\n",
    "- **node #5** shows the left terminal node, which has a predicted value of -0.71, which means that the average value of the target variable for the data that reaches this node is -0.71.\n",
    "- **node #6** shows the right terminal node, which has a predicted value of -0.01, which means that the average value of the target variable for the data that reaches this node is -0.01."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
