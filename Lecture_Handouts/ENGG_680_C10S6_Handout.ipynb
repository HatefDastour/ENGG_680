{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2bf48f",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "Random forests offer an enhancement to bagged trees through a simple adjustment that introduces tree decorrelation. Like in bagging, we construct multiple decision trees on bootstrapped training samples. However, when constructing these trees, at each split point, only a random subset of m predictors is considered as potential candidates for the split, drawn from the full set of p predictors [James et al., 2023].\n",
    "\n",
    "The math behind Random Forests involves a few key concepts that contribute to its effectiveness in enhancing decision trees. Let's break down the main components [Hastie et al., 2013, James et al., 2023]:\n",
    "\n",
    "1. **Bootstrap Sampling:** In Random Forests, multiple decision trees are created, each based on a different subset of the training data. These subsets are obtained through a process called bootstrap sampling. Given a dataset with $n$ observations, bootstrap sampling involves randomly selecting 'n' observations with replacement. This means that some observations may be included multiple times in the subset, while others may not be included at all. This process generates diverse training subsets for building different trees.\n",
    "\n",
    "2. **Random Feature Selection:** At each split point of a decision tree within a Random Forest, instead of considering all available features (predictors), a random subset of features is selected as candidates for the split. This introduces randomness and diversity among the trees. The number of features in the subset, denoted as $m$, is typically smaller than the total number of predictors $p$. This process helps decorrelate the trees, reducing the chance of them making similar errors and leading to more accurate predictions.\n",
    "\n",
    "3. **Voting or Averaging:** Once all the trees are constructed, their predictions are combined to make a final prediction. For regression tasks, the predictions from individual trees are usually averaged to obtain the final prediction. For classification tasks, a majority vote among the predictions is often taken to determine the class label. This ensemble approach helps improve the overall accuracy and stability of the model.\n",
    "\n",
    "Mathematically, the process of Random Forests involves creating $B$ decision trees, each constructed using a different bootstrap sample and a random subset of $m$ features at each split point. The final prediction for a new observation is obtained by averaging (for regression) or majority voting (for classification) the predictions from all the trees:\n",
    "\n",
    "For regression:\n",
    "\\begin{equation}\n",
    "\\hat{f}_{rf}(x) = \\frac{1}{B}\\sum_{b = 1}^{B} \\hat{f}^{b}(x)\n",
    "\\end{equation}\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/RFR_Fig.jpg\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual of Random Forests Algorithm for regression.\n",
    "</center>\n",
    "\n",
    "For classification:\n",
    "\\begin{equation}\n",
    "\\hat{C}_{rf}(x) = \\text{majority vote}\\left(\\hat{C}^{1}(x), \\hat{C}^{2}(x), \\ldots, \\hat{C}^{B}(x)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/RFC_Fig.jpg\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual of Random Forests Algorithm for classification.\n",
    "</center>\n",
    "\n",
    "Here, $\\hat{f}^{b}(x)$ represents the prediction of the 'b'-th tree for observation 'x', and $\\hat{C}^{b}(x)$ represents the class predicted by the 'b'-th tree for observation 'x'.\n",
    "\n",
    "The random forest algorithm's combination of bootstrap sampling and random feature selection helps create a diverse ensemble of trees that work together to provide more accurate and stable predictions, reducing the likelihood of overfitting and improving the model's generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea188739",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Random Forest algorithm**\n",
    "\n",
    "---\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - $N$ = Number of samples in the dataset\n",
    "   - $M$ = Number of features in each sample\n",
    "   - $x_i$ = Input features for the $i$-th sample\n",
    "   - $y_i$ = Output label for regression task (real value)\n",
    "   - $C_i$ = Output class for classification task (categorical value)\n",
    "\n",
    "2. **Bootstrapping:**\n",
    "   - Randomly select $N$ samples with replacement to create multiple bags (bootstrap samples).\n",
    "   - In scikit-learn's API, the parameter controlling this is `bootstrap=True`.\n",
    "\n",
    "3. **Growing Individual Trees:**\n",
    "   - Each individual tree $t$ is trained on one of the bootstrap samples.\n",
    "   - At each node of tree $t$, consider a random subset of features of size $m$ for splitting.\n",
    "   - Stop growing the tree based on stopping criteria like maximum depth or minimum samples per leaf.\n",
    "   - In scikit-learn, you can control maximum depth with `max_depth` and minimum samples per leaf with `min_samples_leaf`.\n",
    "\n",
    "4. **Voting or Averaging:**\n",
    "   - For classification: Let $k$ be the number of classes. Each tree predicts a class $C_i$ for input $x_i$. The final prediction is the majority class among all trees' predictions.\n",
    "   - For regression: Each tree predicts a value $y_i$ for input $x_i$. The final prediction is the average of all trees' predictions.\n",
    "   - In scikit-learn, you can set `n_estimators` to determine the number of trees.\n",
    "\n",
    "5. **Out-of-Bag (OOB) Error:**\n",
    "   - For each sample $i$, if it's not in the training set of tree $t$, we can use its prediction to calculate the OOB error.\n",
    "   - In scikit-learn, OOB error can be calculated by setting `oob_score=True`.\n",
    "\n",
    "6. **Randomness and Diversity:**\n",
    "   - For feature subset selection, $m$ is typically set to $\\sqrt{M}$ for classification and $\\frac{M}{3}$ for regression.\n",
    "   - This randomness encourages different trees to focus on different subsets of features, leading to diversity.\n",
    "\n",
    "7. **Hyperparameters:**\n",
    "   - `n_estimators`: Number of trees in the forest.\n",
    "   - `max_depth`: Maximum depth of each tree.\n",
    "   - `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "   - `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
    "   - `max_features`: Number of features to consider for the best split at each node.\n",
    "   - `bootstrap`: Whether bootstrap samples should be used.\n",
    "   - `oob_score`: Whether to calculate out-of-bag score.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5513ff",
   "metadata": {},
   "source": [
    "The Random Forests algorithm combines the predictions from multiple decision trees, each constructed on a different bootstrap sample and a subset of features. The diversity introduced by these mechanisms helps to reduce overfitting and improve the generalization performance of the ensemble model. Additionally, Random Forests provide insights into feature importance, which can be used for feature selection and understanding the underlying relationships in the data [Breiman, 2001, James et al., 2023].\n",
    "\n",
    "Keep in mind that the algorithm can be further customized and optimized with various hyperparameters and techniques, such as adjusting the number of trees, tuning the size of the feature subset, and handling missing values and categorical variables. Implementation details may vary depending on the programming language or library you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b576a",
   "metadata": {},
   "source": [
    "Here's how these concepts relate to scikit-learn's API:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Creating a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2,\n",
    "                             min_samples_leaf=1, max_features=\"auto\", bootstrap=True,\n",
    "                             oob_score=True)\n",
    "\n",
    "# Creating a Random Forest Regressor\n",
    "reg = RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_split=2,\n",
    "                            min_samples_leaf=1, max_features=\"auto\", bootstrap=True,\n",
    "                            oob_score=True)\n",
    "```\n",
    "\n",
    "You can replace the hyperparameter values above with your desired settings. The scikit-learn API makes it convenient to configure the Random Forest algorithm for your specific task and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b6abe4",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "In the context of a random forest model, the `feature_importances_` attribute serves as an essential metric for gauging the significance of individual features in facilitating accurate predictions. This attribute offers valuable insights into the influential role that each feature plays in shaping the model's predictions [Pedregosa et al., 2011, scikit-learn Developers, 2023].\n",
    "\n",
    "### Calculation of `feature_importances_`:\n",
    "\n",
    "The determination of feature importance in a random forest involves assessing how much each feature contributes to the reduction of impurity, commonly measured using metrics such as Gini impurity or Mean Squared Error, within the individual decision trees constituting the forest. The process unfolds as follows [Pedregosa et al., 2011, scikit-learn Developers, 2023]:\n",
    "\n",
    "1. **Tree Level Calculation:** Within each decision tree of the random forest, candidate features for splitting are identified based on the impurity reduction each feature would bring if chosen as the split feature. Metrics like Gini impurity or Mean Squared Error are frequently employed for this purpose.\n",
    "\n",
    "2. **Feature Contribution:** For each candidate feature in each tree, the algorithm quantifies how much the feature diminishes impurity in the data. Greater reduction implies a higher importance for that specific tree.\n",
    "\n",
    "3. **Averaging Across Trees:** After constructing all individual trees, the importance of each feature is averaged across the entire forest. This results in an importance score for each feature, indicating its collective contribution to the model's predictions.\n",
    "\n",
    "4. **Normalization:** Importance scores are typically normalized to sum up to 1 or 100. This normalization aids in interpreting the relative importance of each feature.\n",
    "\n",
    "5. **Interpretation:** A higher importance score denotes that a feature exerts a more substantial influence on the model's predictions. Conversely, features with lower importance scores contribute less to the model's predictive capabilities.\n",
    "\n",
    "### Interpretation of `feature_importances_`:\n",
    "\n",
    "The values within the `feature_importances_` array sum up to 0 or 1, contingent on normalization. These values are relative and offer insights into which features wield a more pronounced impact on the model's predictions. Higher importance values signify a more significant contribution to the model's ability to make accurate predictions.\n",
    "\n",
    "By scrutinizing `feature_importances_`, one can pinpoint key features steering the model's performance, concentrate on pertinent variables, and potentially engage in feature selection to enhance model efficiency.\n",
    "\n",
    "In essence, `feature_importances_` in a random forest model quantifies the contribution of each feature to the reduction of impurity across individual trees, providing a valuable tool for comprehending feature relevance and model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d7110",
   "metadata": {},
   "source": [
    "##  Random Forest Regressor\n",
    "\n",
    "<font color='Blue'><b>Example</b></font>. Recall the Auto MPG dataset retrieved from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a217f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n",
    "\n",
    "# Define column names based on the dataset's description\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model_Year', 'Origin', 'Car_Name']\n",
    "\n",
    "# Read the dataset with column names, treating '?' as missing values, and remove rows with missing values\n",
    "auto_mpg_df = pd.read_csv('auto-mpg.data', names=column_names,\n",
    "                          na_values='?', delim_whitespace=True).dropna().reset_index(drop = True)\n",
    "\n",
    "# Remove the 'Car_Name' column from the DataFrame\n",
    "auto_mpg_df = auto_mpg_df.drop(columns=['Car_Name'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(auto_mpg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Extract the features (X) and target variable (y)\n",
    "X = auto_mpg_df.drop(columns=['MPG'])\n",
    "y = np.log(auto_mpg_df.MPG.values)  # Take the natural logarithm of the MPG values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "set_size_df = pd.DataFrame({'Size': [len(X_train), len(X_test)]}, index = ['Train', 'Test'])\n",
    "display(set_size_df.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7142af4",
   "metadata": {},
   "source": [
    "Our goal is to create a Random Forest Regression model with certain specifications. In this setup, the model consists of four decision trees, each constrained to a maximum of three leaf nodes. The intention behind these parameter choices is to build an ensemble of decision trees that work together to make accurate regression predictions. The restriction on the number of nodes in each tree serves to manage the overall complexity of the model. The next step involves training the model using the provided training data, where `X_train` represents the features, and `y_train` represents the corresponding target values. Throughout this training process, the model evaluates the importance of each feature, contributing to a comprehensive understanding of its predictive capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b40bdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "def print_bold(txt, c=31):\n",
    "    \"\"\"\n",
    "    Function to print text in bold with specified color.\n",
    "\n",
    "    Parameters:\n",
    "    - txt (str): Text to be printed.\n",
    "    - c (int): Color code for the printed text.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    print(f\"\\033[1;{c}m\" + txt + \"\\033[0m\")\n",
    "\n",
    "# Instantiate RandomForestRegressor with specified parameters\n",
    "rfr = RandomForestRegressor(n_estimators=4, random_state=0, max_leaf_nodes=3)\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Create subplots for each estimator\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9.5, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Initialize DataFrame for feature importance\n",
    "feat_importance_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over estimators to plot trees and calculate MSE\n",
    "for i, (estimator, ax) in enumerate(zip(rfr.estimators_, ax), start=1):\n",
    "    tree.plot_tree(estimator, ax=ax, feature_names=X.columns.tolist(), filled=True)\n",
    "    ax.set_title(f'Estimator {i}', fontsize=14, weight='bold')\n",
    "    \n",
    "    # Calculate MSE for both training and test sets\n",
    "    mse_train = metrics.mean_squared_error(y_train, estimator.predict(X_train.values))\n",
    "    mse_test = metrics.mean_squared_error(y_test, estimator.predict(X_test.values))\n",
    "    txt = f'MSE (Train) = {mse_train:.3f}\\nMSE (Test) = {mse_test:.3f}'\n",
    "    \n",
    "    # Display MSE values on each subplot\n",
    "    text = ax.text(0.68, -0.1, txt,\n",
    "                  transform=ax.transAxes, fontsize=11, weight='bold',\n",
    "                  bbox=dict(facecolor='#dfc8f0', alpha=0.7))\n",
    "    \n",
    "    # Create DataFrame with feature importances for each estimator\n",
    "    df_temp = pd.DataFrame({f'Estimator {i}': 100*estimator.feature_importances_}, index=X.columns)\n",
    "    feat_importance_df = pd.concat([feat_importance_df, df_temp], axis=1)\n",
    "\n",
    "# Ensure tight layout for subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Apply background gradient to the DataFrame and round importance values to 2 decimal places\n",
    "styled_importance = feat_importance_df.style.\\\n",
    "                    background_gradient(cmap='Reds', axis=1, vmin=0, vmax=100).format(precision=2)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "print_bold('Feature Importance:')\n",
    "display(styled_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc40c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = metrics.mean_squared_error(y_train, rfr.predict(X_train))\n",
    "mse_test = metrics.mean_squared_error(y_test, rfr.predict(X_test))\n",
    "txt = f'MSE (Train) = {mse_train:.3f}\\nMSE (Test) = {mse_test:.3f}'\n",
    "print(txt)\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "Importance = pd.DataFrame({'Importance': 100*rfr.feature_importances_}, index = X.columns)\n",
    "\n",
    "# Apply a background gradient to the DataFrame and round importance values to 2 decimal places\n",
    "styled_importance = Importance.style.background_gradient(cmap='Oranges',\n",
    "                                                         subset=['Importance']).format({'Importance': '{:.2f}'})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_importance)\n",
    "\n",
    "# Create a bar plot to visualize feature importances\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "bars = ax.bar(Importance.index, Importance.Importance,\n",
    "              color='#f9cb9c', edgecolor='#cc0000', hatch=\"\\\\\\\\\", lw=2, zorder = 2)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Variable Importance', fontsize=12, weight='bold', color='#191970')\n",
    "ax.set_ylabel('Importance', fontsize=12, weight='bold', color='#191970')\n",
    "ax.set_title('Feature Importance in Random Forest Model', fontsize=16, weight='bold', color='#2F4F4F')\n",
    "\n",
    "# Set y-axis limits and adjust tick parameters\n",
    "ax.set_ylim([0, 80])\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=12, color='#696969')\n",
    "ax.tick_params(axis='y', labelsize=12, color='#696969')\n",
    "\n",
    "# Customize plot aesthetics\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.spines[['bottom', 'left']].set_color('#696969')\n",
    "ax.grid(axis='x')\n",
    "\n",
    "# Ensure a tight layout for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db2ae18",
   "metadata": {},
   "source": [
    "The final accuracy of the random forest model is typically better than the average of the individual decision trees because the random forest model reduces overfitting and variance. Each decision tree in a random forest is trained on a random subset of the training data and a random subset of the features. This helps to reduce overfitting and improve the generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5d113d",
   "metadata": {},
   "source": [
    "In our approach, we implement a comparative analysis using a loop to iterate over specific values of `n_estimators`, namely 3 and 10. Within each iteration, we create a Random Forest Regressor to represent models with different numbers of decision trees. By setting `random_state` to 0, we ensure reproducibility. This methodical process enables us to systematically evaluate and compare the performance of the Random Forest Regression model, specifically examining the influence of varying ensemble sizes on predictive outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfb32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9.5, 9.5))\n",
    "\n",
    "# Loop through different feature sets\n",
    "for i, n_estimators in enumerate([3, 10]):\n",
    "    # Create a Random Forest Regressor\n",
    "    reg = RandomForestRegressor(n_estimators=n_estimators, random_state=0)\n",
    "    reg.fit(X_train, y_train)\n",
    "    \n",
    "    # Train set\n",
    "    y_pred_train = reg.predict(X_train)\n",
    "    ax[i, 0].scatter(y_pred_train, y_train,\n",
    "                     facecolors='#a7e0f7', edgecolors='#191970', alpha=0.8)\n",
    "    ax[i, 0].plot([0, 1], [0, 1], '--k', lw=2, transform=ax[i, 0].transAxes)\n",
    "    mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "    txt_train = f'MSE (Train) = {mse_train:.3f}'\n",
    "    text_train = ax[i, 0].text(0.62, 0.05, txt_train,\n",
    "                               transform=ax[i, 0].transAxes, fontsize=12, weight='bold',\n",
    "                               bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "    ax[i, 0].set(xlabel='Predicted Values', ylabel='Actual Values')\n",
    "    ax[i, 0].set_title(f'n_estimators = {n_estimators} (Train)', fontsize=14, weight='bold')\n",
    "    ax[i, 0].axis('equal')\n",
    "    \n",
    "    # Test set\n",
    "    y_pred_test = reg.predict(X_test)\n",
    "    ax[i, 1].scatter(y_pred_test, y_test,\n",
    "                     facecolors='#9ac989', edgecolors='#217304', alpha=0.8)\n",
    "    ax[i, 1].plot([0, 1], [0, 1], '--k', lw=2, transform=ax[i, 1].transAxes)\n",
    "    mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "    txt_test = f'MSE (Test) = {mse_test:.3f}'\n",
    "    text_test = ax[i, 1].text(0.62, 0.05, txt_test,\n",
    "                              transform=ax[i, 1].transAxes, fontsize=12, weight='bold',\n",
    "                              bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "    ax[i, 1].set(xlabel='Predicted Values', ylabel='Actual Values')\n",
    "    ax[i, 1].set_title(f'n_estimators = {n_estimators} (Test)', fontsize=14, weight='bold')\n",
    "    ax[i, 1].axis('equal')\n",
    "\n",
    "    # Print MSE values\n",
    "    txt = f'MSE (Train) = {mse_train:.3f}, MSE (Test) = {mse_test:.3f}'\n",
    "    print_bold(f'n_estimators = {n_estimators}:')\n",
    "    print(f'\\t{txt}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844265d0",
   "metadata": {},
   "source": [
    "For each subplot, a diagonal dashed line (–) serves as a reference, indicating where predicted values align with actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008832c",
   "metadata": {},
   "source": [
    "The experiment compares the influence of the number of estimators, as denoted by `n_estimators`, on the mean squared error (MSE). Two configurations are assessed:\n",
    "\n",
    "1. **n_estimators = 3**: MSE (Train) = 0.003, MSE (Test) = 0.018\n",
    "\n",
    "2. **n_estimators = 10**: MSE (Train) = 0.002, MSE (Test) = 0.014\n",
    "\n",
    "This experiment examines the performance of the model under varying numbers of decision tree estimators. The MSE serves as the evaluation metric, reflecting the average squared difference between observed and predicted values.\n",
    "\n",
    "The observed trend suggests that increasing the number of estimators from 3 to 10 results in a reduction in MSE, indicative of improved predictive accuracy. This pattern aligns with the common understanding that an ensemble of more diverse and robust decision trees, as achieved with a greater number of estimators, often leads to enhanced model performance by mitigating overfitting and capturing more nuanced patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97c4c9",
   "metadata": {},
   "source": [
    "In our analysis, we delve into the feature importance aspect of a Random Forest Regressor instantiated with specific parameters: `n_estimators=100` and `random_state=0`. Understanding feature importance is pivotal for discerning the significant contributors to the model's predictive power. In the context of a Random Forest, feature importance is computed by assessing how much each feature reduces impurity across all decision trees in the ensemble. The higher the reduction in impurity, the more crucial the feature is deemed. Normalization ensures that the feature importance values sum up to 1, offering a relative measure of each feature's impact on the model's overall predictive accuracy. This exploration aids us in pinpointing and prioritizing the features that play a pivotal role in achieving accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instantiate a RandomForestRegressor with 100 estimators and a random state of 0\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the RandomForestRegressor model on the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = reg.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "Importance = pd.DataFrame({'Importance': reg.feature_importances_ * 100}, index=X.columns)\n",
    "\n",
    "# Apply a background gradient to the DataFrame and round importance values to 3 decimal places\n",
    "styled_importance = Importance.style.background_gradient(cmap='PuBu', subset=['Importance']).format({'Importance': '{:.3f}'})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_importance)\n",
    "print('\\n')\n",
    "\n",
    "# Create a bar plot to visualize feature importances\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "bars = ax.bar(Importance.index, Importance.Importance, color='#e7d2f3', edgecolor='#611589', hatch=\"///\", lw=2)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Variable Importance', fontsize=12, weight='bold', color='#191970')\n",
    "ax.set_ylabel('Importance', fontsize=12, weight='bold', color='#191970')\n",
    "ax.set_title('Feature Importance in Random Forest Model', fontsize=16, weight='bold', color='#2F4F4F')\n",
    "\n",
    "# Set y-axis limits and adjust tick parameters\n",
    "ax.set_ylim([0, 40])\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=12, color='#696969')\n",
    "ax.tick_params(axis='y', labelsize=12, color='#696969')\n",
    "\n",
    "# Customize plot aesthetics\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.spines[['bottom', 'left']].set_color('#696969')\n",
    "ax.grid(axis='x')\n",
    "\n",
    "# Ensure a tight layout for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6c0b2",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Features such as Displacement and Weight have relatively high importance scores (28.724 and 28.257, respectively), indicating their significant influence on predicting the natural logarithm of MPG.\n",
    "- Cylinders and Horsepower also carry substantial importance (19.76 and 13.538, respectively).\n",
    "- Acceleration has a comparatively lower importance (2.141).\n",
    "- Model_Year contributes with a moderate importance (7.354).\n",
    "- Origin has the lowest importance among the features (0.225).\n",
    "\n",
    "These importance values suggest that, in the context of the model used, Displacement, Weight, and Cylinders play pivotal roles in predicting the natural logarithm of MPG, while other features contribute to varying degrees. It's essential to note that feature importance is model-specific and doesn't imply causation. It reflects the contribution of each feature to the model's predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aae6c8",
   "metadata": {},
   "source": [
    "##  Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69779bac",
   "metadata": {},
   "source": [
    "<font color='Blue'><b>Example</b></font>:  In this example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7cba96",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "try:\n",
    "    import sklearnex\n",
    "except ImportError:\n",
    "    !pip install pip install scikit-learn-intelex\n",
    "    import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "# Patch sklearn for compatibility\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "# Define colors and colormap for the plot\n",
    "colors = [\"#f44336\", \"#2986cc\", \"#065535\", '#ffe599']\n",
    "\n",
    "# Define a list of color names for the colormap\n",
    "_cmap = ListedColormap(colors)\n",
    "\n",
    "# Generate synthetic data using make_blobs\n",
    "n_samples = 2000\n",
    "n_features = 2\n",
    "centers = 4\n",
    "cluster_std = 1.0\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features,\n",
    "                  centers=centers, random_state=0, cluster_std=cluster_std)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "def _plot_set(reg, ax, X, y):\n",
    "    # Plot decision boundaries\n",
    "    DecisionBoundaryDisplay.from_estimator(reg, X, cmap=_cmap, ax=ax,\n",
    "                                           response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
    "                                           shading=\"auto\",\n",
    "                                           alpha=0.3)\n",
    "    # Scatter plot for data points\n",
    "    for num in np.unique(y):\n",
    "        ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                   s=40, edgecolors='k', marker='o', label=str(num), zorder=2)\n",
    "    # Display F1-Score on the plot\n",
    "    f1_score = metrics.f1_score(reg.predict(X), y, average='weighted')\n",
    "    txt = f'F1-Score (Train) = {f1_score:.3f}'\n",
    "    ax.text(0.04, 0.04, txt, transform=ax.transAxes, fontsize=12, weight='bold',\n",
    "            bbox=dict(facecolor='Whitesmoke', alpha=0.7))\n",
    "    ax.legend(fontsize=12)\n",
    "    return f1_score\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9.5, 9.5))\n",
    "\n",
    "for i, n_estimators in enumerate([3, 10]):\n",
    "    # Create a RandomForestClassifier with specified max_depth\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, random_state=0)\n",
    "    \n",
    "    # Fit the classifier to the data\n",
    "    rfc.fit(X_train, y_train)\n",
    "    f1_train = _plot_set(rfc, ax[i, 0], X_train, y_train)\n",
    "    ax[i, 0].set_title(f'n_estimators = {n_estimators} (Train)', fontsize=14, weight='bold')\n",
    "    f1_test = _plot_set(rfc, ax[i, 1], X_test, y_test)\n",
    "    ax[i, 1].set_title(f'n_estimators = {n_estimators} (Test)', fontsize=14, weight='bold')\n",
    "    \n",
    "    # Print F1 values\n",
    "    txt = f'F1-Score (Train) = {f1_train:.3f}, F1-Score (Test) = {f1_test:.3f}'\n",
    "    print_bold(f'n_estimators = {n_estimators}:')\n",
    "    print(f'\\t{txt}')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Unpatch sklearn for original behavior\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf068b",
   "metadata": {},
   "source": [
    "The `n_estimators` parameter in a Random Forest classifier represents the number of trees in the forest. Increasing the number of estimators can have several effects on the model's performance, particularly in terms of training and testing F1-scores:\n",
    "\n",
    "1. **n_estimators = 3:**\n",
    "   - **Training F1-Score (Train) = 0.980:** This indicates how well the model is performing on the training data. A F1-score of 0.980 suggests high precision and recall on the training set, meaning the model is effectively capturing patterns in the data.\n",
    "   - **Testing F1-Score (Test) = 0.924:** The F1-score on the test set is slightly lower than the training F1-score. This difference could be due to the model not generalizing as well to unseen data. With only three estimators, the model might be underfitting and not capturing the full complexity of the data.\n",
    "\n",
    "2. **n_estimators = 10:**\n",
    "   - **Training F1-Score (Train) = 0.992:** The higher F1-score on the training set suggests that increasing the number of estimators has allowed the model to better fit the training data. The model is capturing more intricate patterns present in the data.\n",
    "   - **Testing F1-Score (Test) = 0.940:** The higher F1-score on the test set compared to the model with three estimators indicates that the model with ten estimators generalizes better to new, unseen data. It has a good balance between precision and recall, suggesting improved performance.\n",
    "\n",
    "Increasing the number of estimators in a Random Forest generally leads to a more expressive and powerful model. However, it's important to monitor the model's performance on both the training and test sets to avoid overfitting. If the number of estimators is too high, the model might start memorizing the training data, leading to decreased generalization on unseen data. The optimal number of estimators depends on the complexity of the data and the trade-off between bias and variance. The observed improvement in F1-scores from 3 to 10 suggests that a higher number of estimators is beneficial in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a664a1",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "\n",
    "\"Scikit-learn extensions\" or \"sklearnex\" refers to additional modules or libraries that build upon the scikit-learn library, which is a machine learning library for Python. These extensions typically provide extra functionality, new algorithms, or improved features to enhance the capabilities of scikit-learn in various ways. The term \"sklearnex\" may encompass a range of third-party contributions aimed at extending and complementing the existing scikit-learn ecosystem.\n",
    "\n",
    "The function `sklearnex.patch_sklearn()` is a method within the scikit-learn extensions framework. Its primary purpose is to patch or modify the behavior of the scikit-learn library by incorporating additional functionalities or improvements provided by sklearnex.\n",
    "\n",
    "This function is typically employed to seamlessly integrate the extensions into the scikit-learn library, ensuring that the enhanced features or modifications become part of the standard scikit-learn functionality. By invoking `sklearnex.patch_sklearn()`, users can apply the necessary adjustments to the scikit-learn library, enabling the utilization of extended capabilities offered by the sklearnex framework within their machine learning workflows. Additional information can be found by referring to the documentation available [here](https://github.com/intel/scikit-learn-intelex).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from pprint import pprint\n",
    "try:\n",
    "    import sklearnex\n",
    "except ImportError:\n",
    "    !pip install pip install scikit-learn-intelex\n",
    "    import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "# Patch sklearn for compatibility\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "# Create a RandomForestClassifier with default parameters\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Define the hyperparameter search space using param_dist\n",
    "param_dist = dict(max_depth=[3, 5, None],\n",
    "                  max_leaf_nodes=[10, None],\n",
    "                  min_samples_split=[2, 3, 10]\n",
    "                  )\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "scoring = metrics.make_scorer(metrics.f1_score, average='weighted')\n",
    "\n",
    "# Initialize HalvingRandomSearchCV with the estimator and parameter distributions\n",
    "rsh = HalvingRandomSearchCV(estimator=rfc,\n",
    "                            param_distributions=param_dist,\n",
    "                            resource='n_estimators',\n",
    "                            max_resources=10,\n",
    "                            scoring = scoring,\n",
    "                            factor=2,\n",
    "                            random_state=0)\n",
    "\n",
    "# Fit the search object to your data\n",
    "_ = rsh.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters found by the search\n",
    "best_params_ = rsh.best_params_\n",
    "pprint(best_params_)\n",
    "\n",
    "# Unpatch sklearn for original behavior\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c002d",
   "metadata": {},
   "source": [
    "The core of this example is the utilization of the `HalvingRandomSearchCV` technique, which efficiently narrows down the hyperparameter search space. The technique gradually discards suboptimal combinations, ultimately converging on the best configuration. By fitting the search object to a dataset (`X` and `y`), the code extracts and prints the best hyperparameters found by the search process. This example provides a valuable insight into how `HalvingRandomSearchCV` can significantly speed up the search process while identifying hyperparameters that lead to improved model performance. It's a demonstration of harnessing cutting-edge techniques to fine-tune machine learning models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39020d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "try:\n",
    "    import sklearnex\n",
    "except ImportError:\n",
    "    !pip install pip install scikit-learn-intelex\n",
    "    import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "def print_bold(txt, c = 31):\n",
    "    print(f\"\\033[1;{c}m\" + txt + \"\\033[0m\")\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "rfc = RandomForestClassifier(random_state = 0, **best_params_)\n",
    "\n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n",
    "# The splitt would be 80-20!\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "train_class_proportions, test_class_proportions = [], []\n",
    "   \n",
    "# Perform Cross-Validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    rfc.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate class proportions for train and test sets\n",
    "    train_class_proportions.append([np.mean(y_train == rfc) for rfc in np.unique(y)])\n",
    "    test_class_proportions.append([np.mean(y_test == rfc) for rfc in np.unique(y)])\n",
    "    \n",
    "    # train\n",
    "    y_train_pred = rfc.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "    \n",
    "    # test\n",
    "    y_test_pred = rfc.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
    "    print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdb720f",
   "metadata": {},
   "source": [
    "Here are some observations and recommendations based on the output:\n",
    "\n",
    "1. **Model Performance:**\n",
    "   - Our model demonstrates strong learning from the training data, evident in high accuracy and F1 scores.\n",
    "   - The generalization to unseen data is good, with slightly lower but still robust testing accuracy and F1 scores.\n",
    "\n",
    "2. **Consistency Across Folds:**\n",
    "   - Our model maintains consistent performance across different subsets of the data, as indicated by low variability in accuracy and F1 scores across folds.\n",
    "\n",
    "3. **Optimal Hyperparameter Settings:**\n",
    "   - The hyperparameters we selected through the HalvingRandomSearchCV process (such as `max_depth`, `max_leaf_nodes`, `min_samples_split`) show promise, providing effective results. However, fine-tuning may further enhance performance.\n",
    "\n",
    "4. **Addressing Overfitting:**\n",
    "   - The model excels on the training set, suggesting some potential overfitting. To mitigate this, we might explore additional hyperparameter tuning or regularization techniques.\n",
    "\n",
    "5. **Balanced Class Proportions:**\n",
    "   - We've maintained balanced class proportions in both training and testing sets, promoting fair training without favoring any specific class.\n",
    "\n",
    "6. **Future Steps:**\n",
    "   - Further exploration of hyperparameter values or additional parameters could optimize our model.\n",
    "\n",
    "In summary, our model exhibits strong performance, and refining hyperparameters could lead to even better results. Consistent evaluation and potential adjustments will ensure our model's effectiveness in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
