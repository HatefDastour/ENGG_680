{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40275b08",
      "metadata": {
        "id": "40275b08"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "## Why Not Linear Regression?\n",
        "\n",
        "Linear regression is a fundamental statistical technique that is widely used for modeling the relationship between a dependent variable and one or more independent variables. It has its strengths and weaknesses, and there are situations where it may not be the best choice. Here are some reasons why linear regression might not be suitable [James et al., 2023]:\n",
        "\n",
        "1. **Nonlinear Relationships:** Linear regression assumes a linear relationship between the independent and dependent variables. If the true relationship is nonlinear, using linear regression can lead to poor model fit and inaccurate predictions.\n",
        "\n",
        "2. **Complex Interactions:** Linear regression is limited in handling complex interactions between variables. If the relationship involves intricate interactions, nonlinear effects, or higher-order terms, linear regression may not capture these nuances effectively.\n",
        "\n",
        "3. **Assumption Violations:** Linear regression assumes that the residuals (the differences between observed and predicted values) are normally distributed and have constant variance. If these assumptions are violated, the results and inferences from linear regression may be unreliable.\n",
        "\n",
        "4. **Outliers and Influential Points:** Linear regression is sensitive to outliers and influential data points. A single outlier can significantly affect the regression line and the estimated coefficients.\n",
        "\n",
        "5. **Categorical and Binary Variables:** While linear regression can handle continuous and numeric independent variables, it may struggle with categorical or binary predictors without appropriate encoding or treatment.\n",
        "\n",
        "6. **Multi-Collinearity:** When the independent variables in a linear regression model are highly correlated with each other, it can lead to multicollinearity issues. This makes it challenging to interpret the individual contributions of these variables.\n",
        "\n",
        "7. **Limited in Handling Non-Parametric Data:** Linear regression is a parametric method, meaning it makes assumptions about the underlying data distribution. If the data is non-parametric or has heavy-tailed distributions, linear regression may not be appropriate.\n",
        "\n",
        "In cases where these limitations are significant, other techniques such as nonlinear regression, generalized linear models, decision trees, support vector machines, neural networks, or other advanced statistical and machine learning methods may be more suitable. The choice of the appropriate modeling technique depends on the nature of the data, the research question, and the specific goals of the analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3346280e",
      "metadata": {
        "id": "3346280e"
      },
      "source": [
        "## Logistic Regression: Predicting Binary Outcomes\n",
        "\n",
        "\"Logistic regression is a powerful statistical method tailored specifically for binary classification tasks. Its primary objective is to predict binary outcomes, such as yes/no or 0/1, by leveraging the impact of one or more predictor variables. This approach is effectively represented by the logistic regression model [James et al., 2023].\"\n",
        "\n",
        "### Logistic Model\n",
        "\n",
        "At the core of logistic regression is its ingenious model, which seamlessly integrates predictor variables within a linear framework, further molded by a transformative logistic function. This fusion aims to predict the probability of the positive class (class 1) in a binary response scenario. The model is encapsulated by the following equation [James et al., 2023]:\n",
        "\n",
        "\\begin{align}\n",
        "\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p,\n",
        "\\end{align}\n",
        "\n",
        "In this equation:\n",
        "- $X = (X_1, ..., X_p)$ constitutes the vector of predictor variables (features).\n",
        "- $\\beta_0, \\beta_1, ..., \\beta_p$ take on the roles of coefficients, encompassing both the intercept and each predictor variable's contribution.\n",
        "- $p(X)$ designates the probability of the positive class, informed by the predictor variables.\n",
        "\n",
        "Notably, the ratio $p(X)/[1 - p(X)]$, dubbed **the odds**, encapsulates the relationship between the probability of positive and negative outcomes [James et al., 2023]."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bd9f792",
      "metadata": {
        "id": "6bd9f792"
      },
      "source": [
        "### Estimating Probabilities\n",
        "\n",
        "The logistic model allows us to deduce the estimated probability of the positive class ($p(X)$) through the following equation:\n",
        "\n",
        "\\begin{align}\n",
        "p(X) = \\frac{e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}}{1 + e^{\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p}},\n",
        "\\end{align}\n",
        "\n",
        "In this equation, the base $e$ of the natural logarithm (approximately 2.71828) plays a pivotal role in the transformation. This transformation, facilitated by the logistic function, molds the linear combination of predictor variables into a probability value that spans the entire spectrum between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bcd028",
      "metadata": {
        "id": "70bcd028"
      },
      "source": [
        "### Sigmoid (Logistic) Function\n",
        "\n",
        "The equation given:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log\\left(\\frac{p(X)}{1 - p(X)}\\right) = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p\n",
        "\\end{equation}\n",
        "\n",
        "is closely connected to the sigmoid (logistic) function within the context of logistic regression, a method used for binary classification.\n",
        "\n",
        "In logistic regression, the objective is to estimate the likelihood of the positive outcome (class 1) based on the values of certain predictor variables, such as $X_1, X_2, \\ldots, X_p$. The left side of the equation, $\\log\\left(\\frac{p(X)}{1 - p(X)}\\right)$, signifies the log-odds (logit) of the probability $p(X)$ for the positive outcome. This transformation linearizes the connection between the predictor variables and the probability.\n",
        "\n",
        "Now, let's introduce the sigmoid function. The [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), denoted as $f(x) = \\frac{1}{1 + e^{-x}}$, converts a real-number $x$ into a probability ranging from 0 to 1. In the logistic regression equation, this can be related to $p(X$ as follows:\n",
        "\n",
        "\\begin{equation}p(X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_p X_p)}}\\end{equation}\n",
        "\n",
        "In this expression, $p(X)$ represents the probability of the positive outcome. The right-hand side is a linear combination of predictor variables, each multiplied by coefficients $\\beta_0, \\beta_1, \\ldots, \\beta_p$. The sigmoid function is applied to this combination, ensuring that the result, $p(X)$, remains within the range of 0 to 1.\n",
        "\n",
        "In essence, the logistic regression equation incorporates the sigmoid function to transform the linear combination of predictor variables into a valid probability value, enabling binary classification decisions based on this probability threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac94bb4a",
      "metadata": {
        "id": "ac94bb4a"
      },
      "source": [
        "---\n",
        "\n",
        "<font color='Red'><b>Note:</b></font>\n",
        "\n",
        "`scipy.special.expit` is a function within the SciPy library, a widely recognized open-source resource for scientific and mathematical computations. This function, also known as the \"exponential of the logistic function,\" serves the purpose of executing a logistic sigmoid transformation. It operates on either a single value or an array as input and employs the logistic sigmoid function, mathematically expressed as:\n",
        "\n",
        "\\begin{equation}f(x) = \\frac{1}{1 + e^{-x}}\\end{equation}\n",
        "\n",
        "Here, $e$ signifies the natural logarithm base, while $x$ denotes the input variable. The fundamental role of the logistic sigmoid function is to transform input values into a bounded range of 0 to 1. This property finds extensive application in various domains, including machine learning, specifically in logistic regression, where it is instrumental in modeling the probability of binary outcomes.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd7cffb0",
      "metadata": {
        "id": "fd7cffb0"
      },
      "source": [
        "<font color='Blue'><b>Example:</b></font>\n",
        "Let's examine a straightforward scenario with a single variable, denoted as X, and a dependent variable, y. In this instance, we generate X values from a normal distribution with a mean ($\\mu$) of 10 and a standard deviation ($\\sigma$) of 2. The variable y is defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "y = \\begin{cases}\n",
        "1, & \\text{if } x > \\mu \\\\\n",
        "0, & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "Next, we estimate the probability of the default outcome using linear regression, which is depicted in the top plot. It's worth noting that some of the estimated probabilities turn out to be negative, which is problematic because probabilities should always fall within the range of 0 to 1.\n",
        "\n",
        "To address this issue, we employ logistic regression, as shown in the bottom plot. In logistic regression, all estimated probabilities strictly conform to the valid probability range, ensuring that they stay between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "182540fa",
      "metadata": {
        "id": "182540fa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
        "import numpy as np\n",
        "# from scipy.special import expit\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "mu, sigma = 10, 2  # mean and standard deviation\n",
        "np.random.seed(0)\n",
        "X = np.random.normal(mu, sigma, 100)\n",
        "y = np.where(X > mu, 1, 0)\n",
        "X = X.reshape(-1, 1)\n",
        "X_gen = np.linspace(X.min(), X.max(), 300)\n",
        "\n",
        "# Create the models\n",
        "models = [LinearRegression(), LogisticRegression()]\n",
        "\n",
        "# Titles for the plots\n",
        "titles = [ \"Linear Regression Model\", \"Logistic Regression Model\"]\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(9.5, 9), sharex=True, gridspec_kw={'hspace': 0})\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    model = models[i]\n",
        "    model.fit(X, y)\n",
        "\n",
        "    if i == 1:\n",
        "        Px = (1/(1 + np.exp(- (X_gen * model.coef_ + model.intercept_)))).flatten()\n",
        "        # Or we could utilize scipy.special.expit\n",
        "        # Px = expit(X_gen * model.coef_ + model.intercept_).ravel()\n",
        "    else:\n",
        "        Px = model.coef_ * X_gen + model.intercept_\n",
        "\n",
        "    ax.scatter(X, y, color='#b496ff', ec='#602ce5', alpha=0.3, label=\"Sample Data\")\n",
        "    ax.plot(X_gen, Px, color='#C60004', lw=2, label=titles[i])\n",
        "    ax.set_xlim([4, 16])\n",
        "    ax.hlines([0, 1], *ax.get_xlim(), linestyles='dashed', color = '#26912d', lw=1.5)\n",
        "    ax.set(xlabel='' if i == 0 else 'X', ylabel='Probability(X)')\n",
        "    ax.grid(False)\n",
        "    ax.legend(loc = 'right')\n",
        "    ax.set_ylim(-0.1, 1.1)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131d628b",
      "metadata": {
        "id": "131d628b"
      },
      "source": [
        "---\n",
        "\n",
        "<font color='Red'><b>Remark:</b></font>\n",
        "\n",
        "Logistic regression is primarily used for classification, not regression, despite its name. It's a statistical model used to predict the probability of a binary outcome, typically denoted as class 0 and class 1. The logistic regression model estimates the probability that a given input belongs to one of these two classes. This makes it a valuable tool for various classification tasks, such as spam detection, disease diagnosis, or sentiment analysis. In logistic regression, the output is a probability score, and a threshold (often 0.5) is applied to classify the input into one of the two classes. Therefore, it's fundamentally a classification technique, even though the term \"regression\" is in its name.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b7e2fa0",
      "metadata": {
        "id": "9b7e2fa0"
      },
      "source": [
        "### Finding the Coefficients\n",
        "\n",
        "In logistic regression, the goal is to find the optimal coefficients ($\\beta_0$, $\\beta_1$, ..., $\\beta_p$) for the model that best fits the data. This is achieved through the [Maximum Likelihood Estimation (MLE)](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) method. MLE aims to determine the coefficient values that maximize the likelihood of observing the actual binary outcomes in the dataset, given the predictor variables.\n",
        "\n",
        "Here's a step-by-step refined explanation of how to find these coefficients in logistic regression:\n",
        "\n",
        "1. **Formulate the Likelihood Function**: Begin by defining the likelihood function, which represents the probability of observing the binary outcomes (0s and 1s) based on the logistic regression model. This function is a mathematical representation of the relationship between the coefficients and the observed data.\n",
        "\n",
        "2. **Take the Natural Logarithm**: Simplify the likelihood calculations by working with the natural logarithm of the likelihood function, known as the log-likelihood. This transformation preserves the underlying structure while making computations more manageable.\n",
        "\n",
        "3. **Maximize the Log-Likelihood**: Utilize optimization techniques, often numerical methods, to find the coefficient values that maximize the log-likelihood function. The goal is to identify the coefficient values that make the observed data most likely under the logistic regression model.\n",
        "\n",
        "4. **Interpret the Coefficients**: Once the estimated coefficients are obtained, interpret their values. Each coefficient (excluding $\\beta_0$, the intercept) represents the change in the log-odds (logit) of the positive outcome (class 1) associated with a one-unit change in the corresponding predictor variable while holding other variables constant.\n",
        "\n",
        "Practical implementations of logistic regression, especially with multiple predictor variables ($X_1$, $X_2$, ..., $X_p$) and non-linear optimization, can be complex. The choice of optimization method may depend on the software libraries or statistical packages you are using.\n",
        "\n",
        "Most modern statistical software packages for data analysis, such as Python's scikit-learn or R's glm function, provide built-in functions to perform logistic regression. These functions automatically handle the MLE process and provide the estimated coefficients. However, if manual implementation of logistic regression is necessary, you'll likely rely on optimization algorithms like gradient descent or more advanced techniques. Proficiency in numerical optimization and statistical concepts is crucial for successfully implementing this process from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc2ad38",
      "metadata": {
        "id": "1bc2ad38"
      },
      "source": [
        "### Translating Theory into Python\n",
        "\n",
        "In the world of Python, practical implementation of logistic regression is greatly simplified by leveraging powerful libraries like scikit-learn. This Python library provides a comprehensive toolkit for machine learning, and the `LogisticRegression` class it offers is an invaluable tool for binary classification tasks. By utilizing this class, you can effortlessly apply logistic regression to your dataset, allowing you to fit the model and gain access to the estimated coefficients.\n",
        "\n",
        "For those eager to explore the depths of logistic regression within the scikit-learn ecosystem, the official scikit-learn documentation serves as a valuable resource. It acts as a gateway to the `LogisticRegression` class, providing detailed insights and instructions on how to utilize it effectively: [scikit-learn Logistic Regression Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). This documentation is an essential reference for both beginners and experienced practitioners, offering guidance on various parameters, techniques, and best practices for applying logistic regression in diverse real-world scenarios.."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d38f988",
      "metadata": {
        "id": "9d38f988"
      },
      "source": [
        "<font color='Blue'><b>Example:</b></font> In the next example, we turn our attention to the utilization of the Default dataset from the ISLR repository [James et al., 2023]. This dataset, available at (https://www.statlearning.com/resources-python). This dataset encapsulates a comprehensive record of instances where customers defaulted on their credit obligations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the zip file using wget\n",
        "!wget -N \"https://www.statlearning.com/s/ALL-CSV-FILES-2nd-Edition-corrected.zip\"\n",
        "\n",
        "# Unzip the downloaded zip file\n",
        "!unzip -j -o ALL-CSV-FILES-2nd-Edition-corrected.zip \"ALL CSV FILES - 2nd Edition/Default.csv\"\n",
        "\n",
        "# Remove the downloaded zip file after extraction\n",
        "!rm -r ALL-CSV-FILES-2nd-Edition-corrected.zip"
      ],
      "metadata": {
        "id": "UCb2d1kxTYqt"
      },
      "id": "UCb2d1kxTYqt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93eda5f4",
      "metadata": {
        "id": "93eda5f4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "Default = pd.read_csv('Default.csv')\n",
        "Default.columns = [x.title() for x in Default.columns]\n",
        "display(Default.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0daa53db",
      "metadata": {
        "id": "0daa53db"
      },
      "source": [
        "---\n",
        "\n",
        "<font color='Red'><b>Note:</b></font>\n",
        "\n",
        "\n",
        "In the context of credit cards, \"default\" refers to the failure of a cardholder to meet the agreed-upon terms and obligations associated with the credit card account. This typically includes missing minimum monthly payments, exceeding the credit limit, or violating other terms specified in the credit card agreement. When a cardholder defaults, it indicates a breach of the contract between the cardholder and the credit card issuer.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3680fd3",
      "metadata": {
        "id": "a3680fd3"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(9.5, 4.5), gridspec_kw = {'width_ratios':[.6,.2,.2]})\n",
        "CP = {'Yes': 'MediumSeaGreen', 'No': 'OrangeRed'}\n",
        "# Left\n",
        "_ = ax[0].scatter(Default.loc[Default.Default == 'Yes', 'Balance'],\n",
        "              Default.loc[Default.Default == 'Yes', 'Income'],\n",
        "              edgecolors=CP['Yes'], label='Yes')\n",
        "_ = ax[0].scatter(Default.loc[Default.Default == 'No', 'Balance'],\n",
        "              Default.loc[Default.Default == 'No', 'Income'],\n",
        "              edgecolors=CP['No'], label='No', alpha=0.2)\n",
        "_ = ax[0].set_ylim([0, 8e4])\n",
        "_ = ax[0].set(xlabel='Balance', ylabel='Income')\n",
        "_ = ax[0].legend(title='Default', loc='upper right')\n",
        "# Center\n",
        "_ = sns.boxplot(x='Default', y='Balance', data=Default, orient='v', ax=ax[1], palette=CP)\n",
        "_ = ax[1].set_ylim([0, 3e3])\n",
        "# Right\n",
        "_ = sns.boxplot(x='Default', y='Income', data=Default, orient='v', ax=ax[2], palette=CP)\n",
        "_ = ax[2].set_ylim([0, 8e4])\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62ffb3b5",
      "metadata": {
        "id": "62ffb3b5"
      },
      "source": [
        "The Default dataset is portrayed through three enlightening panels. Let's explore each [James et al., 2023]:\n",
        "\n",
        "**Left Panel:**\n",
        "Within this panel, the yearly incomes and monthly credit card balances of diverse individuals are visually depicted. Individuals who encountered default in their credit card payments are distinctly portrayed in Orange, while those who successfully averted default are prominently showcased in Green. This visualization immediately illuminates the dynamic interplay between income, credit card balance, and credit card payment outcomes.\n",
        "\n",
        "**Center Panel:**\n",
        "At the heart of this panel, a series of boxplots takes prominence, providing insights into the distribution of credit card balances in relation to default status. These boxplots offer a comprehensive overview of medians, quartiles, and potential outliers within each default group. The contrast between individuals who experienced default and those who did not is vividly delineated, succinctly revealing disparities in balances.\n",
        "\n",
        "**Right Panel:**\n",
        "Complementing the center panel, the right panel introduces a pair of boxplots that cast a spotlight on income variation according to default status. These boxplots concisely capture nuanced shifts in income distribution for both defaulters and non-defaulters, offering a visual narrative that illuminates the role income plays in credit card payment outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5fc69ee",
      "metadata": {
        "id": "d5fc69ee"
      },
      "source": [
        "The Student status can be effectively represented using a [dummy variable](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)), which is a common technique in data analysis and machine learning. This involves creating a binary variable that takes the value of 1 if the individual is a student, and 0 if the individual is not a student. In mathematical terms, this encoding can be expressed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Student} =\n",
        "\\begin{cases}\n",
        "1, & \\text{if Student}, \\\\\n",
        "0, & \\text{if Non-Student}.\n",
        "\\end{cases}\n",
        "\\end{equation}\n",
        "\n",
        "In this context:\n",
        "- When an individual's Student status is true, the corresponding value of the dummy variable is set to 1, indicating that the individual is a student.\n",
        "- Conversely, if an individual is not a student, the dummy variable takes the value of 0, signifying the non-student status.\n",
        "\n",
        "This approach simplifies the representation of Student status in a format that can be effectively utilized in various analyses and predictive modeling scenarios. It provides a straightforward and standardized way to incorporate categorical information like Student status into computational workflows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a560b5",
      "metadata": {
        "id": "d8a560b5"
      },
      "outputs": [],
      "source": [
        "df = Default.copy()\n",
        "df['Default'], _ = Default.Default.factorize()\n",
        "df['Student'], _ = Default.Student.factorize()\n",
        "print('From:')\n",
        "display(Default.head(5))\n",
        "print('To')\n",
        "display(df.head(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9705335",
      "metadata": {
        "id": "d9705335"
      },
      "source": [
        "## Simple Logistic Regression\n",
        "Let's explore a logistic regression approach characterized by the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log \\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 \\times \\text{Balance}\n",
        "\\end{equation}\n",
        "\n",
        "This formulation encapsulates the likelihood of default occurrence. It revolves around the effect of the 'Balance' predictor. While we initially mentioned 'Student' status and 'Income' as contributors, for clarity, we'll focus on 'Balance' in this representation.\n",
        "\n",
        "The probability of default, denoted as $ p(X) $, is expressed as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "p(X) = \\frac{e^{\\beta_0 + \\beta_1 \\times \\text{Balance}}}\n",
        "{1 + e^{\\beta_0 + \\beta_1 \\times \\text{Balance}}}\n",
        "\\end{equation}\n",
        "\n",
        "Breaking down the elements of the equation:\n",
        "- $ \\beta_0 $ and $ \\beta_1 $ are coefficients associated with the intercept and 'Balance', respectively.\n",
        "- 'Balance' serves as the predictor variable that shapes the logistic regression model's outcome.\n",
        "- The natural logarithm ($ \\log $) is applied to the odds ratio $ \\frac{p(X)}{1 - p(X)} $, enabling the representation of the relationship between the 'Balance' predictor and the log-odds of default in a linear manner.\n",
        "- The exponentiation ($ e^{\\dots} $) incorporated within the formula translates the log-odds into a probability scale.\n",
        "\n",
        "In essence, this logistic regression equation dissects the impact of 'Balance' on predicting the probability of default, providing a focused lens into this specific predictor's role in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c836d011",
      "metadata": {
        "id": "c836d011"
      },
      "source": [
        "Within the context of the Default dataset, logistic regression serves as a potent tool for modeling the probability of encountering default. To illustrate, consider the probability of default given a specific balance, which can be expressed as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Pr (Default = Yes | balance)}.\n",
        "\\end{equation}\n",
        "\n",
        "In this formulation, logistic regression enables us to quantitatively assess the likelihood of a default occurrence based on the observed balance value. This probability-based perspective provides valuable insights into the potential outcomes within the dataset, shedding light on the dynamics between credit card balances and the probability of encountering default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a267a17a",
      "metadata": {
        "id": "a267a17a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X = df['Balance'].values.reshape(-1, 1)\n",
        "y = df['Default'].values\n",
        "\n",
        "# Generating test data\n",
        "X_gen = np.arange(X.min(), X.max()).reshape(-1, 1)\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "_ = log_reg.fit(X, y)\n",
        "Pred_proba = log_reg.predict_proba(X_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4a336fb",
      "metadata": {
        "id": "e4a336fb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(9.5, 4.5))\n",
        "_ = ax.scatter(X, y, color='#7aea49', ec = '#38761d', alpha = 0.3)\n",
        "_ = ax.plot(X_gen, Pred_proba[:, 1], color='#C60004', lw=2)\n",
        "_ = ax.hlines([0, 1], *ax.get_xlim(), linestyles='dashed', lw=1)\n",
        "_ = ax.set(xlabel = 'Balance', ylabel='Probability of Default',\n",
        "              title = 'Estimated probability of default using logistic regression')\n",
        "_ = ax.grid(False)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582bbf9c",
      "metadata": {
        "id": "582bbf9c"
      },
      "source": [
        "The predicted probabilities of default are showcased, derived from the logistic regression method. Unlike the left panel, these probabilities align with the fundamental concept of probability, always lying within the range of 0 to 1. This adherence to the proper probability range is a distinctive trait of logistic regression's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b81cebf",
      "metadata": {
        "id": "2b81cebf"
      },
      "source": [
        "An alternative approach is to utilize the [**Statsmodels Generalized Linear Models**](https://www.statsmodels.org/devel/examples/notebooks/generated/glm_formula.html) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf62f785",
      "metadata": {
        "scrolled": false,
        "id": "bf62f785"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "# Also recall our Reg_Result for OLS Reression results\n",
        "def Reg_Result(Inp):\n",
        "    Temp = pd.read_html(Inp.summary().tables[1].as_html(), header=0, index_col=0)[0]\n",
        "    display(Temp.style\\\n",
        "    .format({'coef': '{:.4e}', 'P>|t|': '{:.4e}', 'std err': '{:.4e}'})\\\n",
        "    .bar(subset=['coef'], align='mid', color='Lime')\\\n",
        "    .set_properties(subset=['std err'], **{'background-color': 'DimGray', 'color': 'White'}))\n",
        "\n",
        "model = smf.glm(formula = 'Default ~ Balance', data = df, family=sm.families.Binomial())\n",
        "Results = model.fit()\n",
        "print(Results.summary())\n",
        "Reg_Result(Results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2900cebc",
      "metadata": {
        "id": "2900cebc"
      },
      "source": [
        "In particular, for\n",
        "```python\n",
        "model = smf.glm(formula='Default ~ Balance', data=df, family=sm.families.Binomial())\n",
        "```\n",
        "we have,\n",
        "\n",
        "1. `formula='Default ~ Balance'`: This is the formula notation used to specify the relationship between the response variable ('Default') and the predictor variable ('Balance'). The tilde symbol (~) separates the response variable from the predictor variable. In this case, it signifies that we are modeling the influence of 'Balance' on the likelihood of 'Default'.\n",
        "\n",
        "1. `family=sm.families.Binomial()`: This parameter defines the error distribution or family of the model. In this case, we're using the Binomial distribution, which is appropriate for binary response variables. The `sm.families.Binomial()` indicates that we're using the Binomial distribution from the Statsmodels library."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6c79d5",
      "metadata": {
        "id": "3d6c79d5"
      },
      "source": [
        "## Multiple Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "239dba77",
      "metadata": {
        "id": "239dba77"
      },
      "source": [
        "Let's delve into an alternative logistic regression approach characterized by the following equation:\n",
        "\n",
        "\\begin{equation}\n",
        "\\log \\left( \\frac{p(X)}{1 - p(X)} \\right) = \\beta_0 + \\beta_1 \\times \\text{Student} + \\beta_2 \\times \\text{Balance} + \\beta_3 \\times \\text{Income}\n",
        "\\end{equation}\n",
        "\n",
        "This formulation encapsulates the likelihood of encountering default. It hinges on the contribution of multiple predictors, including 'Student' status, 'Balance', and 'Income'. The probability of default, $ p(X) $, is expressed as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "p(X) = \\dfrac{e^{\\beta_0 + \\beta_1 \\times \\text{Student} + \\beta_2 \\times \\text{Balance} + \\beta_3 \\times \\text{Income}}}\n",
        "{1 + e^{\\beta_0 + \\beta_1 \\times \\text{Student} + \\beta_2 \\times \\text{Balance} + \\beta_3 \\times \\text{Income}}}\n",
        "\\end{equation}\n",
        "\n",
        "In this equation:\n",
        "- $ \\beta_0 $, $ \\beta_1 $, $ \\beta_2 $, and $ \\beta_3 $ are the coefficients corresponding to the intercept, 'Student' status, 'Balance', and 'Income', respectively.\n",
        "- 'Student', 'Balance', and 'Income' are the predictor variables that contribute to the logistic regression model.\n",
        "- The natural logarithm ($ \\log $) is applied to the odds ratio $ \\frac{p(X)}{1 - p(X)} $ to linearly model the relationship between the predictors and the log-odds of default.\n",
        "- The exponentiation ($ e^{\\dots} $) within the formula facilitates the transformation of the log-odds back to the probability scale.\n",
        "\n",
        "In essence, this logistic regression equation accounts for the combined influence of 'Student' status, 'Balance', and 'Income' in predicting the probability of default."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd8c4925",
      "metadata": {
        "id": "fd8c4925"
      },
      "source": [
        "An alternative approach is to utilize the [**Statsmodels Generalized Linear Models**](https://www.statsmodels.org/devel/examples/notebooks/generated/glm_formula.html) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad103ba5",
      "metadata": {
        "id": "ad103ba5"
      },
      "outputs": [],
      "source": [
        "formula = 'Default ~ Student + Income + Balance'\n",
        "model = smf.glm(formula = formula, data=df, family=sm.families.Binomial())\n",
        "Results = model.fit()\n",
        "print(Results.summary())\n",
        "Reg_Result(Results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e42c6536",
      "metadata": {
        "id": "e42c6536"
      },
      "source": [
        "## Example: Synthetic Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fce6043",
      "metadata": {
        "id": "8fce6043"
      },
      "source": [
        "<font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the `make_blobs` function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:\n",
        "\n",
        "- **Number of Samples:** 1000\n",
        "- **Number of Features:** 2\n",
        "- **Number of Classes:** 2\n",
        "- **Random Seed (random_state):** 0\n",
        "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
        "\n",
        "**Features:**\n",
        "- The dataset contains 1000 data points, each described by a pair of feature values. These features are represented as 'Feature 1' and 'Feature 2'.\n",
        "\n",
        "**Outcome (Target Variable):**\n",
        "- The dataset also includes a target variable called 'Outcome.' This variable assigns each data point to one of two distinct classes, identified as 'Class 0' and 'Class 1'.\n",
        "\n",
        "The dataset has been designed to simulate a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset is associated with one of the two classes, and it can be used for practicing and evaluating machine learning algorithms that deal with binary classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11ee76b0",
      "metadata": {
        "id": "11ee76b0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_blobs(n_samples=1000, centers=2, random_state=0, cluster_std=1.0)\n",
        "\n",
        "# Create a scatter plot using Seaborn\n",
        "fig, ax = plt.subplots(1, 1, figsize=(9.5, 6))\n",
        "\n",
        "colors = [\"#f5645a\", \"#b781ea\"]\n",
        "markers = ['o', 's']\n",
        "\n",
        "# Scatter plot of data points\n",
        "for num in np.unique(y):\n",
        "    ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
        "                s=40, edgecolors=\"k\", marker=markers[num], label=str(num))\n",
        "\n",
        "ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
        "ax.legend(title = 'Outcome')\n",
        "ax.set_title('Synthetic Dataset', weight = 'bold', fontsize = 16)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00432831",
      "metadata": {
        "id": "00432831"
      },
      "source": [
        "The synthetic dataset depicted above exhibits a balanced distribution between two classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09ccac2",
      "metadata": {
        "id": "d09ccac2"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 2))\n",
        "_ = sns.countplot(y = y, palette= colors)\n",
        "_ = ax.set(xlabel = 'Count', ylabel = 'Outcome' )\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae73380d",
      "metadata": {
        "id": "ae73380d"
      },
      "source": [
        "Next, Logistic Regression from the scikit-learn library [scikit-learn Developers, 2023] is employed for the classification of the aforementioned dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10b7893a",
      "metadata": {
        "id": "10b7893a"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "colors = [\"#f5645a\", \"#b781ea\"]\n",
        "edge_colors = ['#8A0002', '#3C1F8B']\n",
        "cmap_light = ListedColormap(['#f7dfdf', '#e3d3f2'])\n",
        "markers = ['o', 's']\n",
        "\n",
        "# Plot decision boundaries\n",
        "fig, ax = plt.subplots(1, 1, figsize=(9.5, 6))\n",
        "log_reg = LogisticRegression(max_iter = 200, solver = 'lbfgs')\n",
        "log_reg.fit(X, y)\n",
        "DecisionBoundaryDisplay.from_estimator(log_reg, X, cmap=cmap_light, ax=ax,\n",
        "                                   response_method=\"predict\",\n",
        "                                   plot_method=\"pcolormesh\",\n",
        "                                   xlabel= 'Feature 1', ylabel='Feature 2',\n",
        "                                   shading=\"auto\")\n",
        "# Scatter plot of data points\n",
        "for num in np.unique(y):\n",
        "    ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
        "                s=40, edgecolors=\"k\", marker=markers[num], label=str(num))\n",
        "\n",
        "ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
        "ax.legend(title = 'Outcome')\n",
        "ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
        "ax.set_title(f'Logistic Regression', fontweight='bold', fontsize = 16)\n",
        "ax.grid(False)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d2f1a04",
      "metadata": {
        "id": "1d2f1a04"
      },
      "source": [
        "In practical application, it is imperative to take into account the division of the dataset into two distinct subsets: one designated for model training and the other for model testing. This separation can be achieved using the `train_test_split` function, as exemplified below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e991a4d7",
      "metadata": {
        "id": "e991a4d7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=0)\n",
        "\n",
        "print(f'Shape of X_train = {X_train.shape}')\n",
        "print(f'Shape of y_train = {y_train.shape}')\n",
        "print(f'Shape of X_test = {X_test.shape}')\n",
        "print(f'Shape of y_test = {y_test.shape}')\n",
        "\n",
        "log_reg = LogisticRegression()\n",
        "_ = log_reg.fit(X_train, y_train)\n",
        "\n",
        "def _gen_cr(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    Results = pd.DataFrame(classification_report(y, y_pred,\n",
        "                                             output_dict=True)).T\n",
        "    display(Results.style.format(precision = 3))\n",
        "\n",
        "print('\\nTrain Data:')\n",
        "_gen_cr(log_reg, X_train, y_train)\n",
        "\n",
        "print('\\nTest Data:')\n",
        "_gen_cr(log_reg, X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acd50e55",
      "metadata": {
        "id": "acd50e55"
      },
      "source": [
        "The performance of the trained model on both the training and testing datasets is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d62da6a",
      "metadata": {
        "id": "3d62da6a"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
        "\n",
        "# Create a loop for train and test sets\n",
        "for i, (X_set, y_set, title) in enumerate([(X_train, y_train, 'Train Set'), (X_test, y_test, 'Test Set')]):\n",
        "    # Plot decision boundaries\n",
        "    DecisionBoundaryDisplay.from_estimator(log_reg, X_set, cmap=cmap_light, ax=axes[i],\n",
        "                                           response_method=\"predict\",\n",
        "                                           plot_method=\"pcolormesh\",\n",
        "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
        "                                           shading=\"auto\")\n",
        "    for num in np.unique(y):\n",
        "        axes[i].scatter(X_set[:, 0][y_set == num],\n",
        "                     X_set[:, 1][y_set == num], c=colors[num],\n",
        "                    s=40, edgecolors=\"k\", marker=markers[num], label=str(num))\n",
        "\n",
        "    axes[i].legend(title = 'Outcome:')\n",
        "    axes[i].set_title(f'{title} - Logistic Regression', fontweight='bold', fontsize=16)\n",
        "    axes[i].grid(False)\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "068c36ac",
      "metadata": {
        "id": "068c36ac"
      },
      "source": [
        "Upon closer examination, it becomes evident that certain data points from the set X have been misclassified. These instances are visually highlighted with yellow circles in the subsequent figure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41933de7",
      "metadata": {
        "id": "41933de7"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
        "\n",
        "# Create a loop for train and test sets\n",
        "for i, (X_set, y_set, title) in enumerate([(X_train, y_train, 'Train Set'), (X_test, y_test, 'Test Set')]):\n",
        "    # Plot decision boundaries\n",
        "    DecisionBoundaryDisplay.from_estimator(log_reg, X_set, cmap=cmap_light, ax=axes[i],\n",
        "                                           response_method=\"predict\",\n",
        "                                           plot_method=\"pcolormesh\",\n",
        "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
        "                                           shading=\"auto\")\n",
        "    for num in np.unique(y):\n",
        "        axes[i].scatter(X_set[:, 0][y_set == num],\n",
        "                     X_set[:, 1][y_set == num], c=colors[num],\n",
        "                    s=40, edgecolors=\"k\", marker=markers[num], label= f'Outcome {num}')\n",
        "\n",
        "    # Plot data points where y_set and log_reg(X_set) differ in color\n",
        "    axes[i].scatter(X_set[:, 0][y_set != log_reg.predict(X_set)],\n",
        "                    X_set[:, 1][y_set != log_reg.predict(X_set)],\n",
        "                    fc='Yellow', ec='black', s=40, marker= 'h', label= 'Inaccurate Predictions')\n",
        "\n",
        "    axes[i].set_title(f'{title} - Logistic Regression', fontweight='bold', fontsize=16)\n",
        "    axes[i].grid(False)\n",
        "    # Remove the legend for each panel\n",
        "    axes[i].legend()\n",
        "    axes[i].get_legend().remove()\n",
        "\n",
        "# Create a single legend for both subplots at the top\n",
        "handles, labels = axes[0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc='upper center', ncol=3, borderaxespad= -0.1)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6e4820",
      "metadata": {
        "id": "db6e4820"
      },
      "source": [
        "Lastly, we present the confusion matrix below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe2039e6",
      "metadata": {
        "id": "fe2039e6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt  # Import matplotlib for plotting\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix  # Import necessary functions/classes\n",
        "\n",
        "def format_confusion_matrix(cm, title):\n",
        "    true_pos, true_neg, false_pos, false_neg = cm.ravel()\n",
        "    result = f\"\\033[1m{title} Set Confusion Matrix\\033[0m:\\n\"\n",
        "    result += f\"- {true_pos} instances were correctly predicted as class 1.\\n\"\n",
        "    result += f\"- {true_neg} instances were correctly predicted as class 0.\\n\"\n",
        "    result += f\"- {false_pos} instance was incorrectly predicted as class 1 when it was actually class 0.\\n\"\n",
        "    result += f\"- {false_neg} instances were incorrectly predicted as class 0 when they were actually class 1.\\n\"\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def plot_cm(model, X_train, X_test, y_train, y_test, class_names, figsize=(7, 4)):\n",
        "    # Create a figure and axes for displaying confusion matrices side by side\n",
        "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    datasets = [(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]\n",
        "\n",
        "    for i in range(2):\n",
        "        X, y, dataset_name = datasets[i]\n",
        "\n",
        "        # Compute confusion matrix for the dataset predictions\n",
        "        cm = confusion_matrix(y, model.predict(X))\n",
        "\n",
        "        result = format_confusion_matrix(cm, dataset_name)\n",
        "        print(result)\n",
        "\n",
        "        # Create a ConfusionMatrixDisplay and plot it on the respective axis\n",
        "        cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names)\\\n",
        "                        .plot(ax=ax[i],\n",
        "                              im_kw=dict(cmap='Greens' if dataset_name == 'Train' else 'Blues'),\n",
        "                              text_kw={\"size\": 16}, colorbar=False)\n",
        "        ax[i].set_title(f'{dataset_name} Data')\n",
        "        ax[i].grid(False)\n",
        "\n",
        "    # Add a super title for the entire figure\n",
        "    fig.suptitle('Confusion Matrices', fontsize=16, weight = 'bold')\n",
        "\n",
        "    # Adjust the layout for better spacing\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54982e39",
      "metadata": {
        "id": "54982e39"
      },
      "outputs": [],
      "source": [
        "plot_cm(log_reg, X_train, X_test, y_train, y_test, ['0', '1'], figsize=(6, 3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}