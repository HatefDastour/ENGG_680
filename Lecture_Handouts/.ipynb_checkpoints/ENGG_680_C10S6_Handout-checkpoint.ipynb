{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2bf48f",
   "metadata": {
    "id": "bc2bf48f"
   },
   "source": [
    "# Random Forests\n",
    "\n",
    "Random forests offer an enhancement to bagged trees through a simple adjustment that introduces tree decorrelation. Like in bagging, we construct multiple decision trees on bootstrapped training samples. However, when constructing these trees, at each split point, only a random subset of m predictors is considered as potential candidates for the split, drawn from the full set of p predictors [James et al., 2023].\n",
    "\n",
    "The math behind Random Forests involves a few key concepts that contribute to its effectiveness in enhancing decision trees. Let's break down the main components [Hastie et al., 2013, James et al., 2023]:\n",
    "\n",
    "1. **Bootstrap Sampling:** In Random Forests, multiple decision trees are created, each based on a different subset of the training data. These subsets are obtained through a process called bootstrap sampling. Given a dataset with 'n' observations, bootstrap sampling involves randomly selecting 'n' observations with replacement. This means that some observations may be included multiple times in the subset, while others may not be included at all. This process generates diverse training subsets for building different trees.\n",
    "\n",
    "2. **Random Feature Selection:** At each split point of a decision tree within a Random Forest, instead of considering all available features (predictors), a random subset of features is selected as candidates for the split. This introduces randomness and diversity among the trees. The number of features in the subset, denoted as 'm', is typically smaller than the total number of predictors 'p'. This process helps decorrelate the trees, reducing the chance of them making similar errors and leading to more accurate predictions.\n",
    "\n",
    "3. **Voting or Averaging:** Once all the trees are constructed, their predictions are combined to make a final prediction. For regression tasks, the predictions from individual trees are usually averaged to obtain the final prediction. For classification tasks, a majority vote among the predictions is often taken to determine the class label. This ensemble approach helps improve the overall accuracy and stability of the model.\n",
    "\n",
    "Mathematically, the process of Random Forests involves creating 'B' decision trees, each constructed using a different bootstrap sample and a random subset of 'm' features at each split point. The final prediction for a new observation is obtained by averaging (for regression) or majority voting (for classification) the predictions from all the trees:\n",
    "\n",
    "For regression:\n",
    "\\begin{equation}\n",
    "\\hat{f}_{rf}(x) = \\frac{1}{B}\\sum_{b = 1}^{B} \\hat{f}^{b}(x)\n",
    "\\end{equation}\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/RFR_Fig.jpg\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual of Random Forests Algorithm for regression.\n",
    "</center>\n",
    "\n",
    "For classification:\n",
    "\\begin{equation}\n",
    "\\hat{C}_{rf}(x) = \\text{majority vote}\\left(\\hat{C}^{1}(x), \\hat{C}^{2}(x), \\ldots, \\hat{C}^{B}(x)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/RFC_Fig.jpg\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual of Random Forests Algorithm for classification.\n",
    "</center>\n",
    "\n",
    "Here, $\\hat{f}^{b}(x)$ represents the prediction of the 'b'-th tree for observation 'x', and $\\hat{C}^{b}(x)$ represents the class predicted by the 'b'-th tree for observation 'x'.\n",
    "\n",
    "The random forest algorithm's combination of bootstrap sampling and random feature selection helps create a diverse ensemble of trees that work together to provide more accurate and stable predictions, reducing the likelihood of overfitting and improving the model's generalization ability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea188739",
   "metadata": {
    "id": "ea188739"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Random Forest algorithm**\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - $N$ = Number of samples in the dataset\n",
    "   - $M$ = Number of features in each sample\n",
    "   - $x_i$ = Input features for the $i$th sample\n",
    "   - $y_i$ = Output label for regression task (real value)\n",
    "   - $C_i$ = Output class for classification task (categorical value)\n",
    "\n",
    "2. **Bootstrapping:**\n",
    "   - Randomly select $N$ samples with replacement to create multiple bags (bootstrap samples).\n",
    "   - In scikit-learn's API, the parameter controlling this is `bootstrap=True`.\n",
    "\n",
    "3. **Growing Individual Trees:**\n",
    "   - Each individual tree $t$ is trained on one of the bootstrap samples.\n",
    "   - At each node of tree $t$, consider a random subset of features of size $m$ for splitting.\n",
    "   - Stop growing the tree based on stopping criteria like maximum depth or minimum samples per leaf.\n",
    "   - In scikit-learn, you can control maximum depth with `max_depth` and minimum samples per leaf with `min_samples_leaf`.\n",
    "\n",
    "4. **Voting or Averaging:**\n",
    "   - For classification: Let $k$ be the number of classes. Each tree predicts a class $C_i$ for input $x_i$. Final prediction is the majority class among all trees' predictions.\n",
    "   - For regression: Each tree predicts a value $y_i$ for input $x_i$. Final prediction is the average of all trees' predictions.\n",
    "   - In scikit-learn, you can set `n_estimators` to determine the number of trees.\n",
    "\n",
    "5. **Out-of-Bag (OOB) Error:**\n",
    "   - For each sample $i$, if it's not in the training set of tree $t$, we can use its prediction to calculate the OOB error.\n",
    "   - In scikit-learn, OOB error can be calculated by setting `oob_score=True`.\n",
    "\n",
    "6. **Randomness and Diversity:**\n",
    "   - For feature subset selection, $m$ is typically set to $\\sqrt{M}$ for classification and $\\frac{M}{3}$ for regression.\n",
    "   - This randomness encourages different trees to focus on different subsets of features, leading to diversity.\n",
    "\n",
    "7. **Hyperparameters:**\n",
    "   - `n_estimators`: Number of trees in the forest.\n",
    "   - `max_depth`: Maximum depth of each tree.\n",
    "   - `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "   - `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
    "   - `max_features`: Number of features to consider for the best split at each node.\n",
    "   - `bootstrap`: Whether bootstrap samples should be used.\n",
    "   - `oob_score`: Whether to calculate out-of-bag score.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5513ff",
   "metadata": {
    "id": "2f5513ff"
   },
   "source": [
    "The Random Forests algorithm combines the predictions from multiple decision trees, each constructed on a different bootstrap sample and a subset of features. The diversity introduced by these mechanisms helps to reduce overfitting and improve the generalization performance of the ensemble model. Additionally, Random Forests provide insights into feature importance, which can be used for feature selection and understanding the underlying relationships in the data [Breiman, 2001, James et al., 2023].\n",
    "\n",
    "Keep in mind that the algorithm can be further customized and optimized with various hyperparameters and techniques, such as adjusting the number of trees, tuning the size of the feature subset, and handling missing values and categorical variables. Implementation details may vary depending on the programming language or library you're using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4b576a",
   "metadata": {
    "id": "3d4b576a"
   },
   "source": [
    "Here's how these concepts relate to scikit-learn's API:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Creating a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None, min_samples_split=2,\n",
    "                             min_samples_leaf=1, max_features=\"auto\", bootstrap=True,\n",
    "                             oob_score=True)\n",
    "\n",
    "# Creating a Random Forest Regressor\n",
    "reg = RandomForestRegressor(n_estimators=100, max_depth=None, min_samples_split=2,\n",
    "                            min_samples_leaf=1, max_features=\"auto\", bootstrap=True,\n",
    "                            oob_score=True)\n",
    "```\n",
    "\n",
    "You can replace the hyperparameter values above with your desired settings. The scikit-learn API makes it convenient to configure the Random Forest algorithm for your specific task and data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d7110",
   "metadata": {
    "id": "181d7110"
   },
   "source": [
    "##  Example: Auto MPG dataset\n",
    "\n",
    "<font color='Blue'><b>Example</b></font>. Consider the Auto MPG dataset retrieved from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VK0BumYDsMcA",
   "metadata": {
    "id": "VK0BumYDsMcA"
   },
   "outputs": [],
   "source": [
    "# Download the zip file using wget\n",
    "!wget -N \"http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\"\n",
    "\n",
    "# Unzip the downloaded zip file\n",
    "!unzip -o auto+mpg.zip auto-mpg.data\n",
    "\n",
    "# Remove the downloaded zip file after extraction\n",
    "!rm -r auto+mpg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9a217f",
   "metadata": {
    "id": "aa9a217f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n",
    "\n",
    "# Define column names based on the dataset's description\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model_Year', 'Origin', 'Car_Name']\n",
    "\n",
    "# Read the dataset with column names, treating '?' as missing values, and remove rows with missing values\n",
    "auto_mpg_df = pd.read_csv('auto-mpg.data', names=column_names,\n",
    "                          na_values='?', delim_whitespace=True).dropna().reset_index(drop = True)\n",
    "\n",
    "# Remove the 'Car_Name' column from the DataFrame\n",
    "auto_mpg_df = auto_mpg_df.drop(columns=['Car_Name'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(auto_mpg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9212a",
   "metadata": {
    "id": "07c9212a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Extract the features (X) and target variable (y)\n",
    "X = auto_mpg_df.drop(columns=['MPG'])\n",
    "y = np.log(auto_mpg_df.MPG.values)  # Take the natural logarithm of the MPG values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "set_size_df = pd.DataFrame({'Size': [len(X_train), len(X_test)]}, index = ['Train', 'Test'])\n",
    "display(set_size_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfb32b",
   "metadata": {
    "id": "46cfb32b"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "\n",
    "# Loop through different feature sets\n",
    "for i, n_estimators in enumerate([3, 10]):\n",
    "    # Create a Random Forest Regressor\n",
    "    reg = RandomForestRegressor(n_estimators = n_estimators, random_state= 0)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "    \n",
    "    # Create scatter plot and a diagonal reference line\n",
    "    scatter = ax[i].scatter(y_pred, y_test, label='medv', facecolors='SkyBlue', edgecolors='MidnightBlue', alpha=0.8)\n",
    "    line = ax[i].plot([0, 1], [0, 1], '--k', lw = 2, transform=ax[i].transAxes)\n",
    "    \n",
    "    _title = f'n_estimators = {n_estimators}'\n",
    "    # Set title and labels for the current subplot\n",
    "    title = ax[i].set(title = _title, xlabel='Predicted Values', ylabel='Actual Values')\n",
    "    \n",
    "    # Calculate and display Mean Squared Error (MSE) with background color\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    text = ax[i].text(0.68, 0.05, f'MSE = {mse:.3f}',\n",
    "                      transform=ax[i].transAxes, fontsize=12, weight='bold',\n",
    "                      bbox=dict(facecolor='Whitesmoke', alpha=0.7))  # Add background color\n",
    "\n",
    "    # Set equal aspect ratio for the subplots\n",
    "    ax[i].axis('equal')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfd3926",
   "metadata": {},
   "source": [
    "## Feature Importances\n",
    "\n",
    "In the context of a random forest model, the `feature_importances_` attribute serves as an essential metric for gauging the significance of individual features in facilitating accurate predictions. This attribute offers valuable insights into the influential role that each feature plays in shaping the model's predictions [Pedregosa et al., 2011, scikit-learn Developers, 2023].\n",
    "\n",
    "### Calculation of `feature_importances_`:\n",
    "\n",
    "The determination of feature importance in a random forest involves assessing how much each feature contributes to the reduction of impurity, commonly measured using metrics such as Gini impurity or Mean Squared Error, within the individual decision trees constituting the forest. The process unfolds as follows [Pedregosa et al., 2011, scikit-learn Developers, 2023]:\n",
    "\n",
    "1. **Tree Level Calculation:** Within each decision tree of the random forest, candidate features for splitting are identified based on the impurity reduction each feature would bring if chosen as the split feature. Metrics like Gini impurity or Mean Squared Error are frequently employed for this purpose.\n",
    "\n",
    "2. **Feature Contribution:** For each candidate feature in each tree, the algorithm quantifies how much the feature diminishes impurity in the data. Greater reduction implies a higher importance for that specific tree.\n",
    "\n",
    "3. **Averaging Across Trees:** After constructing all individual trees, the importance of each feature is averaged across the entire forest. This results in an importance score for each feature, indicating its collective contribution to the model's predictions.\n",
    "\n",
    "4. **Normalization:** Importance scores are typically normalized to sum up to 1 or 100. This normalization aids in interpreting the relative importance of each feature.\n",
    "\n",
    "5. **Interpretation:** A higher importance score denotes that a feature exerts a more substantial influence on the model's predictions. Conversely, features with lower importance scores contribute less to the model's predictive capabilities.\n",
    "\n",
    "### Interpretation of `feature_importances_`:\n",
    "\n",
    "The values within the `feature_importances_` array sum up to 0 or 1, contingent on normalization. These values are relative and offer insights into which features wield a more pronounced impact on the model's predictions. Higher importance values signify a more significant contribution to the model's ability to make accurate predictions.\n",
    "\n",
    "By scrutinizing `feature_importances_`, one can pinpoint key features steering the model's performance, concentrate on pertinent variables, and potentially engage in feature selection to enhance model efficiency.\n",
    "\n",
    "In essence, `feature_importances_` in a random forest model quantifies the contribution of each feature to the reduction of impurity across individual trees, providing a valuable tool for comprehending feature relevance and model behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae18e96e",
   "metadata": {
    "id": "ae18e96e"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Instantiate a RandomForestRegressor with 100 estimators and a random state of 0\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the RandomForestRegressor model on the training data\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Extract feature importances\n",
    "feature_importances = reg.feature_importances_\n",
    "\n",
    "# Create a DataFrame to store feature importances\n",
    "Importance = pd.DataFrame({'Importance': reg.feature_importances_ * 100}, index=X.columns)\n",
    "\n",
    "# Apply a background gradient to the DataFrame and round importance values to 3 decimal places\n",
    "styled_importance = Importance.style.background_gradient(cmap='PuBu', subset=['Importance']).format({'Importance': '{:.3f}'})\n",
    "\n",
    "# Display the styled DataFrame\n",
    "display(styled_importance)\n",
    "print('\\n')\n",
    "\n",
    "# Create a bar plot to visualize feature importances\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "bars = ax.bar(Importance.index, Importance.Importance, color='#e7d2f3', edgecolor='#611589', hatch=\"///\", lw=2)\n",
    "\n",
    "# Set plot labels and title\n",
    "ax.set_xlabel('Variable Importance', fontsize=12, weight='bold', color='midnightblue')\n",
    "ax.set_ylabel('Importance', fontsize=12, weight='bold', color='midnightblue')\n",
    "ax.set_title('Feature Importance in Random Forest Model', fontsize=16, weight='bold', color='darkslategray')\n",
    "\n",
    "# Set y-axis limits and adjust tick parameters\n",
    "ax.set_ylim([0, 40])\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=12, color='dimgray')\n",
    "ax.tick_params(axis='y', labelsize=12, color='dimgray')\n",
    "\n",
    "# Customize plot aesthetics\n",
    "ax.spines[['top', 'right']].set_visible(False)\n",
    "ax.spines[['bottom', 'left']].set_color('dimgray')\n",
    "ax.grid(axis='x')\n",
    "\n",
    "# Ensure a tight layout for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69779bac",
   "metadata": {
    "id": "69779bac"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>:  In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee940965",
   "metadata": {
    "id": "ee940965"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "try:\n",
    "  import sklearnex\n",
    "except ImportError:\n",
    "  !pip install pip install scikit-learn-intelex\n",
    "  import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "# Define colors and colormap for the plot\n",
    "colors = [\"#f44336\", \"#2986cc\", \"#065535\", '#ffe599']\n",
    "\n",
    "# Define a list of color names for the colormap\n",
    "_cmap = ListedColormap(colors)\n",
    "\n",
    "# Generate synthetic data using make_blobs\n",
    "n_samples = 1000\n",
    "n_features = 2\n",
    "centers = 4\n",
    "cluster_std = 1.0\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=centers, random_state=0, cluster_std=cluster_std)\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 9))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, m, alph in zip(axes, [5, 25, 100, None], 'abcd'):\n",
    "    # Create a RandomForestClassifier with specified max_depth\n",
    "    rfc = RandomForestClassifier(max_depth=m)\n",
    "    \n",
    "    # Fit the classifier to the data\n",
    "    rfc.fit(X, y)\n",
    "    \n",
    "    # Plot the decision boundary using DecisionBoundaryDisplay\n",
    "    DecisionBoundaryDisplay.from_estimator(rfc, X, cmap=_cmap, ax=ax,\n",
    "                                           response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
    "                                           shading=\"auto\",\n",
    "                                           alpha=0.3,)\n",
    "    \n",
    "    # Scatter plot for data points\n",
    "    for num in np.unique(y):\n",
    "        ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                   s=40, edgecolors='k', marker='o', label=str(num))\n",
    "    \n",
    "    # Set title and remove grid lines\n",
    "    ax.set_title(f'({alph}) max_depth = {m}', weight='bold')\n",
    "    \n",
    "    # Setaxis limits, and turn off grid\n",
    "#     _ = ax.set(xlim=[-6, 6], ylim=[-3, 12])\n",
    "    _ = ax.grid(False)\n",
    "\n",
    "# Adjust layout for better presentation\n",
    "plt.tight_layout()\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a664a1",
   "metadata": {
    "id": "d6a664a1"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "\n",
    "\"Scikit-learn extensions\" or \"sklearnex\" refers to additional modules or libraries that build upon the scikit-learn library, which is a machine learning library for Python. These extensions typically provide extra functionality, new algorithms, or improved features to enhance the capabilities of scikit-learn in various ways. The term \"sklearnex\" may encompass a range of third-party contributions aimed at extending and complementing the existing scikit-learn ecosystem.\n",
    "\n",
    "The function `sklearnex.patch_sklearn()` is a method within the scikit-learn extensions framework. Its primary purpose is to patch or modify the behavior of the scikit-learn library by incorporating additional functionalities or improvements provided by sklearnex.\n",
    "\n",
    "This function is typically employed to seamlessly integrate the extensions into the scikit-learn library, ensuring that the enhanced features or modifications become part of the standard scikit-learn functionality. By invoking `sklearnex.patch_sklearn()`, users can apply the necessary adjustments to the scikit-learn library, enabling the utilization of extended capabilities offered by the sklearnex framework within their machine learning workflows. Additional information can be found by referring to the documentation available [here](https://github.com/intel/scikit-learn-intelex).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07787b75",
   "metadata": {
    "id": "07787b75"
   },
   "source": [
    "The resulting visualization is a grid of subplots, each depicting a different scenario based on the chosen maximum depth value. The arrangement of these subplots allows for a clear comparison of how the complexity of decision boundaries changes with the depth of the decision trees. Through this example, one gains insight into the flexibility and versatility of Random Forests in handling complex classification tasks and capturing intricate decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58bc49",
   "metadata": {
    "id": "4f58bc49"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from pprint import pprint\n",
    "try:\n",
    "  import sklearnex\n",
    "except ImportError:\n",
    "  !pip install pip install scikit-learn-intelex\n",
    "  import sklearnex\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "rng = np.random.RandomState(0)\n",
    "\n",
    "# Create a RandomForestClassifier with default parameters\n",
    "rfc = RandomForestClassifier(random_state=rng)\n",
    "\n",
    "# Define the hyperparameter search space using param_dist\n",
    "param_dist = {'n_estimators': [10, 20, 25, 50],\n",
    "              \"max_depth\": [3, 5, 8, None],\n",
    "              \"max_features\": [2, 5, 7, 11],\n",
    "              \"min_samples_split\": [2, 5, 7, 11],\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\", \"log_loss\"]\n",
    "              }\n",
    "\n",
    "# Initialize HalvingRandomSearchCV with the estimator and parameter distributions\n",
    "rsh = HalvingRandomSearchCV(estimator=rfc,\n",
    "                            param_distributions=param_dist,\n",
    "                            factor=2, random_state=rng)\n",
    "\n",
    "# Fit the search object to your data\n",
    "_ = rsh.fit(X, y)\n",
    "\n",
    "# Get the best hyperparameters found by the search\n",
    "best_params_ = rsh.best_params_\n",
    "pprint(best_params_)\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c002d",
   "metadata": {
    "id": "5f3c002d"
   },
   "source": [
    "The core of this example is the utilization of the `HalvingRandomSearchCV` technique, which efficiently narrows down the hyperparameter search space. The technique gradually discards suboptimal combinations, ultimately converging on the best configuration. By fitting the search object to a dataset (`X` and `y`), the code extracts and prints the best hyperparameters found by the search process. This example provides a valuable insight into how `HalvingRandomSearchCV` can significantly speed up the search process while identifying hyperparameters that lead to improved model performance. It's a demonstration of harnessing cutting-edge techniques to fine-tune machine learning models effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39020d5c",
   "metadata": {
    "id": "39020d5c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "try:\n",
    "  import sklearnex\n",
    "except ImportError:\n",
    "  !pip install pip install scikit-learn-intelex\n",
    "  import sklearnex\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "def print_bold(txt, c = 31):\n",
    "    print(f\"\\033[1;{c}m\" + txt + \"\\033[0m\")\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "\n",
    "# Create a RandomForestClassifier instance\n",
    "rfc = RandomForestClassifier(random_state=rng, **best_params_)\n",
    "\n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# The splitt would be 80-20!\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "train_class_proportions, test_class_proportions = [], []\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate class proportions for train and test sets\n",
    "    train_class_proportions.append([np.mean(y_train == rfc) for rfc in np.unique(y)])\n",
    "    test_class_proportions.append([np.mean(y_test == rfc) for rfc in np.unique(y)])\n",
    "\n",
    "    # train\n",
    "    y_train_pred = rfc.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "\n",
    "    # test\n",
    "    y_test_pred = rfc.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
    "    print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()\n",
    "sklearnex.unpatch_sklearn()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
