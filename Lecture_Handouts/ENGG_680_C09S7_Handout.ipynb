{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faeee68b",
   "metadata": {
    "id": "faeee68b"
   },
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "Support Vector Machines (SVM) represent a versatile and potent paradigm in machine learning, adeptly handling both classification and regression tasks. The central tenet of SVM revolves around the discovery of an optimal hyperplane within a high-dimensional feature space, strategically demarcating data points belonging to disparate classes. This hyperplane's selection hinges on the maximization of the margin, which denotes the spatial interval between the hyperplane and the nearest data points emanating from each distinct class. Thus, SVM operates as a finely calibrated equilibrium between achieving high-fidelity alignment with training data and fostering robust extrapolation to novel, previously unseen data instances [Scholkopf and Smola, 2018, scikit-learn Developers, 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c4bc9",
   "metadata": {
    "id": "498c4bc9"
   },
   "source": [
    "## Basic principles of SVM\n",
    "\n",
    "- **Hyperplane**: A discernment boundary that partitions data instances of divergent classes, essentially serving as a segregation threshold.\n",
    "- **Support Vectors**: Data instances in closest proximity to the hyperplane, wielding significant influence over its placement and orientation.\n",
    "- **Margin**: The spatial expanse between the hyperplane and the closest support vectors, acting as a buffer zone against potential misclassification.\n",
    "- **Kernel Trick**: An ingenious technique empowering SVM to navigate intricate non-linear data configurations by effectively remapping data points to an augmented dimensional space.\n",
    "- **C Parameter**: Governs the compromise between maximizing the margin's breadth and minimizing classification errors, thereby shaping SVM's risk appetite.\n",
    "- **Kernel Functions**: Distinct kernel functions (e.g., linear, polynomial, radial basis function) pivotal in shaping the curvature and structure of the resultant decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295c80d3",
   "metadata": {
    "id": "295c80d3"
   },
   "source": [
    "## Support Vector Classifiers\n",
    "\n",
    "\n",
    "Support Vector Classifiers (SVCs) are a fundamental tool in binary classification problems, where the objective is to separate two classes, typically labeled as -1 and 1, by finding an optimal hyperplane. The mathematical representation of a hyperplane is as follows [James et al., 2023, Kuttler and Farah, 2020]:\n",
    "\n",
    "\\begin{equation} \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p = 0 \\end{equation}\n",
    "\n",
    "In this equation:\n",
    "- $ \\beta_0, \\beta_1, \\ldots, \\beta_p $ are coefficients that define the orientation and position of the hyperplane.\n",
    "- $ x_1, x_2, \\ldots, x_p $ are the features of the data point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e1804",
   "metadata": {
    "id": "065e1804"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> Consider a two-dimensional space defined by the X-Y plane. A concrete example of a hyperplane within this context could be represented by the equation [James et al., 2023]:\n",
    "\n",
    "\\begin{equation} 1 + 2X_1 + 3X_2 = 0\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850acdf0",
   "metadata": {
    "id": "850acdf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Create a figure and axis with a specific size\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "# Generate a range of values for X1\n",
    "X1 = np.linspace(-1.5, 1.5, 1000)\n",
    "\n",
    "# Calculate corresponding values for X2 based on the hyperplane equation -(1 + 2*X1)/3\n",
    "X2 = -(1 + 2*X1)/3\n",
    "\n",
    "# Plot the hyperplane\n",
    "ax.plot(X1, X2)\n",
    "\n",
    "# Set labels, limits, and aspect ratio for X and Y axes\n",
    "ax.set(xlabel=r'$X_1$', ylabel=r'$X_2$', xlim=[-1.5, 1.5], ylim=[-1.5, 1.5], aspect=1)\n",
    "\n",
    "# Fill the region where 1 + 2X1 + 3X2 > 0 with a light green color and annotate\n",
    "ax.fill_between(X1, -1.5, -(1 + 2*X1)/3, color='LimeGreen', alpha=0.1)\n",
    "ax.annotate(r'$1 + 2X_1 + 3X_2 > 0$', xy=(-0.5, 0.5), fontsize='xx-large')\n",
    "\n",
    "# Fill the region where 1 + 2X1 + 3X2 < 0 with an orange color and annotate\n",
    "ax.fill_between(X1, -(1 + 2*X1)/3, 1.5, color='Orange', alpha=0.1)\n",
    "ax.annotate(r'$1 + 2X_1 + 3X_2 < 0$', xy=(-0.9, -1), fontsize='xx-large')\n",
    "ax.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e34c6c",
   "metadata": {
    "id": "63e34c6c"
   },
   "source": [
    "By executing this code snippet, a vivid graphical representation of a hyperplane within a two-dimensional space emerges. The code generates a plot where the hyperplane, defined by the equation $1 + 2X_1 + 3X_2 = 0$, partitions the plane into regions where this inequality holds true and where it does not. This not only exemplifies the concept of a hyperplane in a 2D plane but also underscores the utility of programming tools in visualizing mathematical constructs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0137deb",
   "metadata": {
    "id": "a0137deb"
   },
   "source": [
    "## Formulating the Maximal Margin Classifier\n",
    "\n",
    "In this section, we delve into the procedure for constructing the maximal margin hyperplane based on a set of *n* training observations *$x_1, ..., x_n \\in \\mathbb{R}^p$*, accompanied by corresponding class labels *$y_1, ..., y_n \\in \\{-1, 1\\}$*. The core objective of a Support Vector Classifier (SVC) lies in the maximization of the margin (*$M$*), which symbolizes the perpendicular distance between the hyperplane and the nearest data points from each class. The essence of the optimization problem is to pinpoint the optimal coefficients *$\\beta_0, \\beta_1, ..., \\beta_p$* that achieve the maximum *$M$*, under specific constraints for each training observation (*$i$*) [James et al., 2023].\n",
    "\n",
    "Mathematically, the optimization problem is formalized as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i (\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_p x_{ip}) \\geq M (1 - \\epsilon_i)\n",
    "\\end{equation}\n",
    "\n",
    "Where:\n",
    "- *$y_i$* denotes the class label of the *$i$*-th observation, taking values -1 or 1.\n",
    "- *$x_{i1}$*, *$x_{i2}$*, ..., *$x_{ip}$* represent the features of the *$i$*-th observation.\n",
    "- *$\\epsilon_i$* acts as a slack variable that accommodates potential misclassification. It assumes a value of zero for accurately classified points and a positive value for misclassified points.\n",
    "\n",
    "These constraints collectively ensure that each observation resides on the appropriate side of the hyperplane, with a margin of at least *$M$*. Incorporating slack variables (*$\\epsilon_i$*) introduces adaptability, allowing certain observations to exist within the margin or on the incorrect side of the hyperplane.\n",
    "\n",
    "Another significant constraint, contributing to coefficient normalization, is captured by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{j=1}^{p} \\beta_j^2 = 1\n",
    "\\end{equation}\n",
    "\n",
    "This constraint mitigates the dominance of individual features in determining the classification outcome.\n",
    "\n",
    "Furthermore, the regularization parameter *$C$* plays a pivotal role in balancing the trade-off between maximizing the margin *$M$* and minimizing misclassifications:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n} \\epsilon_i \\leq C\n",
    "\\end{equation}\n",
    "\n",
    "The optimization challenge centers on identifying coefficients *$\\beta_0, \\beta_1, ..., \\beta_p$* and slack variables *$\\epsilon_i$* that satisfy these constraints while concurrently maximizing the margin *$M$*. This formulation culminates in the establishment of a hyperplane that effectively segregates classes, accounting for the interplay between margin and misclassification.\n",
    "\n",
    "Through the resolution of this optimization problem, a Support Vector Classifier determines the optimal hyperplane for class separation within the feature space. Notably, the observations positioned closest to this hyperplane, known as support vectors, wield substantial influence over the placement and margin of the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc94071",
   "metadata": {
    "id": "7dc94071"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Sample data (features and class labels)\n",
    "X = np.array([[2, 1], [3, 3], [4, 3], [5, 4], [6, 5], [7, 5], [8, 6], [9, 7]])\n",
    "y = np.array([-1, -1, -1, -1, 1, 1, 1, 1])\n",
    "\n",
    "# Create a Support Vector Classifier\n",
    "svc = SVC(kernel='linear')\n",
    "\n",
    "# Fit the model to the data\n",
    "svc.fit(X, y)\n",
    "\n",
    "# Get the support vectors\n",
    "support_vectors = svc.support_vectors_\n",
    "print(\"Support Vectors:\")\n",
    "for i in range(support_vectors.shape[1]):\n",
    "    print(f'\\t\\t{support_vectors[:, i]}')\n",
    "\n",
    "# Get the coefficients (beta values) and the intercept (beta_0)\n",
    "coefficients = svc.coef_\n",
    "intercept = svc.intercept_\n",
    "print(\"Coefficients (Beta values):\\t\\t\", coefficients)\n",
    "print(\"Intercept (Beta_0):\\t\\t\\t\", intercept)\n",
    "\n",
    "# Get the margin (M)\n",
    "margin = 2 / np.linalg.norm(coefficients)\n",
    "print(f'Margin (M):\\t\\t\\t\\t {margin:+.3f}')\n",
    "\n",
    "# Get the slack variables (epsilon values)\n",
    "dual_coefs = svc.dual_coef_[0]\n",
    "epsilon_values = 1 - dual_coefs\n",
    "print(\"Slack Variables (Epsilon values):\\t\", epsilon_values)\n",
    "\n",
    "# Get the regularization parameter (C)\n",
    "C = 1 / (2 * svc.C)\n",
    "print(\"Regularization Parameter (C):\\t\\t\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d5dc3",
   "metadata": {
    "id": "106d5dc3"
   },
   "source": [
    "---\n",
    "\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "Note that in the above code:\n",
    "\n",
    "* The `support_vectors` variable contains the support vectors that were identified by the Support Vector Classifier (SVC) during the fitting of the model. Support vectors are the data points from the training dataset that are closest to the decision boundary (hyperplane) and have the most influence on defining the decision boundary. These vectors \"support\" the position of the decision boundary and, as a result, are crucial in the classification process. Support vectors are important in SVM (Support Vector Machine) algorithms because they determine the margin, which is the distance between the decision boundary and the nearest data points. In the context of the code you've provided, `support_vectors` will hold the coordinates of these important data points in your feature space.\n",
    "\n",
    "* The variable `coefficients` stores the coefficients associated with the features in the Support Vector Classifier (SVC) model. These coefficients represent the weights or importance of each feature in the decision boundary equation. For an SVC model with a linear kernel, the decision boundary is a hyperplane, and the coefficients represent the normal vector to this hyperplane. In other words, they define the orientation of the hyperplane in the feature space. In the context of your code, `coefficients` contains the coefficients for each feature dimension, and you can use them to understand the relative importance of each feature in the classification decision. These coefficients are determined during the training of the SVC model and are essential for making predictions.\n",
    "\n",
    "* The variable `intercept` stores the intercept of the decision boundary in the Support Vector Classifier (SVC) model. The intercept represents the offset of the decision boundary from the origin in the feature space. For an SVC model with a linear kernel, the decision boundary is a hyperplane defined by the coefficients and the intercept. The intercept determines the position of this hyperplane along the feature space's axis that is orthogonal to the hyperplane. In simpler terms, the intercept shifts the decision boundary back and forth along this axis. If we have a positive intercept, it means the decision boundary is shifted in one direction, and if you have a negative intercept, it's shifted in the opposite direction. In the code, `intercept` contains the intercept value for the decision boundary and is determined during the training of the SVC model. It plays a crucial role in the classification process.\n",
    "\n",
    "* In the above code, the variable `margin` calculates the margin of the Support Vector Classifier (SVC) model. The margin represents the distance between the decision boundary (hyperplane) and the nearest support vectors. It is a measure of how well the model can separate the classes. The formula used to calculate `margin` is as follows:\n",
    "\n",
    "    ```python\n",
    "    margin = 2 / np.linalg.norm(coefficients)\n",
    "    ```\n",
    "\n",
    "* In the above code, the comments indicate that we are obtaining the slack variables (epsilon values) in the context of a Support Vector Classifier (SVC) model. Let's break down what these lines of code are doing:\n",
    "\n",
    "    1. `dual_coefs = svc.dual_coef_[0]`: In an SVM, the dual coefficients represent the Lagrange multipliers associated with the support vectors. These coefficients indicate the importance of each support vector in determining the location and orientation of the decision boundary. By accessing `svc.dual_coef_`, you are retrieving the dual coefficients for the support vectors. The `[0]` indexing suggests that you are extracting the dual coefficients for the first class (typically the positive class) of the binary classification problem.\n",
    "\n",
    "    2. `epsilon_values = 1 - dual_coefs`: The variable `epsilon_values` is then calculated. Epsilon values, also known as slack variables, are introduced in soft-margin SVMs. They measure the degree to which a data point is allowed to be on the wrong side of the decision boundary while still satisfying the margin constraints. In this calculation, you are computing the epsilon values by subtracting the dual coefficients from 1. A value close to 1 indicates that the corresponding support vector is far from the margin, while a value close to 0 indicates that the support vector is very close to or on the wrong side of the margin.\n",
    "\n",
    "    These values are useful for understanding the margin and classification properties of the SVM. A larger epsilon value indicates a greater margin violation by the corresponding support vector, and a smaller value indicates that the support vector is correctly classified or is very close to the margin.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b74e3d",
   "metadata": {
    "id": "f7b74e3d"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\"]\n",
    "edge_colors = ['#8A0002', '#3C1F8B']\n",
    "cmap_light = ListedColormap(['#f7dfdf', '#e3d3f2'])\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "DecisionBoundaryDisplay.from_estimator(svc, X, cmap=cmap_light, ax=ax,\n",
    "                                       response_method=\"predict\", plot_method=\"pcolormesh\",\n",
    "                                       xlabel='Feature 1', ylabel='Feature 2', shading=\"auto\")\n",
    "\n",
    "# Define labels and markers for different classes\n",
    "class_labels = [1, -1]\n",
    "markers = ['o', 's']\n",
    "\n",
    "for label, marker in zip(class_labels, markers):\n",
    "    class_data = X[y == label]\n",
    "    ax.scatter(class_data[:, 0], class_data[:, 1], fc=colors[label == 1],\n",
    "               ec=edge_colors[label == 1], label=str(label), marker=marker)\n",
    "\n",
    "# Plot support vectors\n",
    "support_vectors = svc.support_vectors_\n",
    "ax.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200,\n",
    "           facecolors='none', edgecolors='k', lw=2, label='Support Vectors')\n",
    "\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.set_title(\"\"\"Support Vector Classifier\"\"\", fontweight='bold', fontsize=16)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0268570c",
   "metadata": {
    "id": "0268570c"
   },
   "source": [
    "In the context of binary classification using a Support Vector Machine (SVM) with a linear kernel, it is possible to retrieve the coefficients associated with the decision boundary. These coefficients are indicative of the feature weights that define the orientation of the decision boundary, often referred to as a hyperplane. The equation representing the decision boundary takes the form:\n",
    "\n",
    "\\begin{equation}w_1 \\cdot x_1 + w_2 \\cdot x_2 + b = 0\\end{equation}\n",
    "\n",
    "Here, $w_1$ and $w_2$ represent the coefficients, $x_1$ and $x_2$ denote the features, and $b$ stands for the intercept.\n",
    "\n",
    "To access the coefficients $w_1$ and $w_2$, you can utilize `svc.coef_`, and to obtain the intercept $b$, you may employ `svc.intercept_` from your SVM model denoted as `svc`. The procedure for accessing these values is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1aab3d",
   "metadata": {
    "id": "ad1aab3d"
   },
   "outputs": [],
   "source": [
    "# Hyperplane equation\n",
    "w1, w2 = svc.coef_[0]\n",
    "b = svc.intercept_[0]\n",
    "equation = f'${w1:+.3f} \\cdot x_1 {w2:+.3f} \\cdot x_2 {b:+.3f} = 0$'\n",
    "\n",
    "# Display the equation using LaTeX\n",
    "from IPython.display import Latex, display\n",
    "display(Latex(equation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed63e6",
   "metadata": {
    "id": "6aed63e6"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\"]\n",
    "edge_colors = ['#8A0002', '#3C1F8B']\n",
    "cmap_light = ListedColormap(['#f7dfdf', '#e3d3f2'])\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "\n",
    "# Decision boundary display\n",
    "DecisionBoundaryDisplay.from_estimator(svc, X, cmap=cmap_light, ax=ax, response_method=\"predict\", plot_method=\"pcolormesh\", xlabel='Feature 1', ylabel='Feature 2', shading=\"auto\")\n",
    "\n",
    "# Define labels and markers for different classes\n",
    "class_info = [(1, 'o', colors[1]), (-1, 's', colors[0])]\n",
    "\n",
    "for label, marker, color in class_info:\n",
    "    class_data = X[y == label]\n",
    "    ax.scatter(class_data[:, 0], class_data[:, 1], fc=color, ec=edge_colors[label == 1],\n",
    "               label=str(label), marker=marker)\n",
    "\n",
    "# Plot support vectors\n",
    "support_vectors = svc.support_vectors_\n",
    "ax.scatter(support_vectors[:, 0], support_vectors[:, 1], s=200, facecolors='none',\n",
    "           edgecolors='k', lw=2, label='Support Vectors')\n",
    "\n",
    "# Decision boundary line\n",
    "w1, w2 = svc.coef_[0]\n",
    "b = svc.intercept_[0]\n",
    "line_x = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)\n",
    "line_y = (-w1 / w2) * line_x - (b / w2)\n",
    "ax.plot(line_x, line_y, color='black', linestyle='--', label= f'Decision Boundary: {equation}')\n",
    "\n",
    "# Plot settings\n",
    "ax.legend(loc = 'lower right')\n",
    "ax.set_title(\"\"\"Support Vector Classifier\"\"\", fontweight='bold', fontsize=16)\n",
    "ax.grid(False)\n",
    "ax.set_ylim(0, 8)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f93093",
   "metadata": {
    "id": "b9f93093"
   },
   "source": [
    "## Classification with Non-Linear Decision Boundaries\n",
    "\n",
    "Support Vector Machines (SVMs) represent a significant leap beyond the conventional support vector classifier, enabling the handling of intricate non-linear decision boundaries in classification tasks. This transformative advancement stems from a novel approach that expands the feature space, providing a more comprehensive representation of the underlying data structure [James et al., 2023].\n",
    "\n",
    "### Augmented Feature Space\n",
    "\n",
    "In contrast to the reliance on *$p$* traditional features:\n",
    "\n",
    "\\begin{equation}\n",
    "X_1, X_2, \\ldots , X_p.\n",
    "\\end{equation}\n",
    "\n",
    "SVMs introduce a revolutionary paradigm by extending the support vector classifier to function within an augmented feature space encompassing *$2p$* features:\n",
    "\n",
    "\\begin{equation}\n",
    "X_1, X_1^2, X_2, X_2^2, \\ldots , X_p, X^2_p.\n",
    "\\end{equation}\n",
    "\n",
    "This augmentation equips SVMs to adeptly address intricate classification complexities characterized by non-linear relationships [James et al., 2023].\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The foundation of this advancement resides in the core optimization problem of SVMs, defined as follows:\n",
    "\n",
    "\\begin{align}\n",
    "&\\text{Maximize the margin } M \\\\\n",
    "&\\text{subject to:} \\\\\n",
    "&\\begin{cases}\n",
    "\\displaystyle{y_i \\left(\\beta_0 + \\sum_{j=1}^{p}\\beta_{j1} x_{ij}+\\sum_{j=1}^{p}\\beta_{j2} x_{ij}^2 \\right)\n",
    "\\geq M (1-\\epsilon_i)}\n",
    "\\\\\n",
    "\\displaystyle{\\sum_{j=1}^{p} \\sum_{k=1}^{2} \\beta_{jk}^2 = 1}\n",
    "\\\\\n",
    "\\displaystyle{\\epsilon_i \\geq 0}\n",
    "\\\\\n",
    "\\displaystyle{\\sum_{i=1}^{n} \\epsilon_i \\leq C.}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Key Insights from this Formulation:\n",
    "\n",
    "- The ultimate objective remains the maximization of the margin (*$M$*), embodying the distinction between classes.\n",
    "- The introduction of quadratic terms (*$x_{ij}^2$*) for each of the *$p$* original features enhances predictive capacity, enabling SVMs to capture intricate non-linear relationships inherent in the data.\n",
    "- Analogous to the conventional support vector classifier, the conditions ensure that training observations (*$x_i$*) consistently align with the correct side of their respective hyperplanes. The incorporation of slack variables (*$\\epsilon_i$*) accommodates potential misclassifications.\n",
    "- A noteworthy addition lies in the utilization of coefficients (*$\\beta_{j1}$* and *$\\beta_{j2}$*) to capture both linear and quadratic facets of the transformed features. This versatility empowers SVMs to delineate non-linear decision boundaries.\n",
    "- The constraint *$\\sum_{j=1}^{p} \\sum_{k=1}^{2} \\beta_{jk}^2 = 1$* ensures the appropriate scaling of the augmented hyperplane within the expanded feature space.\n",
    "- In alignment with its role in the original support vector classifier, the regularization parameter *$C$* retains its significance in balancing the optimization trade-off between maximizing the margin and permitting controlled misclassification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72ac8f",
   "metadata": {
    "id": "ff72ac8f"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the `make_blobs` function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:\n",
    "\n",
    "- **Number of Samples:** 1000\n",
    "- **Number of Features:** 2\n",
    "- **Number of Classes:** 2\n",
    "- **Random Seed (random_state):** 0\n",
    "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
    "\n",
    "**Features:**\n",
    "- The dataset contains 1000 data points, each described by a pair of feature values. These features are represented as 'Feature 1' and 'Feature 2'.\n",
    "\n",
    "**Outcome (Target Variable):**\n",
    "- The dataset also includes a target variable called 'Outcome.' This variable assigns each data point to one of two distinct classes, identified as 'Class 0' and 'Class 1'.\n",
    "\n",
    "The dataset has been designed to simulate a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset is associated with one of the two classes, and it can be used for practicing and evaluating machine learning algorithms that deal with binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040cafac",
   "metadata": {
    "id": "040cafac"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=1000, centers=2, random_state=0, cluster_std=1.0)\n",
    "\n",
    "# Define colors and markers for data points\n",
    "colors, markers = [\"#f44336\", \"#40a347\"], ['o', 's']\n",
    "cmap_ = ListedColormap(colors)\n",
    "# Titles for the subplots\n",
    "titles = ['Linear Decision Boundaries', 'Non-Linear Decision Boundaries']\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.5, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Create SVM models and visualize decision boundaries\n",
    "for ax, deg in zip(axes, [1, 2]):\n",
    "    # Create SVM model with specific parameters\n",
    "    clf = svm.SVC(kernel=\"poly\", degree=deg, gamma=\"auto\", C=1).fit(X, y)\n",
    "\n",
    "    # Scatter plot of data points\n",
    "    for num in np.unique(y):\n",
    "        ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                   s=40, edgecolors=\"k\", marker=markers[num], label=str(num))\n",
    "\n",
    "    # Display decision boundaries\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(clf, X, response_method=\"predict\",\n",
    "                                                  cmap=cmap_, alpha=0.2, ax=ax,\n",
    "                                                  xlabel=r'$X_{1}$', ylabel=r'$X_{2}$')\n",
    "\n",
    "    # Set the title for the subplot\n",
    "    ax.set_title(f'Classification with {titles.pop(0)}', weight='bold')\n",
    "    ax.grid(False)\n",
    "    ax.legend(title='Class', fontsize=12)\n",
    "\n",
    "# Add a title to the entire figure\n",
    "plt.suptitle(\"SVM Poly Classifiers Comparison\", fontsize=16, fontweight='bold')\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae24783",
   "metadata": {
    "id": "dae24783"
   },
   "source": [
    "## The Support Vector Machine: Kernelized Enhancement\n",
    "\n",
    "### Linear Support Vector Classifier\n",
    "\n",
    "The linear support vector classifier can be expressed as follows [James et al., 2023]:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\beta_0 + \\sum_{i\\in \\mathcal{S}} \\alpha_i\\langle x, x_i\\rangle,\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "- $f(x)$ represents the decision function that outputs a classification score for the input feature vector *$x$*.\n",
    "- $\\beta_0$ is the intercept term.\n",
    "- $\\alpha_i$ are the coefficients associated with the support vectors *$x_i$*.\n",
    "- $\\langle x, x_i\\rangle$ denotes the inner product between the input vector *$x$* and the support vector *$x_i$*.\n",
    "- The set $\\mathcal{S}$ contains indices corresponding to the support vectors, which are the training samples that lie closest to the decision boundary.\n",
    "\n",
    "---\n",
    "<font color='Red'><b>Remark:</b></font>\n",
    "\n",
    "\n",
    "Recall that the inner product of two *r*-vectors *$a$* and *$b$* is formally defined as $\\langle a, b \\rangle = \\sum_{i=1}^{r} a_i b_i$, where the sum extends over *$r$* components [James et al., 2023].\n",
    "\n",
    "---\n",
    "\n",
    "### Kernelized Format\n",
    "\n",
    "In a more generalized form, we replace the inner product with a function denoted as the **kernel**:\n",
    "\n",
    "\\begin{align}\n",
    "K(x_i, x_{i'}) = K(x_{i'}, x_i),\n",
    "\\end{align}\n",
    "\n",
    "Here, *$K$* is a function that computes the similarity between two data points *$x_i$* and *$x_{i'}$*. This kernelized representation enables us to express *$f(x)$* as [James et al., 2023]:\n",
    "\n",
    "\\begin{align}\n",
    "f(x) = \\beta_0 + \\sum_{i\\in \\mathcal{S}} \\alpha_iK(x_i, x_{i'}).\n",
    "\\end{align}\n",
    "\n",
    "Various types of kernels can be harnessed in SVMs to capture different types of relationships between data points. Here are some illustrative examples:\n",
    "\n",
    "- **Linear Kernel:** $\\displaystyle{K(x_i, x_{i'}) = \\sum_{j=1}^{p} x_{ij}x_{i'j}}$\n",
    "- **Polynomial Kernel:** $\\displaystyle{K(x_i, x_{i'}) = \\left(1+\\sum_{j=1}^{p} x_{ij}x_{i'j}\\right)^d}$\n",
    "- **Radial Kernel (RBF):** $\\displaystyle{K(x_i, x_{i'}) = \\exp\\left(-\\gamma \\sum_{j=1}^{p} (x_{ij}-x_{i'j})^2\\right)}$\n",
    "\n",
    "where $d$ and $\\gamma$ are hyperparameters that influence the shape and flexibility of the decision boundary.\n",
    "\n",
    "Scikit-learn's SVM implementation provides flexibility in choosing the appropriate kernel and tuning hyperparameters to achieve the best classification performance for different types of data. The classifier aims to maximize the margin while ensuring accurate classification and controlled misclassification, guided by the chosen kernel's ability to capture the data's underlying structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba609bf",
   "metadata": {
    "id": "4ba609bf"
   },
   "source": [
    "---\n",
    "\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "\n",
    "In the context of the provided text, $x'$ is used as a placeholder to represent another data point. It's not a specific variable; rather, it's a general notation to indicate that the kernel function $K(x_i, x_{i'})$ computes the similarity between the data point $x_i$ and some other data point $x_{i'}$.\n",
    "\n",
    "So, $x_i$ and $x_{i'}$ are two different data points, and $K(x_i, x_{i'})$ measures the similarity between them using the chosen kernel function. The notation $x'$ is used to emphasize that this kernel function can be applied to any pair of data points in the dataset, not just $x_i$ and $x_{i'}$). It's a way to express the general concept of similarity between data points in the context of Support Vector Machines (SVM) and kernelized representations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d697e",
   "metadata": {
    "id": "226d697e"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> In this example, we're using SVM classifiers along with matplotlib to demonstrate how various SVM kernels behave on the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). We aim to show their decision boundaries in a grid of four smaller plots (2x2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1a2f3",
   "metadata": {
    "id": "8ad1a2f3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# Create SVM models\n",
    "models = [svm.SVC(kernel=\"linear\", C=1.0),\n",
    "          svm.SVC(kernel=\"rbf\", gamma=0.7, C=1.0),\n",
    "          svm.SVC(kernel=\"poly\", degree=3, gamma=\"auto\", C=1.0),\n",
    "          svm.SVC(kernel=\"poly\", degree=5, gamma=\"auto\", C=1.0)]\n",
    "\n",
    "# Feature labels\n",
    "xlabel, ylabel = [x.title().replace('Cm','CM') for x in iris.feature_names[:2]]\n",
    "\n",
    "# Fit models to data\n",
    "models = [clf.fit(X, y) for clf in models]\n",
    "\n",
    "# Titles for the plots\n",
    "titles = [\"LinearSVC (linear kernel)\",\n",
    "          \"SVC with RBF kernel\",\n",
    "          \"SVC with polynomial (degree 3) kernel\",\n",
    "          \"SVC with polynomial (degree 5) kernel\"]\n",
    "\n",
    "# Define colors and markers\n",
    "colors = [\"#f44336\", \"#40a347\", '#0086ff']\n",
    "edge_colors = [\"#cc180b\", \"#16791d\", '#11548f']\n",
    "markers = ['o', 's', 'd']\n",
    "cmap_ = ListedColormap(colors)\n",
    "\n",
    "# Create 2x2 grid for plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 9))\n",
    "\n",
    "# Create a dictionary to map target names to numbers\n",
    "target_names = {0: 'Setosa', 1: 'Versicolor', 2: 'Virginica'}\n",
    "\n",
    "# Plot decision boundaries\n",
    "for clf, title, ax in zip(models, titles, axes.flatten()):\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(clf, X, response_method=\"predict\", cmap=cmap_,\n",
    "                                                  alpha=0.2, ax=ax, xlabel=xlabel, ylabel=ylabel)\n",
    "\n",
    "    # Scatter plot of data points with target names\n",
    "    for num in np.unique(y):\n",
    "        ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                   s=40, ec=edge_colors[num], marker=markers[num], label=target_names[num])\n",
    "\n",
    "    ax.set_title(title, weight='bold')\n",
    "    ax.grid(False)\n",
    "    ax.legend(title = 'Flower Type', fontsize = 12)\n",
    "\n",
    "# Add a title\n",
    "plt.suptitle(\"SVM Classifier Comparison\", fontsize=16, fontweight='bold')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eeceec",
   "metadata": {
    "id": "92eeceec"
   },
   "source": [
    "## sklearn's  SVC and SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add4059",
   "metadata": {
    "id": "8add4059"
   },
   "source": [
    "Support Vector Machine (SVM), a prominent machine learning algorithm accessible within the scikit-learn (sklearn) library, exhibits versatility and finds utility in both classification and regression tasks [scikit-learn Developers, 2023]. Within the sklearn SVM module, two primary classes are available:\n",
    "\n",
    "* **Support Vector Classification (SVC):**\n",
    "  SVC specializes in addressing classification tasks. Its core aim is to identify the optimal hyperplane that effectively segregates data points into distinct classes, with the additional goal of maximizing the margin separating these classes. The definition of this hyperplane is influenced by a subset of data points referred to as support vectors. Furthermore, SVC can adeptly address both linear and non-linear classification challenges through the application of various kernel functions, including linear, polynomial, radial basis function (RBF), and sigmoid kernels [scikit-learn Developers, 2023].\n",
    "\n",
    "* **Support Vector Regression (SVR):**\n",
    "  In contrast to SVC, SVR is tailored to regression tasks. Instead of seeking a hyperplane for class separation, SVR aims to determine a hyperplane that minimizes the error between predicted values and actual target values, all while considering a predefined margin of tolerance. Similar to SVC, SVR possesses the capability to handle non-linear regression tasks through the use of kernel functions [scikit-learn Developers, 2023].\n",
    "\n",
    "The choice of kernel and hyperparameters is a pivotal decision, contingent on the data's characteristics and the specific problem at hand. Rigorous data preprocessing and comprehensive model evaluation are indispensable for achieving optimal performance within the context of academic or research endeavors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05d14f2",
   "metadata": {
    "id": "d05d14f2"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font>\n",
    "In this example, we focus on the Auto MPG dataset, which is sourced from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg). The aim is to demonstrate the use of Support Vector Regression (SVR) with this dataset.\n",
    "\n",
    "| **Feature**    | **Description**                                                                                                |\n",
    "|----------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| MPG            | Fuel efficiency in miles per gallon. Higher values indicate better fuel efficiency.                        |\n",
    "| Cylinders      | Number of engine cylinders, indicating engine capacity and power. Common values: 4, 6, and 8 cylinders.    |\n",
    "| Displacement   | Engine volume in cubic inches or cubic centimeters, reflecting engine size and power. Higher values mean more power. |\n",
    "| Horsepower     | Engine horsepower, measuring its ability to perform work. Higher values indicate a more powerful engine. |\n",
    "| Weight         | Vehicle mass in pounds or kilograms, influencing fuel efficiency. Lighter vehicles tend to have better MPG. |\n",
    "| Acceleration   | Vehicle's acceleration performance, usually measured in seconds to reach 60 mph (or 100 km/h) from a standstill. |\n",
    "| Model Year     | Year of vehicle manufacturing, useful for tracking technology and efficiency trends. |\n",
    "| Origin         | Country or region of vehicle origin, often a categorical variable. Values: 1 (USA), 2 (Germany), 3 (Japan), and more. |\n",
    "| Car Name       | Name or model of the car, useful for identification and categorization of different car models. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c-gC39t7oCYz",
   "metadata": {
    "id": "c-gC39t7oCYz"
   },
   "outputs": [],
   "source": [
    "# Download the zip file using wget\n",
    "!wget -N \"http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\"\n",
    "\n",
    "# Unzip the downloaded zip file\n",
    "!unzip -o auto+mpg.zip auto-mpg.data\n",
    "\n",
    "# Remove the downloaded zip file after extraction\n",
    "!rm -r auto+mpg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae325fc",
   "metadata": {
    "id": "aae325fc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n",
    "\n",
    "# Define column names based on the dataset's description\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model_Year', 'Origin', 'Car_Name']\n",
    "\n",
    "# Read the dataset with column names, treating '?' as missing values, and remove rows with missing values\n",
    "auto_mpg_df = pd.read_csv('auto-mpg.data', names=column_names,\n",
    "                          na_values='?', delim_whitespace=True).dropna()\n",
    "\n",
    "# Remove the 'Car_Name' column from the DataFrame\n",
    "auto_mpg_df = auto_mpg_df.drop(columns=['Car_Name']).reset_index(drop= True)\n",
    "\n",
    "auto_mpg_df['MPG'] = np.log(auto_mpg_df['MPG'])\n",
    "auto_mpg_df.rename(columns = {'MPG' : 'ln(MPG)'}, inplace = True)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(auto_mpg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782d39b",
   "metadata": {
    "id": "b782d39b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "\n",
    "# Prepare the data\n",
    "X = auto_mpg_df.drop('ln(MPG)', axis=1)  # Features\n",
    "y = auto_mpg_df['ln(MPG)']  # Target variable\n",
    "\n",
    "# Initialize KFold cross-validator\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state= 0)\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_r2_scores, test_r2_scores, train_MSE_scores, test_MSE_scores = [], [], [], []\n",
    "\n",
    "svr = svm.SVR(kernel='rbf')  # Create a Support Vector Regression model with an RBF kernel\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "    \n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;34m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "    \n",
    "# Perform Cross-Validation\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]  # Extract training and testing subsets by index\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    svr.fit(X_train, y_train)  # Train the SVR model\n",
    "    # train\n",
    "    y_train_pred = svr.predict(X_train)\n",
    "    train_r2_scores.append(metrics.r2_score(y_train, y_train_pred))\n",
    "    train_MSE_scores.append(metrics.mean_squared_error(y_train, y_train_pred))\n",
    "    # test\n",
    "    y_test_pred = svr.predict(X_test)\n",
    "    test_r2_scores.append(metrics.r2_score(y_test, y_test_pred))\n",
    "    test_MSE_scores.append(metrics.mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain R-Squared Score = {train_r2_scores[fold]:.4f}, Test R-Squared Score = {test_r2_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain MSE Score = {train_MSE_scores[fold]:.4f}, Test MSE Score= {test_MSE_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('R-Squared Score:')\n",
    "print(f\"\\tMean Train R-Squared Score: {np.mean(train_r2_scores):.4f} ± {np.std(train_r2_scores):.4f}\")\n",
    "print(f\"\\tMean Test R-Squared Score: {np.mean(test_r2_scores):.4f} ± {np.std(test_r2_scores):.4f}\")\n",
    "print_bold('MSE Score:')\n",
    "print(f\"\\tMean MSE Accuracy Score: {np.mean(train_MSE_scores):.4f} ± {np.std(train_MSE_scores):.4f}\")\n",
    "print(f\"\\tMean MSE Accuracy Score: {np.mean(test_MSE_scores):.4f} ± {np.std(test_MSE_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e033e",
   "metadata": {
    "id": "5c3e033e"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> Here, we have a code example that demonstrates the use of a Decision Tree Classifier to visualize decision boundaries on synthetic data. This synthetic dataset is created using scikit-learn's `make_blobs` function, specifically designed for generating artificial datasets for various machine learning experiments. This particular dataset has the following characteristics:\n",
    "\n",
    "- **Number of Samples:** 2000\n",
    "- **Number of Features:** 2\n",
    "- **Number of Classes:** 4\n",
    "- **Random Seed (random_state):** 0\n",
    "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
    "\n",
    "**Features:**\n",
    "- Within the dataset, there are 2000 data points, each characterized by a pair of feature values, denoted as 'Feature 1' and 'Feature 2'.\n",
    "\n",
    "**Target Variable:**\n",
    "- The dataset also includes a target variable named 'Outcome.' This variable assigns each data point to one of four distinct classes, labeled as 'Class 0', 'Class 1', 'Class 2', and 'Class 3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18200902",
   "metadata": {
    "id": "18200902"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\", '#B2FF66', '#0096ff']\n",
    "edge_colors = ['#8A0002', '#3C1F8B', '#6A993D', '#2e658c']\n",
    "markers = ['o', 's', 'd', '*']\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=2000, centers=4, random_state=0, cluster_std=1.0)\n",
    "\n",
    "\n",
    "# Create a scatter plot using Seaborn\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9.5, 7))\n",
    "\n",
    "for num in np.unique(y):\n",
    "    ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "               s=40, ec=edge_colors[num], marker=markers[num], label=str(num))\n",
    "\n",
    "ax.set_title(title, weight='bold')\n",
    "ax.grid(True)\n",
    "ax.legend(title='Class', fontsize=14)\n",
    "\n",
    "ax.set(xlim=[-6, 6], ylim=[-2, 12])\n",
    "ax.set_title('Synthetic Dataset', weight = 'bold', fontsize = 16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b1f86a",
   "metadata": {
    "id": "64b1f86a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "    \n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;31m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "    \n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# The splitt would be 80-20!\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "train_class_proportions, test_class_proportions = [], []\n",
    "\n",
    "cls = SVC(kernel = 'rbf')\n",
    "    \n",
    "# Perform Cross-Validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    cls.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate class proportions for train and test sets\n",
    "    train_class_proportions.append([np.mean(y_train == cls) for cls in np.unique(y)])\n",
    "    test_class_proportions.append([np.mean(y_test == cls) for cls in np.unique(y)])\n",
    "    \n",
    "    # train\n",
    "    y_train_pred = cls.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "    \n",
    "    # test\n",
    "    y_test_pred = cls.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
    "    print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
