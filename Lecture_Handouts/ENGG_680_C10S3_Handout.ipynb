{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249b9789",
   "metadata": {
    "id": "249b9789"
   },
   "source": [
    "# Classification Trees\n",
    "\n",
    "A classification tree, similar to a regression tree, is a predictive model used to handle **categorical** responses rather than **continuous** ones. In a regression tree, the predicted response for an observation is the mean response of the training observations belonging to the same terminal node. In contrast, for a classification tree, we predict that each observation belongs to the most frequently occurring class among the training observations within its corresponding region. When interpreting the results of a classification tree, we're not only interested in the predicted class for a specific terminal node region but also in the proportions of classes among the training observations falling into that region [James et al., 2023].\n",
    "\n",
    "The process of developing a classification tree closely mirrors the approach used for constructing a regression tree. We utilize a technique called **recursive binary splitting** to build the classification tree. This technique involves repeatedly splitting the predictor space into two regions, such that the observations within each region are as homogeneous as possible with respect to the response variable. The splitting process continues until a stopping criterion is reached, such as a minimum node size or a maximum tree depth. Recursive binary splitting is simple and efficient, but it can also lead to overfitting, which means that the tree may not generalize well to new data. To avoid overfitting, some methods such as pruning, cross-validation, or regularization can be applied to reduce the complexity of the tree [James et al., 2023].\n",
    "\n",
    "However, when dealing with classification tasks, the conventional criterion used in regression trees, known as the Residual Sum of Squares (RSS), is unsuitable for guiding binary split decisions. Instead, a more appropriate option is the **classification error rate**. The fundamental goal is to assign an observation within a specific region to the class that appears most frequently among the training observations in that same region. The classification error rate quantifies the proportion of training observations within that region which do not align with the most prevalent class [James et al., 2023].\n",
    "\n",
    "Mathematically, the classification error rate ($E$) is defined as [scikit-learn Developers, 2023]:\n",
    "\\begin{equation}\n",
    "E = 1 − \\max_k (\\hat{p}_{mk}),\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "- $E$ stands for the classification error rate.\n",
    "- $\\max_k$ signifies the maximum value taken over different classes ($k$).\n",
    "- $\\hat{p}_{mk}$ represents the fraction of training observations in the mth region that belong to the kth class.\n",
    "\n",
    "However, it's worth noting that the classification error rate is found to lack the desired level of sensitivity for effective tree growth. As a result, practical experience has shown that two alternative measures are more advantageous in the tree-building process. These are the **Gini index** and the **cross-entropy**, which are both based on the concept of **impurity**. Impurity measures how often a randomly chosen observation from a region would be incorrectly classified. The Gini index and the cross-entropy are more sensitive to changes in the node probabilities than the classification error rate, and therefore lead to better splits [James et al., 2023].\n",
    "\n",
    "The Gini index is formulated as follows [scikit-learn Developers, 2023]:\n",
    "\n",
    "\\begin{equation}\n",
    "G = \\sum_{k = 1}^{K} \\hat{p}_{mk}(1 - \\hat{p}_{mk}).\n",
    "\\end{equation}\n",
    "\n",
    "This index serves as a metric to gauge the overall variance among the K classes. Its calculation involves considering the proportions ($\\hat{p}_{mk}$) of training observations belonging to the kth class in a particular region. A crucial point to note is that the Gini index assumes lower values when the $\\hat{p}_{mk}$ values are closer to either zero or one. This characteristic lends the Gini index its characterization as a measure of node purity. Specifically, when the Gini index is small, it indicates that a node primarily comprises observations from a single class [James et al., 2023].\n",
    "\n",
    "An alternative approach to quantify node impurity is through the use of entropy, which is defined by the formula [scikit-learn Developers, 2023]:\n",
    "\n",
    "\\begin{equation}\n",
    "D = − \\sum_{k = 1}^{K} \\hat{p}_{mk} \\log\\hat{p}_{mk},\n",
    "\\end{equation}\n",
    "\n",
    "Here, the entropy measure captures the information content within a node by considering the proportions ($\\hat{p}_{mk}$) of training observations belonging to the kth class in that specific node. Since the range of values for $\\hat{p}_{mk}$ lies between 0 and 1, the term $\\hat{p}_{mk} \\log\\hat{p}_{mk}$ is non-negative ($0 \\leq \\hat{p}_{mk} \\log\\hat{p}_{mk}$). The entropy value tends to approach zero when the $\\hat{p}_{mk}$ values are close to either zero or one. Hence, just like the Gini index, low entropy signifies that the node primarily comprises observations from a single class, implying node purity. In fact, it's interesting to note that the Gini index and entropy are quite similar, and in practice, they tend to produce similar results [James et al., 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb696282",
   "metadata": {
    "id": "bb696282"
   },
   "source": [
    "## DecisionTreeClassifier algorithm in scikit-learn\n",
    "\n",
    "The DecisionTreeClassifier in scikit-learn (sklearn) is a part of the machine learning library that implements decision tree-based classification algorithms. The classifier is based on the concept of **recursive binary splitting** of the feature space into regions that correspond to different class labels. This concept means that the algorithm repeatedly divides the data into two subsets based on a feature and a threshold, such that the observations within each subset are as homogeneous as possible with respect to the response variable. The mathematical formulation of the DecisionTreeClassifier involves several components [Breiman, 2017, Géron, 2022, James et al., 2023, scikit-learn Developers, 2023]:\n",
    "\n",
    "\n",
    "***\n",
    "**DecisionTreeClassifier Algorithm**\n",
    "\n",
    "1. **Objective Function:** The primary goal of the DecisionTreeClassifier is to create a tree that effectively partitions the feature space, maximizing classification accuracy. It does this by recursively finding the optimal features and thresholds for splitting the data.\n",
    "\n",
    "2. **Splitting Criterion:** At each internal node of the tree, the algorithm chooses a feature ($x_i$) and a threshold ($t$) to split the data into two subsets: $D_{\\text{left}}$ and $D_{\\text{right}}$. The split is determined by minimizing a chosen **impurity measure**, which can be one of the following:\n",
    "     - **Gini impurity** (commonly used in scikit-learn):\n",
    "       \\begin{equation} \\text{Gini}(D) = 1 - \\sum_{k=1}^{K} p_k^2 \\end{equation}\n",
    "     - **Entropy**:\n",
    "       \\begin{equation} \\text{Entropy}(D) = -\\sum_{k=1}^{K} p_k \\log_2(p_k) \\end{equation}\n",
    "     - **Classification error** (misclassification error):\n",
    "       \\begin{equation} \\text{Classification Error}(D) = 1 - \\max_k(p_k) \\end{equation}\n",
    "    where $p_k$ is the proportion of samples of class $k$ in the node. The impurity measure quantifies how often a randomly chosen observation from the node would be incorrectly classified. A lower impurity value indicates a higher purity of the node, meaning that most of the samples in the node belong to the same class.\n",
    "\n",
    "3. **Recursive Splitting:** The data is recursively split into child nodes, applying the splitting criterion, until a **stopping criterion** is met. Common stopping criteria include reaching a maximum depth, having a minimum number of samples in a node, or having impurity below a certain threshold. These criteria help to prevent overfitting, which means that the tree may not generalize well to new data.\n",
    "\n",
    "4. **Leaf Nodes:** When a stopping criterion is reached, a leaf node is created. This leaf node represents a class label prediction. The predicted class is often determined by the **majority class** of samples in that node.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c39598b",
   "metadata": {
    "id": "1c39598b"
   },
   "source": [
    "## Example: Synthetic Dataset with Two Classes\n",
    "\n",
    "<font color='Blue'><b>Example</b></font>: In this code example, we use a Decision Tree Classifier to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the `make_blobs` function from scikit-learn¹, which creates artificial datasets for various machine learning experiments. This particular dataset has the following characteristics:\n",
    "\n",
    "- **Number of Samples:** 1000\n",
    "- **Number of Features:** 2\n",
    "- **Number of Classes:** 2\n",
    "- **Random Seed (random_state):** 0\n",
    "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
    "\n",
    "**Features:**\n",
    "- The dataset contains 1000 data points, each described by two feature values. These features are labeled as 'Feature 1' and 'Feature 2'.\n",
    "\n",
    "**Outcome (Target Variable):**\n",
    "- The dataset also includes a target variable called 'Outcome.' This variable assigns each data point to one of two classes, labeled as 'Class 0' and 'Class 1'.\n",
    "\n",
    "The dataset simulates a scenario with two well-separated clusters, making it suitable for binary classification tasks. Each data point in this dataset belongs to one of the two classes, and it can be used to practice and evaluate machine learning algorithms that deal with binary classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc67762",
   "metadata": {
    "id": "1cc67762"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\"]\n",
    "edge_colors = ['#8A0002', '#3C1F8B']\n",
    "cmap_light = ListedColormap(['#f7dfdf', '#e3d3f2'])\n",
    "markers = ['o', 's']\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=1000, centers=2, random_state=0, cluster_std=1.0)\n",
    "\n",
    "# Create a scatter plot using Matplotlib\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9.5, 6), gridspec_kw={'width_ratios': [8, 1]})\n",
    "\n",
    "for num in np.unique(y):\n",
    "    ax[0].scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "               s=40, ec=edge_colors[num], marker=markers[num], label=str(num))\n",
    "\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(title='Class', fontsize=14)\n",
    "ax[0].set(xlim=[-2, 6], ylim=[-2, 8], xlabel='Feature 1', ylabel='Feature 2')\n",
    "ax[0].set_title('Synthetic Dataset', weight = 'bold', fontsize = 16, y = 1.02)\n",
    "\n",
    "bar_heights, bar_labels = np.unique(y, return_counts=True)\n",
    "bars = ax[1].bar(bar_heights, bar_labels, color=colors, edgecolor='k')\n",
    "\n",
    "# Add xticks with labels\n",
    "ax[1].set_xticks(bar_heights)\n",
    "ax[1].set_xticklabels([str(x) for x in np.unique(y)])\n",
    "\n",
    "ax[1].grid(which='major', axis='x')\n",
    "ax[1].set_title('Class Distribution', weight='bold', fontsize=16, y = 1.02)\n",
    "\n",
    "# Add labels for bar heights inside each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax[1].annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords='offset points', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e7a9d",
   "metadata": {
    "id": "d23e7a9d"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 9))\n",
    "axes = axes.ravel()\n",
    "for i, (ax, m, alph) in enumerate(zip(axes, [10, 50, 200, None], 'abcd')):\n",
    "    dts = DecisionTreeClassifier(criterion='gini', splitter='best', max_leaf_nodes=m)\n",
    "    dts.fit(X, y)\n",
    "    display = DecisionBoundaryDisplay.from_estimator(dts, X=X, cmap=cmap_light, ax=ax,\n",
    "                                                     response_method=\"predict\",\n",
    "                                                     plot_method=\"pcolormesh\",\n",
    "                                                     xlabel='Feature 1', ylabel='Feature 2',\n",
    "                                                     shading=\"auto\")\n",
    "    for num in np.unique(y):\n",
    "        ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                   s=40, edgecolors=edge_colors[num], marker=markers[num], label=str(num))\n",
    "\n",
    "    ax.set(xlim=[-2, 6], ylim=[-2, 8])\n",
    "    ax.legend()\n",
    "    ax.set_title(f'({alph}) max_leaf_nodes = {m}', weight = 'bold')\n",
    "    ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495607c",
   "metadata": {
    "id": "8495607c"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dts = DecisionTreeClassifier(max_depth = None, max_leaf_nodes=3, max_features=3)\n",
    "dts.fit(X, y)\n",
    "\n",
    "feature_names = [f'Feature_{i + 1}' for i in range(2)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9.5, 3.5))\n",
    "_ = tree.plot_tree(dts, ax = ax,\n",
    "                   filled = True,\n",
    "                   node_ids=True,\n",
    "                   feature_names = feature_names,\n",
    "                   proportion = True,\n",
    "                   fontsize = 12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e4e02",
   "metadata": {
    "id": "460e4e02"
   },
   "outputs": [],
   "source": [
    "print(tree.export_text(dts, feature_names = [f'Feature_{i + 1}' for i in range(2)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9ee79",
   "metadata": {
    "id": "fcd9ee79"
   },
   "source": [
    "The following figure was generated utilizing  [dtreeviz](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/dts_fig01.png\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual representation of the above Decision Tree Classifier.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ab9e0",
   "metadata": {
    "id": "d13ab9e0"
   },
   "source": [
    "## Synthetic Dataset with Three Classes\n",
    "\n",
    "<font color='Blue'><b>Example</b></font>:  In this code example, we use a Decision Tree Classifier to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the `make_blobs` function from scikit-learn¹, which creates artificial datasets for various machine learning experiments. This particular dataset has the following characteristics:\n",
    "\n",
    "- **Number of Samples:** 2000\n",
    "- **Number of Features:** 2\n",
    "- **Number of Classes:** 3\n",
    "- **Random Seed (random_state):** 0\n",
    "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
    "\n",
    "**Features:**\n",
    "- The dataset contains 2000 data points, each described by two feature values. These features are labeled as 'Feature 1' and 'Feature 2'.\n",
    "\n",
    "**Outcome (Target Variable):**\n",
    "- The dataset also includes a target variable called 'Outcome.' This variable assigns each data point to one of three classes, labeled as 'Class 0,' 'Class 1,' and 'Class 2'.\n",
    "\n",
    "**Colormap:**\n",
    "- To facilitate class differentiation in the visualization, we use a vibrant colormap for the scatter plot. It displays three distinct colors: 'Red,' 'Blue,' and 'Green.'\n",
    "\n",
    "The dataset simulates a scenario with three well-separated clusters, making it suitable for multi-class classification tasks. Each data point in this dataset belongs to one of the three classes, and it can be used to practice and evaluate machine learning algorithms that deal with multi-class classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9017b3e0",
   "metadata": {
    "id": "9017b3e0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"#f5645a\", \"#0096ff\", '#B2FF66']\n",
    "edge_colors = ['#8A0002', '#2e658c', '#6A993D']\n",
    "markers = ['o', '*', 's']\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=2000, centers=3, random_state=0, cluster_std=1.0)\n",
    "\n",
    "# Create a scatter plot using Seaborn\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9.5, 7), gridspec_kw={'width_ratios': [8, 1]})\n",
    "\n",
    "for num in np.unique(y):\n",
    "    ax[0].scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "               s=40, ec=edge_colors[num], marker=markers[num], label=str(num))\n",
    "\n",
    "ax[0].grid(True)\n",
    "ax[0].legend(title='Class', fontsize=14)\n",
    "ax[0].set(xlim = [-6, 6], ylim = [-2, 8])\n",
    "ax[0].set_title('Synthetic Dataset', weight = 'bold', fontsize = 16, y = 1.02)\n",
    "\n",
    "bar_heights, bar_labels = np.unique(y, return_counts=True)\n",
    "bars = ax[1].bar(bar_heights, bar_labels, color=colors, edgecolor='k')\n",
    "\n",
    "# Add xticks with labels\n",
    "ax[1].set_xticks(bar_heights)\n",
    "ax[1].set_xticklabels(['0', '1', '2'])\n",
    "\n",
    "ax[1].grid(which='major', axis='x')\n",
    "ax[1].set_title('Class Distribution', weight='bold', fontsize=16, y = 1.02)\n",
    "\n",
    "# Add labels for bar heights inside each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax[1].annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                   xytext=(0, 3), textcoords='offset points', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d0007",
   "metadata": {
    "id": "ed6d0007"
   },
   "source": [
    "The classifier is trained with varying levels of maximum leaf nodes (10, 50, 200, and None) to demonstrate the impact of this parameter on the decision boundaries. The resulting plots are organized in a 2x2 grid to allow easy comparison. The decision boundaries are displayed using a light colormap, while the scatterplot of the synthetic data is overlaid for context. The x and y axes are labeled as 'Feature 1' and 'Feature 2', respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99340183",
   "metadata": {
    "id": "99340183"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "cmap_light = ListedColormap([\"MistyRose\", \"LightBlue\", \"HoneyDew\"])\n",
    "\n",
    "# Plot decision boundaries\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 9))\n",
    "axes = axes.ravel()\n",
    "for ax, m, alph in zip(axes, [10, 50, 200, None], 'abcd'):\n",
    "    dts = DecisionTreeClassifier(criterion = 'gini', splitter = 'best', max_leaf_nodes = m)\n",
    "    dts.fit(X, y)\n",
    "    DecisionBoundaryDisplay.from_estimator(dts, X, cmap=cmap_light, ax=ax,\n",
    "                                       response_method=\"predict\",\n",
    "                                       plot_method=\"pcolormesh\",\n",
    "                                       xlabel= 'Feature 1', ylabel='Feature 2',\n",
    "                                       shading=\"auto\")\n",
    "    for num in np.unique(y):\n",
    "        ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                   s=40, ec=edge_colors[num], marker=markers[num], label=str(num))\n",
    "    _ = ax.set(title = f'({alph}) max_leaf_nodes = {m}',  xlim = [-6, 6], ylim = [-2, 8])\n",
    "    _ = ax.grid(False)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b836e3",
   "metadata": {
    "id": "d1b836e3"
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dts = DecisionTreeClassifier(max_depth = None, max_leaf_nodes=4, max_features=4)\n",
    "dts.fit(X, y)\n",
    "\n",
    "feature_names = [f'Feature_{i + 1}' for i in range(2)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9.5, 3.5))\n",
    "_ = tree.plot_tree(dts, ax = ax,\n",
    "                   filled = True,\n",
    "                   node_ids=True,\n",
    "                   feature_names = feature_names,\n",
    "                   proportion = True,\n",
    "                   fontsize = 12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f90352",
   "metadata": {
    "id": "c7f90352"
   },
   "outputs": [],
   "source": [
    "print(tree.export_text(dts, feature_names = feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20872313",
   "metadata": {
    "id": "20872313"
   },
   "source": [
    "The following figure was generated utilizing  [dtreeviz](https://github.com/parrt/dtreeviz).\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/dts_fig02.png\" alt=\"picture\" width=\"850\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual representation of the above Decision Tree Classifier.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16bf2e0",
   "metadata": {
    "id": "d16bf2e0"
   },
   "source": [
    "## Optimizing DecisionTreeClassifier Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1586a6a5",
   "metadata": {
    "id": "1586a6a5"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> In our initial demonstration, we will use the previously synthesized dataset with three classes, employing the default settings of the DecisionTreeClassifier (DTS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3c6ffc",
   "metadata": {
    "id": "9c3c6ffc"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "dts = DecisionTreeClassifier()\n",
    "\n",
    "# Get the parameters of the DecisionTreeClassifier and print the parameters\n",
    "pprint(dts.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99eb27c",
   "metadata": {
    "id": "e99eb27c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;34m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "dts = DecisionTreeClassifier()\n",
    "\n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# The splitt would be 80-20!\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "train_class_proportions, test_class_proportions = [], []\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    dts.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate class proportions for train and test sets\n",
    "    train_class_proportions.append([np.mean(y_train == dts) for dts in np.unique(y)])\n",
    "    test_class_proportions.append([np.mean(y_test == dts) for dts in np.unique(y)])\n",
    "\n",
    "    # train\n",
    "    y_train_pred = dts.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "\n",
    "    # test\n",
    "    y_test_pred = dts.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
    "    print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072b180",
   "metadata": {
    "id": "2072b180"
   },
   "source": [
    "***\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "Optimizing and identifying the best results is not the goal of the final example.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b151e",
   "metadata": {
    "id": "541b151e"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> We will work with the previously generated synthetic dataset. However, we will introduce some modifications to the default settings of the DecisionTreeClassifier (DTS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e6f5e",
   "metadata": {
    "id": "db7e6f5e"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from pprint import pprint\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "dts = DecisionTreeClassifier(max_depth = None, max_leaf_nodes=6, max_features=6)\n",
    "\n",
    "# Get the parameters of the DecisionTreeClassifier and print the parameters\n",
    "pprint(dts.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a37e9d",
   "metadata": {
    "id": "b9a37e9d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;34m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "dts = DecisionTreeClassifier(max_depth = None, max_leaf_nodes=6, max_features=6)\n",
    "\n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# The splitt would be 80-20!\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "train_class_proportions, test_class_proportions = [], []\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    dts.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate class proportions for train and test sets\n",
    "    train_class_proportions.append([np.mean(y_train == dts) for dts in np.unique(y)])\n",
    "    test_class_proportions.append([np.mean(y_test == dts) for dts in np.unique(y)])\n",
    "\n",
    "    # train\n",
    "    y_train_pred = dts.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "\n",
    "    # test\n",
    "    y_test_pred = dts.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
    "    print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae01b0",
   "metadata": {
    "id": "2eae01b0"
   },
   "source": [
    "## GridSearchCV\n",
    "\n",
    "`GridSearchCV` is a method provided by the scikit-learn library in Python that performs **hyperparameter tuning** for machine learning models. Hyperparameters are parameters that are set before the training process begins and cannot be learned from the data. They significantly impact the performance and behavior of a machine learning model. `GridSearchCV` is a technique used to systematically search through a predefined set of hyperparameter values to find the combination that results in the best model performance [Pedregosa et al., 2011, scikit-learn Developers, 2023].\n",
    "\n",
    "Here's how `GridSearchCV` works:\n",
    "1. **Select Model and Hyperparameters:** You first choose the machine learning algorithm you want to use (e.g., Decision Tree, Random Forest, Support Vector Machine, etc.) and determine the hyperparameters associated with that algorithm that you want to tune. For example, for a Decision Tree, you may want to tune the maximum depth, the minimum number of samples per leaf, or the splitting criterion.\n",
    "\n",
    "2. **Define Parameter Grid:** You create a dictionary where the keys are the names of the hyperparameters you want to tune, and the values are lists of possible values for those hyperparameters. This forms a grid of all possible combinations. For example, for a Decision Tree, you may define the parameter grid as follows:\n",
    "\n",
    "    ```python\n",
    "    param_grid = {'max_depth': [3, 5, 7, 9],\n",
    "                  'min_samples_leaf': [1, 2, 4, 8],\n",
    "                  'criterion': ['gini', 'entropy']}\n",
    "    ```\n",
    "\n",
    "3. **Cross-Validation:** `GridSearchCV` uses cross-validation to evaluate the performance of each combination of hyperparameters. Cross-validation involves splitting your dataset into multiple subsets (folds), training the model on a subset of the data, and validating it on the remaining subset. This process is repeated for each fold, and the results are averaged to provide a more robust evaluation. For example, you may use a 5-fold cross-validation, which means that your dataset is divided into 5 parts, and each part is used as a validation set once, while the other 4 parts are used as a training set.\n",
    "\n",
    "4. **Evaluation:** For each combination of hyperparameters, `GridSearchCV` trains the model using cross-validation and calculates a performance metric (such as accuracy, F1-score, etc.) for each fold. The average performance across all folds is then used as the evaluation metric for that combination. For example, you may use accuracy as the performance metric, which means that you measure how many predictions are correct out of the total number of predictions.\n",
    "\n",
    "5. **Best Hyperparameters:** After evaluating all combinations, `GridSearchCV` identifies the combination of hyperparameters that resulted in the best performance metric. For example, you may find that the best combination of hyperparameters for a Decision Tree is:\n",
    "\n",
    "    ```python\n",
    "    best_params = {'max_depth': 7,\n",
    "                   'min_samples_leaf': 2,\n",
    "                   'criterion': 'gini'}\n",
    "    ```\n",
    "\n",
    "6. **Retrain and Test:** Once the best hyperparameters are found, you can retrain your model using the entire training dataset and these optimized hyperparameters. Then, you can test the final model on an independent test dataset to assess its generalization performance. For example, you may use a separate test set that was not used during the cross-validation process, and measure the accuracy of the final model on this test set.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2375e",
   "metadata": {
    "id": "57e2375e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# Define the hyperparameters to tune\n",
    "param_grid = {\n",
    "    'max_depth': [None, 1, 2, 3, 5, 20],\n",
    "    'max_leaf_nodes': [None, 5, 8, 10, 15, 20],\n",
    "    'max_features': [None, 2, 3]\n",
    "}\n",
    "\n",
    "# Create a custom scoring function for F1 score in a multiclass setting\n",
    "scoring = metrics.make_scorer(metrics.f1_score, average='weighted')\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=DecisionTreeClassifier(),\n",
    "                           param_grid=param_grid,\n",
    "                           scoring = scoring,\n",
    "                           cv=5)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print best hyperparameters and corresponding score\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f\"Best Score = {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e714d8c8",
   "metadata": {
    "id": "e714d8c8"
   },
   "source": [
    "Now, with the above parameters, we have,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c50360",
   "metadata": {
    "id": "53c50360"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;34m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "\n",
    "# Create a DecisionTreeClassifier instance\n",
    "dts = DecisionTreeClassifier(**best_params)\n",
    "\n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "# The splitt would be 80-20!\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "train_class_proportions, test_class_proportions = [], []\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    dts.fit(X_train, y_train)\n",
    "\n",
    "    # Calculate class proportions for train and test sets\n",
    "    train_class_proportions.append([np.mean(y_train == dts) for dts in np.unique(y)])\n",
    "    test_class_proportions.append([np.mean(y_test == dts) for dts in np.unique(y)])\n",
    "\n",
    "    # train\n",
    "    y_train_pred = dts.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "\n",
    "    # test\n",
    "    y_test_pred = dts.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Class Proportions: {train_class_proportions[fold]}*{len(y_train)}\")\n",
    "    print(f\"\\tTest Class Proportions: {test_class_proportions[fold]}*{len(y_test)}\")\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aac8c8",
   "metadata": {
    "id": "41aac8c8"
   },
   "source": [
    "The tree for the above model can be represented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058ee28",
   "metadata": {
    "id": "6058ee28"
   },
   "outputs": [],
   "source": [
    "print(tree.export_text(dts, feature_names =[f'Feature_{i + 1}' for i in range(2)]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
