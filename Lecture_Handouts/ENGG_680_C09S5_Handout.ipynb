{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7DRjbmKdkMT"
   },
   "source": [
    "# K-Nearest Neighbors (K-NN)\n",
    "\n",
    "K-Nearest Neighbors (K-NN) is a straightforward and intuitive machine learning algorithm employed for classification and regression tasks. This approach belongs to the instance-based learning category, where predictions are made by considering the \"k\" nearest data points to the input point. Let's delve into the mathematical foundations of the K-NN algorithm:\n",
    "\n",
    "1. **Assumption: Similar Inputs have similar outputs:**\n",
    "   The cornerstone of the K-NN algorithm is the assumption that data points with analogous features (inputs) tend to share similar labels (outputs). This notion underpins the K-NN approach to classify a new test input by examining the labels of its k most similar training inputs.\n",
    "   \n",
    "2. **Classification rule: For a test input x, assign the most common label amongst its k most similar training inputs:**\n",
    "   When faced with a fresh test input (denoted as x), the K-NN algorithm identifies the k training inputs (neighbors) that closely resemble x using a specified distance metric. It then designates the label that emerges most frequently among these k neighbors as the anticipated label for the test input x.\n",
    "\n",
    "3. **Formal definition of k-NN (in terms of nearest neighbors):**\n",
    "   Let's dissect the formal definition of k-NN [Cover and Hart, 1967, Weinberger, 2022]:\n",
    "   - Test point: x\n",
    "   - The collection of k nearest neighbors of x is symbolized as $S_x$\n",
    "   - $S_x$ constitutes a subset of the training data $D$, with $|S_x| = k$\n",
    "   - For every pair (x', y') in the training data $D$ not within $S_x$, the distance from x to x' exceeds or equals the distance from x to the farthest point in $S_x$:\n",
    "   \\begin{equation}\n",
    "   \\text{dist}(\\mathbf{x},\\mathbf{x}')\\ge\\max_{(\\mathbf{x}'',y'')\\in S_\\mathbf{x}} \\text{dist}(\\mathbf{x},\\mathbf{x}'')\n",
    "   \\end{equation}\n",
    "   \n",
    "3. **Distance Metric:**\n",
    "   The initial step in the K-NN algorithm entails selecting a distance metric, which gauges the resemblance or distinction between data points.\n",
    "\n",
    "    1. **Euclidean Distance:**\n",
    "       Euclidean distance corresponds to the linear distance between two points in a plane or hyperplane. It equates to measuring the shortest path between these points as if drawing a straight line. This metric provides insight into the extent of displacement between two states of an object.\n",
    "\n",
    "       Formula:\n",
    "       \\begin{equation}\n",
    "       d(x, y) = \\sqrt{\\sum_{i=1}^{n}(x_i - y_i)^2}\n",
    "       \\end{equation}\n",
    "\n",
    "    2. **Manhattan Distance:**\n",
    "       The Manhattan distance is apt when total distance traveled by an object is of interest, irrespective of direction. It simulates calculating the distance covered when moving within a city grid pattern. It is computed by summing the absolute differences between coordinates of points in n-dimensional space.\n",
    "\n",
    "       Formula:\n",
    "       \\begin{equation}\n",
    "       d(x, y) = \\sum_{i=1}^{n}|x_i - y_i|\n",
    "       \\end{equation}\n",
    "\n",
    "    3. **Minkowski Distance:**\n",
    "       The Minkowski distance generalizes the Euclidean and Manhattan distances as special cases. The parameter \"p\" shapes the distance metric. With p = 1, it's akin to the Manhattan distance, and p = 2 corresponds to the Euclidean distance.\n",
    "\n",
    "       Formula:\n",
    "       \\begin{equation}\n",
    "       d(x, y) = \\left(\\sum_{i=1}^{n}|x_i - y_i|^p\\right)^{\\frac{1}{p}}\n",
    "       \\end{equation}\n",
    "\n",
    "4. **Training:**\n",
    "   During training, the algorithm stores feature vectors along with their corresponding class labels from the training dataset.\n",
    "\n",
    "5. **Prediction:**\n",
    "   In making predictions for new data points, K-NN follows these steps:\n",
    "   \n",
    "   a. **Calculate Distances:**\n",
    "      Compute the distance between the new data point and all training data points using the chosen distance metric.\n",
    "   \n",
    "   b. **Find K Neighbors:**\n",
    "      Identify the \"k\" nearest neighbors based on the computed distances.\n",
    "   \n",
    "   c. **Majority Voting (Classification) or Average (Regression):**\n",
    "      For classification tasks, the algorithm tallies the occurrences of each class among the \"k\" neighbors and designates the class with the highest count as the predicted class. For regression tasks, the algorithm computes the average of the target values of the \"k\" neighbors to predict the value.\n",
    "\n",
    "6. **Choosing the Value of K:**\n",
    "   Selecting the appropriate \"k\" is pivotal. A smaller \"k\" might yield noisy predictions, while a larger \"k\" could smooth decision boundaries. The optimal \"k\" value hinges on the dataset and the specific problem.\n",
    "\n",
    "7. **Weighted K-NN (Optional):**\n",
    "   Weighted K-NN can be employed to grant greater influence to closer neighbors than those farther away. This is accomplished by assigning weights to neighbors inversely proportional to their distances.\n",
    "\n",
    "8. **Normalization and Scaling (Optional):**\n",
    "   To prevent dominance by individual features in distance calculations, normalizing or scaling the features is recommended.\n",
    "\n",
    "It's important to note that K-NN's simplicity and interpretability come with potential limitations, such as reduced efficacy on high-dimensional or feature-irrelevant data due to the curse of dimensionality. Moreover, computation can become intensive for large datasets because of the need to calculate distances for each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRqHoTuPdkMU"
   },
   "source": [
    "Explaining the Operation of the K-Nearest Neighbors Algorithm\n",
    "\n",
    "The K-Nearest Neighbors (K-NN) algorithm is a method used to classify a new data point within a dataset containing various classes or categories. It operates by assessing the proximity and similarity of the new data point to the existing data entries. This process is executed in the following sequential steps:\n",
    "\n",
    "1. **Determination of K-Value**: In the first step, a value is assigned to K, which signifies the number of neighboring data points that will be considered for classification.\n",
    "\n",
    "2. **Calculation of Distances**: Subsequently, the algorithm computes the distances between the new data point and all the data entries present in the dataset. The specific method for calculating these distances is elaborated upon in subsequent sections. These distances are then arranged in ascending order.\n",
    "\n",
    "3. **Identification of Nearest Neighbors**: The next step involves the selection of the K nearest neighbors based on the previously computed distances. These neighbors represent the existing data points in the dataset that are most closely related to the new data point.\n",
    "\n",
    "4. **Classification Assignment**: Finally, the new data point is classified into a specific class or category based on the majority class among its K nearest neighbors. In essence, the class that is most prevalent among these neighbors determines the classification of the new data point.\n",
    "\n",
    "This method provides a clear and structured approach for understanding the functioning of the K-Nearest Neighbors algorithm.\n",
    "\n",
    "<font color='Blue'><b>Example:</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHfUaNx4dkMU"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/KKN_Example.jpg\" alt=\"picture\" width=\"700\">\n",
    "<br>\n",
    "<b>Figure</b>: An example of using KKN.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb16Lbc1dkMV"
   },
   "source": [
    "* Panel (a) illustrates a dataset comprising three distinct classes, namely red, green, and blue.\n",
    "\n",
    "* Moving to Panel (b), a pivotal step is introduced: the assignment of a value to K, which signifies the quantity of neighboring data points to take into account before categorizing the new data entry. For the sake of this explanation, let us assume a value of K equal to 3.\n",
    "\n",
    "* Continuing in Panel (b), out of the three nearest neighbors depicted in the diagram above, the majority class identified is red. Consequently, the new data entry is designated to belong to the red class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_ljCQj-dkMV"
   },
   "source": [
    "We can define a simplified KKN classified as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKLlTKEqdkMV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3):\n",
    "        \"\"\"\n",
    "        K-Nearest Neighbors Classifier.\n",
    "\n",
    "        Parameters:\n",
    "        k (int): Number of neighbors to consider for classification.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the KNN model on the training data.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): Training feature data.\n",
    "        y (numpy.ndarray): Training labels.\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict labels for input data.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): Input feature data.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: Predicted labels for input data.\n",
    "        \"\"\"\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict a single label for an input data point.\n",
    "\n",
    "        Parameters:\n",
    "        x (numpy.ndarray): Input feature data point.\n",
    "\n",
    "        Returns:\n",
    "        int: Predicted label for the input data point.\n",
    "        \"\"\"\n",
    "        # This is Euclidean Distance\n",
    "        distances = [np.linalg.norm(x - x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = np.bincount(k_nearest_labels).argmax()\n",
    "        return most_common\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clv0jMUhdkMV"
   },
   "source": [
    "Let's explain the above code:\n",
    "\n",
    "1. **Initialization:**\n",
    "   The `KNNClassifier` class is initialized with a parameter `k`, which specifies the number of neighbors to consider for classification.\n",
    "\n",
    "2. **Training (fit method):**\n",
    "   In the `fit` method, the model is trained on a labeled dataset. The training feature data is stored in `self.X_train`, and the corresponding labels are stored in `self.y_train`.\n",
    "\n",
    "3. **Prediction (predict method):**\n",
    "   In the `predict` method, predictions are made for a set of input feature data points (`X`). For each data point `x` in `X`, the `_predict` method is called to predict its label.\n",
    "\n",
    "4. **Individual Prediction (_predict method):**\n",
    "   The `_predict` method is where the core prediction happens for a single data point `x`. Here's how it works:\n",
    "\n",
    "   a. **Calculate Distances:**\n",
    "      - For the input data point `x`, the Euclidean distance is computed to all data points in the training set `self.X_train`. The distances are stored in the `distances` list, which contains the Euclidean distance between `x` and each point in the training set.\n",
    "\n",
    "   b. **Find Nearest Neighbors:**\n",
    "      - The indices of the `k` data points with the smallest distances are determined using `np.argsort(distances)[:self.k]`. These indices represent the `k` nearest neighbors.\n",
    "\n",
    "   c. **Count Labels:**\n",
    "      - The labels of these `k` nearest neighbors are collected from the `self.y_train` array.\n",
    "\n",
    "   d. **Majority Vote:**\n",
    "      - Finally, a majority vote is performed among the labels of the `k` nearest neighbors. The label that occurs most frequently is selected as the predicted label for the input data point `x`.\n",
    "\n",
    "5. **Predict for All Data Points:**\n",
    "   In the `predict` method, this process is repeated for all data points in the input feature data `X`, and the predicted labels are stored in the `y_pred` list.\n",
    "\n",
    "6. **Return Predictions:**\n",
    "   The predicted labels for all input data points are returned as a NumPy array.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_c6M-Mi6dkMW"
   },
   "source": [
    "For practical implementation, the widely-used [**KNeighborsClassifier**](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) class from scikit-learn is a powerful tool. This class simplifies the creation of a KNN classifier, its fitting to training data, and the making of predictions for new observations. It wraps the entire process described earlier and provides a range of options for customization, including distance metrics, weighting strategies, and more. Using this class, you can efficiently harness the capabilities of the K-Nearest Neighbors algorithm while enjoying the convenience of a well-designed interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MeECon3dkMW"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>: The **Iris Dataset** serves as a quintessential example in explaining cross-validations. Crafted by the eminent British biologist and statistician Ronald A. Fisher in 1936, the dataset finds its roots in the realm of discriminant analysis. Named after the Iris flower species it encapsulates, this dataset stands as a foundational cornerstone in machine learning and statistics.\n",
    "\n",
    "**Dataset Composition**\n",
    "\n",
    "This dataset encompasses measurements of four distinct attributes in three diverse species of Iris flowers:\n",
    "\n",
    "1. Sepal Length\n",
    "2. Sepal Width\n",
    "3. Petal Length\n",
    "4. Petal Width\n",
    "\n",
    "The dataset encapsulates three distinct species of Iris flowers:\n",
    "\n",
    "1. Iris setosa\n",
    "2. Iris versicolor\n",
    "3. Iris virginica\n",
    "\n",
    "Each Iris flower species is accompanied by precisely 50 samples, culminating in a total dataset size of 150 instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIGmgT2vdkMW"
   },
   "source": [
    "Before delving into that, let's first examine the distribution of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9M6wegndkMW"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "iris_df = pd.DataFrame(data=iris.data, columns= iris.feature_names)\n",
    "iris_df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "# Print the first few rows of the DataFrame\n",
    "display(iris_df.head())\n",
    "\n",
    "# Pairplot: Scatterplots for each pair of features\n",
    "g = sns.pairplot(iris_df, hue=\"species\", markers=[\"o\", \"s\", \"D\"], height = 2)\n",
    "# _ = g.map_lower(sns.kdeplot, levels=4, color=\".2\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a 2x2 grid of subplots with a specified figure size\n",
    "fig, ax = plt.subplots(2, 2, figsize=(9.5, 6))\n",
    "\n",
    "# Flatten the 2x2 grid into a 1D array of subplots\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Get the column names of the iris DataFrame, excluding the 'species' column\n",
    "Cols = iris_df.drop(columns=['species']).columns\n",
    "\n",
    "# Iterate through the columns using enumerate\n",
    "for c, col in enumerate(Cols):\n",
    "    # Create a violin plot using Seaborn\n",
    "    sns.violinplot(x=\"species\", y=col, data = iris_df, ax=ax[c])\n",
    "\n",
    "    # Set the title for the subplot\n",
    "    ax[c].set_title(f\"Violinplot of {col} by species\")\n",
    "\n",
    "# Adjust the layout of the subplots for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6w7oYWYdkMW"
   },
   "source": [
    "Classifying using our KKN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSOqOwUvdkMX"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import datasets, neighbors\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Select only the first two features for visualization\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "xlim = [3, 9]\n",
    "ylim = [1, 5.5]\n",
    "feature_1, feature_2 = np.meshgrid(np.linspace(xlim[0], xlim[1]),\n",
    "                                   np.linspace(ylim[0], ylim[1]))\n",
    "\n",
    "grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap([\"MistyRose\", \"LightBlue\", \"HoneyDew\"])\n",
    "cmap_bold = [\"Red\", \"Blue\", \"Green\"]\n",
    "\n",
    "# Different values of n_neighbors\n",
    "n_neighbors_values = [3, 5, 8, 12]\n",
    "\n",
    "# Create a 2x2 plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(9.5, 7))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, n_neighbors in zip(axes, n_neighbors_values):\n",
    "    # Create a K-Nearest Neighbors classifier\n",
    "    knn = KNNClassifier(k = n_neighbors)\n",
    "    knn.fit(X, y)\n",
    "    # Plot decision boundaries\n",
    "    y_pred = np.reshape(knn.predict(grid), feature_1.shape)\n",
    "    display_dbd = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred)\n",
    "    display_dbd.plot(ax = ax, cmap=cmap_light)\n",
    "\n",
    "    # Plot training points\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n",
    "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\", ax=ax)\n",
    "\n",
    "    # Set the title\n",
    "    _ = ax.set(xlim = xlim, ylim = ylim,\n",
    "               xlabel=iris.feature_names[0],\n",
    "               ylabel=iris.feature_names[1],\n",
    "               title = \"KKN classification (k = %i)\" % n_neighbors)\n",
    "\n",
    "# Adjust the layout of the subplots for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVmF_UPFdkMX"
   },
   "source": [
    "We can apply the sklearn's k-Nearest Neighbors (k-NN) algorithm to the Iris dataset, experimenting with different numbers of neighbors: 3, 5, 8, 12, 15, and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMP966HSdkMX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Different values of n_neighbors\n",
    "n_neighbors_values = [3, 5, 8, 12, 15, 20]\n",
    "\n",
    "# Create a 2x2 plot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(9.5, 11))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, n_neighbors in zip(axes, n_neighbors_values):\n",
    "    # Create a K-Nearest Neighbors classifier\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=\"distance\")\n",
    "    clf.fit(X, y)\n",
    "    # Plot decision boundaries\n",
    "    DecisionBoundaryDisplay.from_estimator(clf, X, cmap=cmap_light, ax=ax,\n",
    "                                           response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel=iris.feature_names[0],\n",
    "                                           ylabel=iris.feature_names[1],\n",
    "                                           shading=\"auto\")\n",
    "\n",
    "    # Plot training points\n",
    "    sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=iris.target_names[y],\n",
    "                    palette=cmap_bold, alpha=1.0, edgecolor=\"black\", ax=ax)\n",
    "\n",
    "    # Set the title\n",
    "    _ = ax.set(title = \"KKN classification (k = %i)\" % n_neighbors)\n",
    "\n",
    "\n",
    "# Adjust the layout of the subplots for better visualization\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GYqN6AudkMX"
   },
   "source": [
    "## Scenarios where KNN is Used\n",
    "\n",
    "1. **Small to Medium-Sized Datasets**: KNN performs well on datasets of modest size, where the computational cost of calculating distances between data points remains manageable.\n",
    "\n",
    "2. **Nonlinear Data**: KNN excels in handling complex and nonlinear decision boundaries. It can capture intricate relationships between features that might not be captured by linear classifiers.\n",
    "\n",
    "3. **Multi-Class Classification**: KNN is naturally suited for multi-class classification tasks. It assigns a class label based on the majority class among the K nearest neighbors.\n",
    "\n",
    "4. **Lazy Learning**: KNN is considered a lazy learning algorithm. It doesn't create an explicit model during training, which can be advantageous in situations where the underlying data distribution is unknown or subject to frequent changes.\n",
    "\n",
    "5. **No Assumptions about Data Distribution**: KNN is non-parametric and doesn't make any assumptions about the distribution of the data. This makes it valuable when the data distribution is complex or unclear.\n",
    "\n",
    "6. **Prototype-Based Learning**: KNN can be viewed as a prototype-based learning method. It classifies new instances by comparing them to existing instances in the training set.\n",
    "\n",
    "7. **Feature Selection and Exploration**: KNN's simplicity can aid in feature selection and exploration. It can help identify influential features by observing how the choice of neighbors impacts classification.\n",
    "\n",
    "8. **Imbalanced Datasets**: KNN can be adapted to handle imbalanced datasets by assigning different weights to neighbors or utilizing techniques to address class imbalances.\n",
    "\n",
    "9. **Anomaly Detection**: KNN can identify anomalies by flagging observations that are significantly dissimilar from their neighbors.\n",
    "\n",
    "10. **Collaborative Filtering**: In recommendation systems, KNN is used for collaborative filtering. It identifies similar users or items based on interaction patterns, enabling personalized recommendations.\n",
    "\n",
    "11. **Initial Baseline Model**: KNN serves as a starting point in model building. It provides a baseline performance that other, more complex algorithms can be compared against."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages of KNN\n",
    "\n",
    "- **Flexibility**: KNN can handle various types of data and doesn't impose strong assumptions about the data's distribution.\n",
    "- **Simple Concept**: The algorithm's concept is easy to understand and implement, making it accessible to beginners.\n",
    "- **Intuitive Interpretation**: The classifications made by KNN can often be interpreted intuitively by examining the nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of KNN\n",
    "\n",
    "- **Computational Cost**: As the dataset grows, calculating distances between data points becomes computationally intensive, leading to longer processing times.\n",
    "- **Curse of Dimensionality**: KNN's performance degrades in high-dimensional spaces due to the curse of dimensionality. In such cases, data points tend to be equidistant, making nearest neighbors less meaningful.\n",
    "- **Sensitive to Noise**: Outliers or noisy data points can significantly affect KNN's predictions, leading to suboptimal results.\n",
    "- **Choosing K**: Selecting the appropriate number of neighbors (K) can be challenging and can impact the algorithm's performance.\n",
    "- **Scaling**: Features with different scales can dominate the distance calculations, leading to biased results.\n",
    "\n",
    "KNN is valuable in scenarios where data isn't too high-dimensional, and computational resources are sufficient for distance calculations. While it's not a panacea, KNN serves as a benchmark algorithm, a quick baseline model, and a starting point for more advanced techniques. Understanding its strengths and limitations is crucial for effectively applying KNN in various real-world scenarios [Singh et al., 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZYIZG2XdkMX"
   },
   "source": [
    "## Example: Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22l6zBaedkMX"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>: In this code example, a Decision Tree Classifier is utilized to illustrate decision boundaries on synthetic data. The synthetic dataset is generated using the `make_blobs` function from scikit-learn, designed for creating artificial datasets for various machine learning experiments. This particular dataset consists of the following characteristics:\n",
    "\n",
    "- **Number of Samples:** 2000\n",
    "- **Number of Features:** 2\n",
    "- **Number of Classes:** 4\n",
    "- **Random Seed (random_state):** 0\n",
    "- **Cluster Standard Deviation (cluster_std):** 1.0\n",
    "\n",
    "**Features:**\n",
    "- The dataset contains 2000 data points, each described by a pair of feature values. These features are represented as 'Feature 1' and 'Feature 2'.\n",
    "\n",
    "**Outcome (Target Variable):**\n",
    "- The dataset also includes a target variable called 'Outcome.' This variable assigns each data point to one of two distinct classes, identified as 'Class 0',  'Class 1',  'Class 2', and 'Class 3'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n84O6i6LdkMX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_blobs(n_samples=2000, centers=4, random_state=0, cluster_std=1.0)\n",
    "\n",
    "# Create a DataFrame\n",
    "Data = pd.DataFrame(data=X, columns=['Feature %i' % (i + 1) for i in range(2)])\n",
    "Data['Outcome'] = y\n",
    "\n",
    "# Create a scatter plot using Seaborn\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9.5, 7))\n",
    "\n",
    "colors = [\"#f5645a\", \"#b781ea\", '#B2FF66', '#0096ff']\n",
    "edge_colors = ['#8A0002', '#3C1F8B','#6A993D', '#2e658c']\n",
    "cmap_light = ListedColormap(['#fdceca', '#ebdbfa', '#e9ffd3', '#c0def4'])\n",
    "markers = ['o', 's', 'd', '^']\n",
    "cmap_ = ListedColormap(colors)\n",
    "\n",
    "# Scatter plot of data points\n",
    "for num in np.unique(y):\n",
    "    ax.scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                s=40, edgecolors= edge_colors[num], marker=markers[num], label=str(num))\n",
    "\n",
    "ax.set(xlim=[-6, 6], ylim=[-2, 12])\n",
    "ax.legend()\n",
    "ax.set_title('Synthetic Dataset', weight = 'bold', fontsize = 16)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wT8cRn0UdkMY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 2))\n",
    "ax.barh(np.unique(y, return_counts=True)[0], np.unique(y, return_counts=True)[1], color=colors, edgecolor='k')\n",
    "ax.set_ylabel('Outcome')\n",
    "ax.grid(which='major', axis='y')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBDcYuUwdkMY"
   },
   "source": [
    "---\n",
    "\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "In the realm of machine learning and data analysis, a crucial step is the stratified train and test data split, typically carried out using the Scikit-Learn library in Python. This process aids in the evaluation of model performance by creating representative training and testing datasets.\n",
    "\n",
    "To execute a stratified split using Scikit-Learn, you can employ the `train_test_split` function from the `sklearn.model_selection` module. Stratification ensures that the distribution of class labels in both the training and testing sets is similar to the original dataset, which is particularly useful when dealing with imbalanced datasets.\n",
    "\n",
    "Here's a code snippet illustrating this process:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X represents your feature data, and y represents the corresponding labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "```\n",
    "\n",
    "In this code, `X` refers to your feature matrix, `y` represents the target variable, and the `test_size` parameter specifies the proportion of data allocated to the test set. By setting the `stratify` parameter to `y`, you ensure that the class distribution in the training and testing sets closely mirrors that of the original data. The `random_state` parameter is used to maintain reproducibility.\n",
    "\n",
    "This stratified train and test split methodology is a fundamental practice in machine learning to prevent bias in model evaluation and validation.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ncs_r-aldkMY"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split the DataFrame into train and test DataFrames\n",
    "train_data, test_data = train_test_split(Data, test_size=0.25, stratify = Data['Outcome'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZcD-GardkMY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_training_testing_pie_charts(train_data, test_data, test_size=0.25, colors=None):\n",
    "\n",
    "    # Calculate the total number of observations in the training and testing datasets\n",
    "    train_total = len(train_data)\n",
    "    test_total = len(test_data)\n",
    "\n",
    "    # Create a figure with two subplots side by side\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    for ax, data, title in zip((ax1, ax2), (train_data, test_data), ('Training', 'Testing')):\n",
    "        wedges, texts, autotexts = ax.pie(data['Outcome'].value_counts(), labels=data['Outcome'].value_counts().index,\n",
    "                autopct='%1.1f%%', startangle=140, shadow=True, colors=colors)\n",
    "        ax.set_title(f'{title} Data\\n({train_total if title == \"Training\" else test_total} observations)', fontsize=14)\n",
    "\n",
    "        # Add count values as annotations with a larger font size\n",
    "        for text, autotext in zip(texts, autotexts):\n",
    "            text.set(size=14)\n",
    "            autotext.set(size=14)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_testing_pie_charts(train_data, test_data, test_size=0.25, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI8FFJ1_dkMY"
   },
   "source": [
    "It is evident that the 1500 instances in the training set have been categorized into four groups. Similarly, the test set's four categories have also been divided into their respective groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs-x5gsBdkMY"
   },
   "source": [
    "Training a model with `n_neighbors = 5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gGCZV4IcdkMY"
   },
   "outputs": [],
   "source": [
    "# Split 'train_data' into X_train (features) and y_train (labels)\n",
    "X_train = train_data.drop('Outcome', axis=1)  # Assuming 'Outcome' is the label column\n",
    "y_train = train_data['Outcome']\n",
    "\n",
    "# Split 'test_data' into X_test (features) and y_test (labels)\n",
    "X_test = test_data.drop('Outcome', axis=1)  # Assuming 'Outcome' is the label column\n",
    "y_test = test_data['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFi3ftnadkMY"
   },
   "outputs": [],
   "source": [
    "# Import the necessary class from scikit-learn\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a KNN classifier with 3 neighbors\n",
    "KKN = KNeighborsClassifier(n_neighbors = 5)\n",
    "\n",
    "# Fit the KNN classifier to the training data\n",
    "KKN.fit(X_train, y_train)\n",
    "\n",
    "def _gen_cr(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    Results = pd.DataFrame(classification_report(y, y_pred,\n",
    "                                             output_dict=True)).T\n",
    "    display(Results.style.format(precision = 3))\n",
    "\n",
    "print('\\nTrain Data:')\n",
    "_gen_cr(KKN, X_train, y_train)\n",
    "\n",
    "print('\\nTest Data:')\n",
    "_gen_cr(KKN, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78pGwyqcdkMZ"
   },
   "source": [
    "The results can be visualized as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6YdxbxSdkMZ"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "\n",
    "# Create a loop for train and test sets\n",
    "for i, (X_set, y_set, title) in enumerate([(X_train, y_train, 'Train Set'), (X_test, y_test, 'Test Set')]):\n",
    "    # Plot decision boundaries\n",
    "    DecisionBoundaryDisplay.from_estimator(KKN, X_set, cmap=cmap_light, ax=ax[i],\n",
    "                                           response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
    "                                           shading=\"auto\")\n",
    "    \n",
    "    # Scatter plot of data points\n",
    "    for num in np.unique(y):\n",
    "        ax[i].scatter(X[:, 0][y == num], X[:, 1][y == num], c=colors[num],\n",
    "                    s=40, edgecolors= edge_colors[num], marker=markers[num], label=str(num))\n",
    "        \n",
    "    ax[i].legend(title=\"Outcome\")\n",
    "    ax[i].set_title(f'{title} - KNN (neighbors = 5)', fontweight='bold', fontsize=16)\n",
    "    ax[i].grid(False)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpbTaoz6hSVs"
   },
   "source": [
    "Inaccurate Predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CxXTGHV1dkMZ"
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.5))\n",
    "\n",
    "# Create a loop for train and test sets\n",
    "for i, (X_set, y_set, title) in enumerate([(X_train, y_train, 'Train Set'), (X_test, y_test, 'Test Set')]):\n",
    "    # Plot decision boundaries\n",
    "    DecisionBoundaryDisplay.from_estimator(KKN, X_set, cmap=cmap_light, ax=axes[i],\n",
    "                                           response_method=\"predict\",\n",
    "                                           plot_method=\"pcolormesh\",\n",
    "                                           xlabel='Feature 1', ylabel='Feature 2',\n",
    "                                           shading=\"auto\")\n",
    "    for num in np.unique(y):\n",
    "        axes[i].scatter(X_set.loc[y_set == num].values[:, 0], \n",
    "                        X_set.loc[y_set == num].values[:, 1],\n",
    "                        c = colors[num], s=40,\n",
    "                        edgecolors = edge_colors[num],\n",
    "                        marker=markers[num], label= f'Outcome {num}')\n",
    "        \n",
    "    # Plot data points where y_set and log_reg(X_set) differ in color\n",
    "    diff_points = X_set[y_set != KKN.predict(X_set)]  # Filter points where predictions differ\n",
    "    \n",
    "    axes[i].scatter(diff_points.values[:, 0], \n",
    "                    diff_points.values[:, 1], \n",
    "                    fc='Yellow', ec='black',\n",
    "                    s = 40, marker= 'h', label= 'Inaccurate Predictions')\n",
    "    \n",
    "    axes[i].set_title(f'{title} - Logistic Regression', fontweight='bold', fontsize=16)\n",
    "    axes[i].grid(False)\n",
    "    # Remove the legend for each panel\n",
    "    axes[i].legend()\n",
    "    axes[i].get_legend().remove()\n",
    "\n",
    "# Create a single legend for both subplots at the top\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', ncol=5, borderaxespad= -0.1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2MDA9dxfdkMZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "def plot_cm(model, X_train, X_test, y_train, y_test, class_names, figsize=(7, 4), normalize = False):\n",
    "    # Create a figure and axes for displaying confusion matrices side by side\n",
    "    fig, ax = plt.subplots(1, 2, figsize=figsize)\n",
    "\n",
    "    datasets = [(X_train, y_train, 'Train'), (X_test, y_test, 'Test')]\n",
    "\n",
    "    for i in range(2):\n",
    "        X, y, dataset_name = datasets[i]\n",
    "\n",
    "        # Compute confusion matrix for the dataset predictions\n",
    "        cm = confusion_matrix(y, model.predict(X))\n",
    "        if normalize:\n",
    "            cm = np.round(cm/cm.sum(axis = 1), 2)\n",
    "            fig.suptitle('Confusion Matrices (Normalized)', fontsize=16, weight = 'bold')\n",
    "        else:\n",
    "            fig.suptitle('Confusion Matrices', fontsize=16, weight = 'bold')\n",
    "        # Create a ConfusionMatrixDisplay and plot it on the respective axis\n",
    "        cm_display = ConfusionMatrixDisplay(cm, display_labels=class_names)\\\n",
    "                        .plot(ax=ax[i],\n",
    "                              im_kw=dict(cmap='Greens' if dataset_name == 'Train' else 'Blues'),\n",
    "                              text_kw={\"size\": 16}, colorbar=False)\n",
    "        ax[i].set_title(f'{dataset_name} Data')\n",
    "        ax[i].grid(False)\n",
    "\n",
    "    # Adjust the layout for better spacing\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_j9LCrsdkMZ"
   },
   "outputs": [],
   "source": [
    "plot_cm(KKN, X_train, X_test, y_train, y_test, np.unique(y).astype('str'), figsize=(8, 4))\n",
    "plot_cm(KKN, X_train, X_test, y_train, y_test, np.unique(y).astype('str'), figsize=(8, 4), normalize=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
