{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f4fdcfb",
   "metadata": {
    "id": "6f4fdcfb"
   },
   "source": [
    "# Resampling Methods\n",
    "\n",
    "Resampling methods involve the iterative extraction of sample data from a training set, followed by refitting a model of interest on each sample. This iterative process yields additional information about the fitted model. For instance, it allows us to assess the variability of a linear regression fit and compare the results. Two widely used resampling methods are [James et al., 2023]:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e104016",
   "metadata": {
    "id": "6e104016"
   },
   "source": [
    "## Cross-Validation (CV)\n",
    "\n",
    "Cross-Validation is a powerful resampling technique used to assess the performance of predictive models. Particularly valuable when dealing with limited data, it allows you to estimate how well your model will perform on unseen data points, providing a glimpse into its generalization capabilities [James et al., 2023].\n",
    "\n",
    "In the context of k-fold Cross-Validation, a commonly used approach, the dataset is divided into k subsets or \"folds.\" The steps for performing k-fold Cross-Validation are as follows [James et al., 2023, Refaeilzadeh et al., 2009]:\n",
    "\n",
    "1. **Partitioning Data:**\n",
    "   - Divide the dataset into k equally sized subsets, referred to as $D_1, D_2, \\ldots, D_k$.\n",
    "\n",
    "2. **Iterative Training and Testing:**\n",
    "   - For each fold $D_i$, treat it as the test set while combining the remaining folds $D_1 \\cup \\ldots \\cup D_{i-1} \\cup D_{i+1} \\cup \\ldots \\cup D_k$ to form the training set.\n",
    "   - Train your model on the training set and evaluate its performance on the test set.\n",
    "   - Repeat this process for each fold, resulting in k iterations.\n",
    "\n",
    "3. **Performance Metrics:**\n",
    "   - For each iteration, compute a performance metric (e.g., accuracy, mean squared error) based on your model's predictions on the test set.\n",
    "\n",
    "4. **Overall Performance Estimate:**\n",
    "   - Calculate the average of the performance metrics obtained from the k iterations. This average provides an overall estimate of your model's performance.\n",
    "\n",
    "Mathematically, let's consider a performance metric denoted as $P$, exemplified by accuracy, and a model $M$ designed to map input features $X$ to predictions $\\hat{y}$. In the context of k-fold Cross-Validation, the following steps are undertaken for each fold $D_i$:\n",
    "\n",
    "1. **Training:** The model $M$ is trained on the training set, excluding the current fold $D_i$. This process results in a trained model, represented as $M_i$: \n",
    "     \\begin{equation} M_i = M.fit(X_{\\text{train}_i}, y_{\\text{train}_i}) \\end{equation}\n",
    "\n",
    "    where\n",
    "    - $M_i$: This represents the model obtained after training on a specific fold $D_i$. It is a distinct instance of the original model $M$ and is specific to the training set $X_{\\text{train}_i}, y_{\\text{train}_i}$.\n",
    "\n",
    "    - $M$: The original model that maps input features to predictions.\n",
    "\n",
    "    - $\\text{fit}$: This method is typically used in machine learning libraries to train a model. It adjusts the parameters of the model based on the provided training data.\n",
    "\n",
    "    - $X_{\\text{train}_i}$: The input features of the training set for fold $D_i$. It is a subset of the entire dataset, excluding the instances in fold $D_i$.\n",
    "\n",
    "    - $y_{\\text{train}_i}$: The corresponding labels of the training set for fold $D_i$. Like $X_{\\text{train}_i}$, it is a subset of the entire set of labels, excluding those in fold $D_i$.\n",
    "\n",
    "2. **Evaluation:** The trained model $M_i$ is evaluated on the fold $D_i$, which serves as the test set. This evaluation produces a performance metric for the current iteration, denoted as $P_i$:\n",
    "     \\begin{equation} P_i = P(M_i.predict(X_{\\text{test}_i}), y_{\\text{test}_i}) \\end{equation}\n",
    "     \n",
    "     where\n",
    "    - $P_i$: This represents the performance metric (e.g., accuracy, mean squared error) obtained for the current fold $D_i$. It is the result of evaluating the model $M_i$ on the test set.\n",
    "\n",
    "    - $P$: The performance metric function, which quantifies how well the model's predictions align with the actual values.\n",
    "\n",
    "    - $M_i$: The model trained on the specific training set for the current fold $D_i$.\n",
    "\n",
    "    - $\\text{predict}$: This method is commonly used in machine learning libraries to generate predictions based on a trained model.\n",
    "\n",
    "    - $X_{\\text{test}_i}$: The input features of the test set for fold $D_i$. It represents the instances that were set aside for testing in the current iteration.\n",
    "\n",
    "    - $y_{\\text{test}_i}$: The corresponding actual labels of the test set for fold $D_i$. These are the true values against which the model's predictions are compared.\n",
    "\n",
    "These steps are iteratively applied for each fold, resulting in k performance metrics $P_1, P_2, ..., P_k$. The average performance estimate $\\bar{P}$ is then computed by taking the mean of these metrics:\n",
    "\n",
    "\\begin{equation} \\bar{P} = \\frac{1}{k} \\sum_{i=1}^{k} P_i \\end{equation}\n",
    "\n",
    "This average provides an aggregated measure of the model's performance across all folds, offering a comprehensive evaluation of its generalization capabilities. Importantly, Cross-Validation transcends the limitations of training error, delivering a robust performance estimate crucial for informed model assessment and selection in predictive modeling contexts [James et al., 2023, Refaeilzadeh et al., 2009]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5bdd2",
   "metadata": {
    "id": "01c5bdd2"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>: Consider a dataset $S$ containing 6 samples, denoted as {$x_1$, $x_2$, $x_3$, $x_4$, $x_5$, $x_6$}, and the objective is to perform a 3-fold cross-validation.\n",
    "\n",
    "To begin, the dataset $S$ is divided into 3 subsets in a randomized manner, resulting in:\n",
    "\n",
    "* Subset 1: $S_1 = \\{x_1, x_2\\}$\n",
    "* Subset 2: $S_2 = \\{x_3, x_4\\}$\n",
    "* Subset 3: $S_3 = \\{x_5, x_6\\}$\n",
    "\n",
    "In the subsequent steps, the machine-learning model is trained and evaluated 3 times. During each iteration, two of the subsets are combined to form the training set, while the remaining subset serves as the test set. The process unfolds as follows:\n",
    "\n",
    "**Iteration 1:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S_1 = \\{x_1, x_2\\}$, $S_2 = \\{x_3, x_4\\}$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_3 = \\{x_5, x_6\\}$\n",
    "\n",
    "**Iteration 2:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S_2 = \\{x_3, x_4\\}$, $S_3 = \\{x_5, x_6\\}$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_1 = \\{x_1, x_2\\}$\n",
    "\n",
    "**Iteration 3:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S_1 = \\{x_1, x_2\\}$, $S_3 = \\{x_5, x_6\\}$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_2 = \\{x_3, x_4\\}$\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/CV.png\" alt=\"picture\" width=\"500\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95fe4e9",
   "metadata": {
    "id": "d95fe4e9"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>: Returning to the Iris dataset, we employ a cross-validation strategy with a 5-fold partitioning scheme. In this approach, the dataset is divided into five subsets of roughly equal size, and the model is trained and evaluated five times. Each time, a different subset is held out as the test set, while the remaining four subsets are combined to form the training set. This process ensures that every data point is used for both training and testing, leading to a more robust evaluation of the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d60080",
   "metadata": {
    "id": "85d60080"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a KNN Classifier\n",
    "KKN = KNeighborsClassifier(n_neighbors=3)  # Using 3 nearest neighbors\n",
    "\n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;35m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "    \n",
    "# Initialize KFold cross-validator\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    KKN.fit(X_train, y_train)\n",
    "    # train\n",
    "    y_train_pred = KKN.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "    # test\n",
    "    y_test_pred = KKN.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbceb847",
   "metadata": {
    "id": "dbceb847"
   },
   "source": [
    "On the other hand, observe that,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60224a23",
   "metadata": {
    "id": "60224a23"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a Pandas DataFrame\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target_names[iris.target]\n",
    "\n",
    "# Use groupby to count observations by species\n",
    "species_counts = iris_df.groupby('species').size().to_frame('Count')\n",
    "\n",
    "print(\"Number of Observations by Species:\")\n",
    "display(species_counts.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac00798",
   "metadata": {
    "id": "cac00798"
   },
   "source": [
    "Now, let's invistigate the previous splitss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aef0d0b",
   "metadata": {
    "id": "6aef0d0b"
   },
   "outputs": [],
   "source": [
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;34m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "    \n",
    "# Perform Cross-Validation\n",
    "for C, (_, test_idx) in enumerate(kf.split(X)):\n",
    "    print_bold(f'Fold {C+1}:')\n",
    "    df = pd.Series(iris.target_names[iris.target][test_idx])\n",
    "    display(df.value_counts().to_frame('Count').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc500fe",
   "metadata": {
    "id": "dcc500fe"
   },
   "source": [
    "When working with a small dataset like the Iris dataset, which has only 150 samples, the choice of cross-validation strategy becomes even more crucial. Using StratifiedKFold for splitting a small dataset into train and test sets offers several benefits:\n",
    "\n",
    "1. **Preserving Data Distribution**: The Iris dataset contains three classes (species) with 50 samples each. If you use simple random splitting without stratification, there's a chance that one or more classes might be underrepresented or absent in either the train or test set. StratifiedKFold ensures that each fold maintains the original class distribution, which is particularly important when working with a limited number of samples.\n",
    "\n",
    "2. **More Reliable Performance Estimates**: In small datasets, individual data points can have a larger impact on model training and evaluation. By using stratification, you're making sure that each fold accurately represents the underlying data distribution. This leads to more reliable performance estimates and reduces the risk of overfitting or underestimation.\n",
    "\n",
    "3. **Preventing Overfitting**: Small datasets are prone to overfitting, especially if you're using a complex model. Using stratified cross-validation helps in mitigating this risk by providing consistent evaluation across folds and ensuring that each fold has a representative distribution of classes.\n",
    "\n",
    "4. **Robustness to Variability**: Small datasets often have more variability in terms of data distribution and noise. StratifiedKFold provides a way to handle this variability by maintaining class balance, leading to a more stable evaluation process.\n",
    "\n",
    "5. **Comparable Results**: StratifiedKFold ensures that performance metrics are calculated over similar data distributions for each fold. This makes your results more comparable and interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f6a30",
   "metadata": {
    "id": "972f6a30"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Create a KNN Classifier\n",
    "KKN = KNeighborsClassifier(n_neighbors=3)  # Using 3 nearest neighbors\n",
    "\n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;43m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "    \n",
    "# Initialize StratifiedKFold cross-validator\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "\n",
    "\n",
    "# Perform Cross-Validation\n",
    "for train_idx, test_idx in skf.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    KKN.fit(X_train, y_train)\n",
    "    # train\n",
    "    y_train_pred = KKN.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "    # test\n",
    "    y_test_pred = KKN.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "_Line()\n",
    "#  Print the Train and Test Scores for each fold\n",
    "for fold in range(n_splits):\n",
    "    print_bold(f'Fold {fold + 1}:')\n",
    "    print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "    print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed000247",
   "metadata": {
    "id": "ed000247"
   },
   "source": [
    "Observe that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ce2be",
   "metadata": {
    "id": "c84ce2be"
   },
   "outputs": [],
   "source": [
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;31m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "    \n",
    "# Perform Cross-Validation\n",
    "for C, (_, test_idx) in enumerate(skf.split(X, y)):\n",
    "    print_bold(f'Fold {C+1}:')\n",
    "    df = pd.Series(iris.target_names[iris.target][test_idx])\n",
    "    display(df.value_counts().to_frame('Count').T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bdc56",
   "metadata": {
    "id": "577bdc56"
   },
   "source": [
    "### Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "Leave-One-Out Cross-Validation is a special case of k-fold Cross-Validation where \\(k\\) is set to the number of samples in the dataset. In other words, for each iteration of LOOCV, only one sample is used as the test set, and the rest of the samples form the training set. LOOCV is particularly useful when dealing with a small dataset since it maximizes the use of available data for testing purposes.\n",
    "\n",
    "Here's how LOOCV works:\n",
    "\n",
    "1. **Iteration:**\n",
    "   - For each data point in the dataset, designate it as the test instance, and use the remaining \\(n-1\\) instances as the training set.\n",
    "\n",
    "2. **Model Training and Testing:**\n",
    "   - Train your model on the \\(n-1\\) training instances.\n",
    "   - Evaluate the model's performance on the single test instance.\n",
    "   - Calculate the performance metric (e.g., accuracy, mean squared error) for this iteration.\n",
    "\n",
    "3. **Repeat for All Data Points:**\n",
    "   - Repeat steps 1 and 2 for all \\(n\\) data points in the dataset.\n",
    "\n",
    "4. **Overall Performance Estimate:**\n",
    "   - Calculate the average performance metric across all \\(n\\) iterations. This provides an overall estimate of the model's performance.\n",
    "\n",
    "**Advantages of LOOCV:**\n",
    "- Utilizes the entire dataset for testing, ensuring maximum information is used.\n",
    "- Provides a nearly unbiased estimate of the model's performance, as each instance serves as both training and test data.\n",
    "- Particularly useful for small datasets where other forms of cross-validation may result in too few instances for testing.\n",
    "\n",
    "**Disadvantages of LOOCV:**\n",
    "- Can be computationally expensive for large datasets, as it requires fitting the model \\(n\\) times.\n",
    "- May lead to high variance estimates due to the potential similarity between training and test instances in each iteration.\n",
    "\n",
    "LOOCV provides a reliable estimate of a model's performance, especially when dealing with limited data. While it can be more computationally intensive, its comprehensiveness and minimal bias make it a valuable tool for model assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f106e29",
   "metadata": {
    "id": "7f106e29"
   },
   "source": [
    "Example: In leave-one-out (LOO) cross-validation, we iteratively train our machine-learning model n times, where n represents the size of our dataset. During each iteration, a single sample is set aside as the test set, while the remaining samples are used for training the model.\n",
    "\n",
    "LOO can be thought of as an extreme case of k-fold cross-validation, where the value of **k** becomes equal to **n**. If we apply the LOO approach to the previous example, we'll create 6 distinct test subsets:\n",
    "\n",
    "- Subset 1: $S_1 = \\{x_1\\}$\n",
    "- Subset 2: $S_2 = \\{x_2\\}$\n",
    "- Subset 3: $S_3 = \\{x_3\\}$\n",
    "- Subset 4: $S_4 = \\{x_4\\}$\n",
    "- Subset 5: $S_5 = \\{x_5\\}$\n",
    "- Subset 6: $S_6 = \\{x_6\\}$\n",
    "\n",
    "For each of these subsets, we perform an iteration. During iteration **i = 1, 2, ..., 6**, we use the dataset **$S$** without the samples from **$S_i$** (denoted as **$S \\setminus S_i$**) as the training data. Then, we evaluate the model's performance on **$S_i$**, which serves as the test set:\n",
    "\n",
    "**Iteration 1:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S \\setminus S_1$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_1 = \\{x_1\\}$\n",
    "\n",
    "**Iteration 2:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S \\setminus S_2$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_2 = \\{x_2\\}$\n",
    "\n",
    "**Iteration 3:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S \\setminus S_3$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_3 = \\{x_3\\}$\n",
    "\n",
    "**Iteration 4:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S \\setminus S_4$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_4 = \\{x_4\\}$\n",
    "\n",
    "**Iteration 5:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S \\setminus S_5$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_5 = \\{x_5\\}$\n",
    "\n",
    "**Iteration 6:**\n",
    "- <font color='Green'><b>Training Set:</b></font> $S \\setminus S_6$\n",
    "- <font color='Blue'><b>Test Set:</b></font> $S_6 = \\{x_6\\}$\n",
    "\n",
    "In each iteration, the model learns from all but one data point and then gets evaluated on the held-out sample. This process provides a detailed assessment of the model's performance for every individual sample in the dataset.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/LeaveOneOut.png\" alt=\"picture\" width=\"400\">\n",
    "<br>\n",
    "<b>Figure</b>: An example of using KKN.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744cdc6a",
   "metadata": {
    "id": "744cdc6a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "def _Line(n = 50):\n",
    "    print(n * '_')\n",
    "    \n",
    "# Create a KNN Classifier\n",
    "KKN = KNeighborsClassifier(n_neighbors=3)  # Using 3 nearest neighbors\n",
    "\n",
    "def print_bold(txt):\n",
    "    _left = \"\\033[1;43m\"\n",
    "    _right = \"\\033[0m\"\n",
    "    print(_left + txt + _right)\n",
    "\n",
    "def _Line(n = 80):\n",
    "    print(n * '_')\n",
    "    \n",
    "# Initialize Leave-One-Out cross-validator\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Lists to store train and test scores for each iteration\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Lists to store train and test scores for each fold\n",
    "train_acc_scores, test_acc_scores, train_f1_scores, test_f1_scores = [], [], [], []\n",
    "\n",
    "# Perform Leave-One-Out Cross-Validation\n",
    "for train_idx, test_idx in loo.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    KKN.fit(X_train, y_train)\n",
    "    # train\n",
    "    y_train_pred = KKN.predict(X_train)\n",
    "    train_acc_scores.append(metrics.accuracy_score(y_train, y_train_pred))\n",
    "    train_f1_scores.append(metrics.f1_score(y_train, y_train_pred, average = 'weighted'))\n",
    "    # test\n",
    "    y_test_pred = KKN.predict(X_test)\n",
    "    test_acc_scores.append(metrics.accuracy_score(y_test, y_test_pred))\n",
    "    test_f1_scores.append(metrics.f1_score(y_test, y_test_pred, average = 'weighted'))\n",
    "\n",
    "# _Line()\n",
    "# #  Print the Train and Test Scores for each fold\n",
    "# for fold in range(len(train_acc_scores)):\n",
    "#     print_bold(f'Fold {fold + 1}:')\n",
    "#     print(f\"\\tTrain Accuracy Score = {train_acc_scores[fold]:.4f}, Test Accuracy Score = {test_acc_scores[fold]:.4f}\")\n",
    "#     print(f\"\\tTrain F1 Score (weighted) = {train_f1_scores[fold]:.4f}, Test F1 Score (weighted)= {test_f1_scores[fold]:.4f}\")\n",
    "\n",
    "_Line()\n",
    "print_bold('Accuracy Score:')\n",
    "print(f\"\\tMean Train Accuracy Score: {np.mean(train_acc_scores):.4f} ± {np.std(train_acc_scores):.4f}\")\n",
    "print(f\"\\tMean Test Accuracy Score: {np.mean(test_acc_scores):.4f} ± {np.std(test_acc_scores):.4f}\")\n",
    "print_bold('F1 Score:')\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(train_f1_scores):.4f} ± {np.std(train_f1_scores):.4f}\")\n",
    "print(f\"\\tMean F1 Accuracy Score (weighted): {np.mean(test_f1_scores):.4f} ± {np.std(test_f1_scores):.4f}\")\n",
    "_Line()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be23199",
   "metadata": {
    "id": "8be23199"
   },
   "source": [
    "Note that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2597a2",
   "metadata": {
    "id": "3e2597a2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform Cross-Validation\n",
    "C = 0\n",
    "for _, test_idx in loo.split(X, y):\n",
    "    print_bold(f'Fold {C + 1}:')\n",
    "    print(iris.target_names[iris.target][test_idx])\n",
    "    C+=1\n",
    "    # Only the first 5 folds\n",
    "    if C == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7423063",
   "metadata": {
    "id": "f7423063"
   },
   "source": [
    "## Bootstrap Method\n",
    "\n",
    "The Bootstrap method is a powerful resampling technique designed to estimate the sampling distribution of a given statistic, such as the mean, median, or standard deviation. It offers a robust way to assess the variability and uncertainty associated with a statistic, without relying on specific assumptions about the underlying data distribution [James et al., 2023]:.\n",
    "\n",
    "Here's a detailed breakdown of the Bootstrap method:\n",
    "\n",
    "1. **Bootstrap Sample Generation:**\n",
    "   - For each iteration $b = 1, 2, \\ldots, B$, create a new bootstrap sample $x_1^*, x_2^*, \\ldots, x_n^*$ by randomly selecting $n$ observations with replacement from the original dataset $x_1, x_2, \\ldots, x_n$. This allows for duplicates within the sample.\n",
    "\n",
    "2. **Statistic Calculation:**\n",
    "   - Calculate the statistic of interest $T^*$ based on the bootstrap sample: $T^* = g(x_1^*, x_2^*, \\ldots, x_n^*)$. Here, $g$ is a function that computes the desired statistic. For instance, if estimating the mean, $T^* = \\frac{1}{n} \\sum_{i=1}^{n} x_i^*$.\n",
    "\n",
    "3. **Repeat and Collect Statistics:**\n",
    "   - Repeat steps 1 and 2 for a significant number of iterations $B$, resulting in a collection of bootstrap statistics: $T_1^*, T_2^*, \\ldots, T_B^*$.\n",
    "\n",
    "4. **Estimating Variability:**\n",
    "   - The distribution of the calculated bootstrap statistics $T_1^*, T_2^*, \\ldots, T_B^*$ approximates the sampling distribution of the original statistic $T$.\n",
    "   - From this distribution, you can calculate various measures of variability, such as confidence intervals, standard errors, or percentiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7443d2",
   "metadata": {
    "id": "6c7443d2"
   },
   "source": [
    "Bootstraping, in the context of statistics and data analysis, refers to a resampling technique that helps us make inferences about a population based on a sample of data. It's particularly useful when we want to understand the variability of a statistic (such as the mean or standard deviation) and estimate its properties, like confidence intervals, when the underlying population distribution might be unknown or complex.\n",
    "\n",
    "Here's a simple explanation of bootstrapping:\n",
    "\n",
    "1. **The Problem:** Imagine you have a small dataset, and you want to understand something about a specific statistic (like the mean) of the entire population from which the data was drawn. However, drawing conclusions directly from your limited sample might not accurately reflect the true characteristics of the population.\n",
    "\n",
    "2. **Resampling:** Bootstrapping addresses this by simulating the process of drawing samples from your original data. To do this, you randomly select data points from your original dataset with replacement, creating a new \"bootstrap sample\" of the same size as your original data. Because you're sampling with replacement, some data points will appear multiple times in the bootstrap sample, while others might not appear at all.\n",
    "\n",
    "3. **Calculating Statistic:** For each bootstrap sample, you calculate the desired statistic (e.g., mean) based on the data points in that sample.\n",
    "\n",
    "4. **Repeated Process:** You repeat the resampling process (step 2 and 3) a large number of times (often thousands of times). Each time, you're essentially creating a simulated dataset by drawing samples from your original data with replacement.\n",
    "\n",
    "5. **Inference:** By analyzing the distribution of the calculated statistic across all these bootstrap samples, you can make inferences about the population. For instance, you can estimate the variability of the statistic (by looking at its standard deviation), calculate confidence intervals, or even visualize the distribution itself.\n",
    "\n",
    "6. **Advantages:** Bootstrapping is powerful because it doesn't rely on assumptions about the shape or parameters of the population distribution. It works well for small sample sizes and can provide valuable insights even when traditional statistical methods might not be applicable.\n",
    "\n",
    "Bootstrapping is a resampling technique that helps us understand the characteristics of a population by repeatedly creating simulated datasets from our original data and calculating statistics of interest on these simulated datasets. It's a versatile and useful tool in situations where traditional statistical methods might not be suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae055799",
   "metadata": {
    "id": "ae055799",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Load the Iris dataset from Seaborn\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "# Choose a specific iris species (let's say \"setosa\")\n",
    "species_data = iris[iris[\"species\"] == \"setosa\"]\n",
    "petal_length_data = species_data[\"petal_length\"]\n",
    "\n",
    "# Number of bootstrap samples\n",
    "num_bootstraps = 1000\n",
    "\n",
    "# Initialize an array to store bootstrap sample means\n",
    "bootstrap_means = np.zeros(num_bootstraps)\n",
    "\n",
    "# Perform bootstrapping\n",
    "for i in range(num_bootstraps):\n",
    "    bootstrap_sample = np.random.choice(petal_length_data,\n",
    "                                        size=len(petal_length_data), replace=True)\n",
    "    bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "# Calculate the 95% confidence interval for the mean\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram of bootstrap sample means\n",
    "sns.histplot(bootstrap_means, kde=True, ax=ax)\n",
    "ax.axvline(x=np.mean(petal_length_data), color='r', linestyle='dashed', label='Original Mean')\n",
    "ax.axvline(x=np.mean(bootstrap_means), color='g', linestyle='dashed', label='Bootstrap Mean')\n",
    "ax.axvline(x=confidence_interval[0], color='b', linestyle='dashed', label='95% CI Lower')\n",
    "ax.axvline(x=confidence_interval[1], color='b', linestyle='dashed', label='95% CI Upper')\n",
    "\n",
    "# Set various plot settings in one call\n",
    "ax.set(xlabel=\"Mean Petal Length\", ylabel=\"Frequency\",\n",
    "       title=\"Bootstrapped Mean Petal Length Distribution\")\n",
    "ax.legend()\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "print(\"Original Mean: %.5f\" % np.mean(petal_length_data))\n",
    "print(\"Bootstrap Mean %.5f\" % np.mean(bootstrap_means))\n",
    "print(\"95% Confidence Interval:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287f904",
   "metadata": {
    "id": "9287f904"
   },
   "source": [
    "Let's break down the code step by step:\n",
    "\n",
    "1. **Import Libraries:**\n",
    "   - `numpy` is imported as `np` for numerical operations.\n",
    "   - `seaborn` is imported as `sns` for data visualization.\n",
    "   - `matplotlib.pyplot` is imported as `plt` for creating plots.\n",
    "\n",
    "2. **Load the Dataset:**\n",
    "   - The Iris dataset is loaded using `sns.load_dataset(\"iris\")`.\n",
    "\n",
    "3. **Data Preparation:**\n",
    "   - A specific species (\"setosa\") from the dataset is chosen.\n",
    "   - The \"petal_length\" data for the chosen species is extracted.\n",
    "\n",
    "4. **Number of Bootstrap Samples:**\n",
    "   - `num_bootstraps` is set to 1000, indicating the number of bootstrap samples to generate.\n",
    "\n",
    "5. **Initialize Arrays:**\n",
    "   - An array named `bootstrap_means` is initialized to store the means of each bootstrap sample.\n",
    "\n",
    "6. **Perform Bootstrapping:**\n",
    "   - A loop runs `num_bootstraps` times.\n",
    "   - For each iteration, a bootstrap sample is created by randomly selecting data points from `petal_length_data` with replacement.\n",
    "   - The mean of the bootstrap sample is calculated and stored in the `bootstrap_means` array.\n",
    "\n",
    "7. **Calculate Confidence Interval:**\n",
    "    The code snippet `np.percentile(bootstrap_means, [2.5, 97.5])` calculates the percentiles of the `bootstrap_means` array. In particular, it calculates the 2.5th and 97.5th percentiles of the distribution, which correspond to the lower and upper bounds of a 95% confidence interval.\n",
    "\n",
    "    In the context of bootstrapping and statistical inference, the calculated confidence interval provides an estimate of the range in which the true population parameter (in this case, the mean) is likely to fall. Here's how it works:\n",
    "\n",
    "    1. `bootstrap_means`: This is an array that contains the means of various bootstrap samples. Each value in this array represents the mean of a specific bootstrap sample.\n",
    "\n",
    "    2. `np.percentile(bootstrap_means, [2.5, 97.5])`: This function call calculates the specified percentiles from the `bootstrap_means` array. Percentiles are values that divide a dataset into corresponding percentages. In this case, the code calculates the 2.5th and 97.5th percentiles.\n",
    "\n",
    "       - The 2.5th percentile represents the value below which 2.5% of the data falls. It gives the lower bound of the confidence interval.\n",
    "       - The 97.5th percentile represents the value below which 97.5% of the data falls. It gives the upper bound of the confidence interval.\n",
    "\n",
    "    3. The calculated percentiles provide the lower and upper bounds of a confidence interval. Specifically, the range between the 2.5th percentile and the 97.5th percentile is a 95% confidence interval for the parameter being estimated (in this case, the mean of the population).\n",
    "\n",
    "\n",
    "8. **Create Plot:**\n",
    "   - A new figure and axis are created using `plt.subplots()`.\n",
    "   - The histogram of bootstrapped means is plotted using `sns.histplot()`, with a kernel density estimate (KDE) overlaid.\n",
    "   - Dashed vertical lines are added to indicate the original mean, bootstrap mean, and lower and upper bounds of the confidence interval.\n",
    "\n",
    "9. **Set Plot Settings:**\n",
    "   - Various plot settings such as labels, title, and legends are set using the `ax.set()` function.\n",
    "\n",
    "10. **Display the Plot:**\n",
    "    - The plot is displayed using `plt.show()`.\n",
    "\n",
    "11. **Print Results:**\n",
    "    - The original mean of \"petal_length_data\", bootstrap mean, and the calculated confidence interval are printed using `print()` statements.\n",
    "\n",
    "The code essentially demonstrates how bootstrapping can be used to estimate the mean of a dataset, and how the distribution of bootstrapped means provides insights into the variability of the estimate. The confidence interval helps us understand the range within which the true population mean is likely to fall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7767c3a",
   "metadata": {
    "id": "b7767c3a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Link = 'https://climate.weather.gc.ca/climate_data/bulk_data_e.html?format=csv&stationID=27211&Year=2022&Month=1&Day=1&time=&timeframe=2&submit=Download+Data'\n",
    "\n",
    "df = pd.read_csv(Link, usecols = ['Date/Time', 'Year', 'Month' , 'Mean Temp (°C)']).dropna()\n",
    "df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "df = df.rename(columns = {'Date/Time':'Date'})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28ca55",
   "metadata": {
    "id": "be28ca55",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import calendar\n",
    "\n",
    "def print_bold(txt):\n",
    "    print(\"\\033[1;46m\" + txt + \"\\033[0m\")\n",
    "\n",
    "year = df.Year.unique()[0]\n",
    "\n",
    "fig, axes = plt.subplots(6, 2, figsize=(9.5, 20))\n",
    "axes = axes.ravel()\n",
    "for m, ax in enumerate(axes, start=1):\n",
    "    \n",
    "    mean_temp_data = df.loc[df.Month == m, 'Mean Temp (°C)'].values\n",
    "\n",
    "    # Number of bootstrap samples\n",
    "    num_bootstraps = 1000\n",
    "\n",
    "    # Initialize an array to store bootstrap sample means\n",
    "    bootstrap_means = np.zeros(num_bootstraps)\n",
    "\n",
    "    # Perform bootstrapping\n",
    "    for i in range(num_bootstraps):\n",
    "        bootstrap_sample = np.random.choice(mean_temp_data,\n",
    "                                            size=len(mean_temp_data), replace=True)\n",
    "        bootstrap_means[i] = np.mean(bootstrap_sample)\n",
    "\n",
    "    # Calculate the 95% confidence interval for the mean\n",
    "    confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "    # Plot the histogram of bootstrap sample means\n",
    "    sns.histplot(bootstrap_means, kde=True, ax=ax, color = 'LightCyan', stat = 'probability')\n",
    "    ax.axvline(x=np.mean(mean_temp_data), color='r', linestyle='dashed', lw = 2, label='Original Mean')\n",
    "    ax.axvline(x=np.mean(bootstrap_means), color='g', linestyle='dashed', lw = 2, label='Bootstrap Mean')\n",
    "    ax.axvline(x=confidence_interval[0], color='b', linestyle='dashed', lw = 2, label='95% CI Lower')\n",
    "    ax.axvline(x=confidence_interval[1], color='b', linestyle='dashed', lw = 2, label='95% CI Upper')\n",
    "\n",
    "    # Set various plot settings in one call\n",
    "    ax.set(xlabel = \"Mean Monthly Temp (°C)\", ylabel=\"Frequency\",\n",
    "        title= f\"{calendar.month_name[m]} {year}\")\n",
    "    ax.legend()\n",
    "    print_bold(f'\\n{calendar.month_name[m]}:')\n",
    "    print(\"\\tOriginal Mean: %.5f\" % np.mean(mean_temp_data))\n",
    "    print(\"\\tBootstrap Mean %.5f\" % np.mean(bootstrap_means))\n",
    "    print(\"\\t95% Confidence Interval:\", confidence_interval)\n",
    "    ax.xaxis.grid(False)\n",
    "fig.suptitle('Bootstrapped Mean Monthly Temp (°C) Distribution',\n",
    "             fontsize =16, weight = 'bold', y = 1.0)\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
