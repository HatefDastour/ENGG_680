{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5735b73",
   "metadata": {
    "id": "d5735b73"
   },
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce892b39",
   "metadata": {},
   "source": [
    "Gradient Boosting Algorithm (GBA) is a powerful machine learning technique used to build predictive models. It creates an ensemble of weak learners, meaning that it combines several smaller, simpler models to obtain a more accurate prediction than what an individual model would produce. Gradient boosting works by iteratively training the weak learners on gradient-based functions and incorporating them into the model as “boosted” participants. At its core, gradient boosting works by combining multiple gradient steps to build up a strong predicting model from weak estimators residing in a gradient function space with additional weak learners joining the gradient function space after each iteration of gradient boosting. At each step, gradient descent is used to identify which small components help the function most and are thus added to the overall gradient model. This allows for complicated problems such as data analysis, text processing, and image recognition to be solved with greater accuracy and enhanced performance in `GradientBoostingClassifier` and `GradientBoostingRegressor`.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/Gradient_Boosting_Trees.jpg\" alt=\"picture\" width=\"600\">\n",
    "<br>\n",
    "<b>Figure</b>: A simplified representation of Gradient Boosting Algorithm (GBA).\n",
    "</center>\n",
    "\n",
    "The gradient boosting algorithm is an effective machine learning method used to solve supervised learning problems, such as classification and regression. It is becoming increasingly popular among data scientists due to its ability to increase the accuracy of predictions. The algorithm works by iteratively constructing a series of decision trees, each aimed at rectifying errors made by its predecessors. The algorithm optimizes a loss function, usually the mean squared error for regression or the deviance (log loss) for classification `GradientBoostingClassifier` and `GradientBoostingRegressor`.\n",
    "\n",
    "At its core, Gradient Boosting iteratively crafts a series of decision trees, each aimed at rectifying errors made by its predecessors. It optimizes a loss function, usually the mean squared error for regression or the deviance (log loss) for classification. The algorithm operates as follows [Alpaydin, 2020, James et al., 2023, scikit-learn Developers, 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39855f00",
   "metadata": {
    "id": "39855f00"
   },
   "source": [
    "## Gradient Boosting Regressor and Classifier\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Begin with a model set at a constant value, typically the mean of the target variable, serving as the initial prediction:\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\hat{f}_0(x) = \\underset{c}{\\mathrm{argmin}} \\sum_{i=1}^{n} L(y_i, c)\n",
    "    \\end{equation}\n",
    "\n",
    "    The term $ L(y_i, c) $ represents the loss function, quantifying the difference between true target values ($y_i$) and predicted values ($c$). The algorithm aims to minimize this loss function during training. The model starts with a constant value ($c$), often set to the mean of the target variable, seeking the optimal constant value minimizing the overall loss across all data points. The specific form of the loss function ($L$) varies based on the problem type:\n",
    "\n",
    "    - For regression problems, the Mean Squared Error (MSE) is commonly used:\n",
    "\n",
    "    \\begin{equation}\n",
    "    L(y_i, c) = (y_i - c)^2\n",
    "    \\end{equation}\n",
    "\n",
    "    - For classification problems, different loss functions, such as the deviance (log loss), may be employed.\n",
    "\n",
    "2. **Boosting Iterations (b = 1 to B):**\n",
    "    a. **Compute Negative Gradient:**\n",
    "\n",
    "    \\begin{equation}\n",
    "    r_i = - \\left[\\frac{\\partial L(y_i, \\hat{f}(x_i))}{\\partial \\hat{f}(x_i)}\\right]_{\\hat{f}(x)=\\hat{f}_{b-1}(x)}\n",
    "    \\end{equation}\n",
    "\n",
    "    This expression signifies the negative gradient of the loss function ($L$) concerning the current model's predictions ($\\hat{f}(x)$) at each data point ($x_i$). The partial derivative quantifies the sensitivity of the loss to changes in the model's prediction and is evaluated at the previous iteration's predictions ($\\hat{f}_{b-1}(x)$). The negative gradient serves as a pseudo-residual in the training process.\n",
    "\n",
    "    b. **Fit Decision Tree:** Fit a decision tree to the negative gradient (residuals) values ($r_i$) as the new target variable. This tree is typically shallow and controlled by hyperparameters like `max_depth`.\n",
    "    \n",
    "    c. **Update Model:**\n",
    "    \\begin{equation}\n",
    "    \\hat{f}_b(x) = \\hat{f}_{b-1}(x) + \\alpha \\cdot \\text{new_tree}(x)\n",
    "    \\end{equation}\n",
    "\n",
    "    Update the model by incorporating the prediction from the new tree, scaled by a learning rate ($\\alpha$).\n",
    "\n",
    "3. **Final Prediction:**\n",
    "    - The ultimate prediction is the sum of all individual trees:\n",
    "\n",
    "    \\begin{equation}\n",
    "    \\hat{f}(x) = \\sum_{b=1}^{B} \\hat{f}_b(x)\n",
    "    \\end{equation}\n",
    "\n",
    "The Gradient Boosting algorithm optimizes a loss function, typically mean squared error for regression or deviance (log loss) for classification [Alpaydin, 2020, James et al., 2023, scikit-learn Developers, 2023]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec7e68",
   "metadata": {
    "id": "4aec7e68"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/HatefDastour/hatefdastour.github.io/master/_notes/Introduction_to_Digital_Engineering/_images/Gradient_Boosting_Flowchart.jpg\" alt=\"picture\" width=\"450\">\n",
    "<br>\n",
    "<b>Figure</b>: A visual representation of the above description.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdd270b",
   "metadata": {
    "id": "4cdd270b"
   },
   "source": [
    "**Scikit-learn Implementation:**\n",
    "\n",
    "Scikit-learn offers the `GradientBoostingRegressor` and `GradientBoostingClassifier` classes for regression and classification tasks. These classes allow customization of hyperparameters like learning rate, tree depth, and the number of boosting iterations (`n_estimators`). Here's how to use them:\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# For regression\n",
    "regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.01, max_depth=3)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# For classification\n",
    "classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.01, max_depth=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "```\n",
    "\n",
    "Utilizing these classes simplifies the implementation of Gradient Boosting for regression and classification tasks within scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b23b339",
   "metadata": {
    "id": "8b23b339"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "scikit-learn's Gradient Boosting Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Initialize the model's prediction as a constant value, often the mean of the target variable for regression or class probabilities for classification.\n",
    "   - Compute the initial negative gradient (residuals) for regression or the negative gradient of the log-loss for classification.\n",
    "\n",
    "2. **Boosting Iterations:**\n",
    "   - For each boosting iteration (b = 1 to B), perform the following steps:\n",
    "\n",
    "     a. **Construct a Decision Tree:**\n",
    "        - Fit a decision tree to the negative gradient (residuals) or the negative gradient of the log-loss values as the new target variable.\n",
    "        - The decision tree typically has a limited depth (controlled by hyperparameters like `max_depth`).\n",
    "\n",
    "     b. **Update the Model:**\n",
    "        - Compute the prediction from the new decision tree and scale it by a learning rate (α).\n",
    "        - Update the model's prediction by adding the scaled tree prediction:\n",
    "\n",
    "           \\begin{equation} \\hat{f}_b(x) = \\hat{f}_{b-1}(x) + \\alpha \\cdot \\text{new_tree}(x) \\end{equation}\n",
    "\n",
    "3. **Final Prediction:**\n",
    "   - The final prediction is the sum of predictions from all individual trees:\n",
    "\n",
    "      \\begin{equation} \\hat{f}(x) = \\sum_{b=1}^{B} \\hat{f}_b(x) \\end{equation}\n",
    "\n",
    "4. **Classification Specifics:**\n",
    "   - For classification tasks, the class probabilities are often transformed into class labels using a threshold or argmax operation.\n",
    "\n",
    "5. **Hyperparameters:**\n",
    "   - The algorithm's behavior is influenced by hyperparameters like the number of boosting iterations (`n_estimators`), learning rate (`learning_rate`), maximum tree depth (`max_depth`), and more.\n",
    "\n",
    "6. **Regularization:**\n",
    "   - Regularization techniques, such as early stopping and subsampling, are commonly used to prevent overfitting and improve efficiency.\n",
    "\n",
    "7. **Prediction and Evaluation:**\n",
    "   - Use the trained model to make predictions on new data. For classification, the class with the highest probability (or predicted label) is assigned.\n",
    "   - Evaluate the model's performance using appropriate metrics for regression or classification tasks.\n",
    "\n",
    "Remember that while this description provides an overview of the algorithm, the actual implementation in scikit-learn includes optimizations and additional features. For comprehensive details, you can refer to scikit-learn's documentation on `GradientBoostingClassifier` and `GradientBoostingRegressor`.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35177fa",
   "metadata": {
    "id": "f35177fa"
   },
   "source": [
    "## XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a highly optimized and popular gradient boosting framework that excels in performance and predictive accuracy. Developed by Tianqi Chen, XGBoost has become one of the go-to choices for structured data in machine learning competitions and real-world applications. It extends the basic gradient boosting algorithm by introducing enhancements and fine-tuning to achieve superior results. Here's an overview of the XGBoost algorithm [xgboost developers, 2023]:\n",
    "\n",
    "**Gradient Boosting Framework:**\n",
    "XGBoost shares the fundamental idea of gradient boosting. It iteratively builds an ensemble of weak learners, usually decision trees, to form a powerful predictive model. Each weak learner attempts to correct the errors made by its predecessors.\n",
    "\n",
    "**Key Features of XGBoost:**\n",
    "XGBoost introduces several innovations that contribute to its remarkable performance:\n",
    "\n",
    "1. **Regularization:** XGBoost includes L1 (Lasso) and L2 (Ridge) regularization terms in the optimization objective to control model complexity and prevent overfitting.\n",
    "\n",
    "2. **Custom Loss Functions:** XGBoost allows the use of custom loss functions, extending its applicability to various problem domains.\n",
    "\n",
    "3. **Handling Missing Values:** XGBoost can automatically handle missing values during tree construction.\n",
    "\n",
    "4. **Feature Importance:** XGBoost provides insights into feature importance, aiding in understanding the model's decision-making process.\n",
    "\n",
    "5. **Built-in Cross-Validation:** XGBoost includes a built-in cross-validation function for model assessment and hyperparameter tuning.\n",
    "\n",
    "6. **Parallel and Distributed Computing:** XGBoost supports parallel and distributed computing to accelerate training on large datasets.\n",
    "\n",
    "7. **Pruning:** XGBoost employs a technique called \"pruning\" to remove splits that lead to negative gains during tree growth, improving efficiency.\n",
    "\n",
    "**XGBoost Algorithm:**\n",
    "The XGBoost algorithm, like gradient boosting, consists of iterative steps to build an ensemble of decision trees. The main steps are as follows:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Start with a constant prediction (often the mean of the target variable for regression) for all instances.\n",
    "   - Compute the initial gradient (negative gradient of the loss function) for each instance.\n",
    "\n",
    "2. **Boosting Iterations:**\n",
    "   - For each boosting iteration (b = 1 to B), perform these steps:\n",
    "\n",
    "     a. **Construct a Decision Tree:**\n",
    "        - Fit a decision tree to the negative gradient values, incorporating regularization terms.\n",
    "        - Prune the tree using depth and gain-based criteria.\n",
    "\n",
    "     b. **Update the Model:**\n",
    "        - Compute the prediction from the new tree and scale it by a learning rate (α).\n",
    "        - Update the model's prediction by adding the scaled tree prediction.\n",
    "\n",
    "     c. **Update the Gradient:**\n",
    "        - Compute the new gradient using the residuals (negative gradient) from the previous iteration.\n",
    "\n",
    "3. **Final Prediction:**\n",
    "   - The final prediction is the sum of predictions from all individual trees.\n",
    "\n",
    "**XGBoost Implementation:**\n",
    "XGBoost is implemented as a Python package with wrappers for various programming languages. The Python API allows you to create instances of `xgboost.XGBRegressor` for regression tasks and `xgboost.XGBClassifier` for classification tasks. These classes provide a wide range of hyperparameters for fine-tuning, such as learning rate, maximum depth, and regularization terms.\n",
    "\n",
    "XGBoost's flexibility, scalability, and performance make it a popular choice for many machine learning projects, especially when working with structured data. Its advanced features and optimizations contribute to its effectiveness in achieving high predictive accuracy. For comprehensive details, you can refer to the official XGBoost documentation and resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d7110",
   "metadata": {
    "id": "181d7110"
   },
   "source": [
    "<font color='Blue'><b>Example</b></font>. Consider the Auto MPG dataset retrieved from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/dataset/9/auto+mpg). We will now apply these concepts by actively engaging with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oV7BRgAB-GB0",
   "metadata": {
    "id": "oV7BRgAB-GB0"
   },
   "outputs": [],
   "source": [
    "# Download the zip file using wget\n",
    "!wget -N \"http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\"\n",
    "\n",
    "# Unzip the downloaded zip file\n",
    "!unzip -o auto+mpg.zip auto-mpg.data\n",
    "\n",
    "# Remove the downloaded zip file after extraction\n",
    "!rm -r auto+mpg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94529d3",
   "metadata": {
    "id": "c94529d3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# You can download the dataset from: http://archive.ics.uci.edu/static/public/9/auto+mpg.zip\n",
    "\n",
    "# Define column names based on the dataset's description\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model_Year', 'Origin', 'Car_Name']\n",
    "\n",
    "# Read the dataset with column names, treating '?' as missing values, and remove rows with missing values\n",
    "auto_mpg_df = pd.read_csv('auto-mpg.data', names=column_names,\n",
    "                          na_values='?', delim_whitespace=True).dropna().reset_index(drop = True)\n",
    "\n",
    "# Remove the 'Car_Name' column from the DataFrame\n",
    "auto_mpg_df = auto_mpg_df.drop(columns=['Car_Name'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "display(auto_mpg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9212a",
   "metadata": {
    "id": "07c9212a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# Extract the features (X) and target variable (y)\n",
    "X = auto_mpg_df.drop(columns=['MPG'])\n",
    "y = np.log(auto_mpg_df.MPG.values)  # Take the natural logarithm of the MPG values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "set_size_df = pd.DataFrame({'Size': [len(X_train), len(X_test)]}, index = ['Train', 'Test'])\n",
    "display(set_size_df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02876fbc",
   "metadata": {
    "id": "02876fbc"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "try:\n",
    "  import sklearnex\n",
    "except ImportError:\n",
    "  !pip install pip install scikit-learn-intelex\n",
    "  import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "plt.style.use('https://raw.githubusercontent.com/HatefDastour/ENGG_680/main/Files/mystyle.mplstyle')\n",
    "\n",
    "# Create a figure and subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "feature_set_labels = ['Using sklearn GradientBoostingRegressor', 'Using xgboost.XGBRegressor']\n",
    "\n",
    "# Loop through different regressors\n",
    "for i, regressor_class in enumerate([GradientBoostingRegressor, xgb.XGBRegressor]):\n",
    "    # Create a regressor\n",
    "    reg = regressor_class(random_state = random_state)\n",
    "    reg.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)\n",
    "\n",
    "    # Create scatter plot and a diagonal reference line\n",
    "    scatter = ax[i].scatter(y_pred, y_test, label='medv', facecolors='SkyBlue', edgecolors='MidnightBlue', alpha=0.8)\n",
    "    line = ax[i].plot([0, 1], [0, 1], '--k', lw=2, transform=ax[i].transAxes)\n",
    "\n",
    "    # Set title and labels for the current subplot\n",
    "    title = ax[i].set(title=feature_set_labels[i], xlabel='Predicted Values', ylabel='Actual Values')\n",
    "\n",
    "    # Calculate and display Mean Squared Error (MSE) with a background color\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    print(f'{feature_set_labels[i]}: MSE = {mse:.4f}')\n",
    "    text = ax[i].text(0.64, 0.04, f'MSE = {mse:.4f}',\n",
    "                      transform=ax[i].transAxes, fontsize=12, weight='bold',\n",
    "                      bbox=dict(facecolor='Whitesmoke', alpha=0.7))  # Add background color\n",
    "\n",
    "    # Set an equal aspect ratio for the subplots\n",
    "    ax[i].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# Adjust the layout and display the plots\n",
    "plt.tight_layout()\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8ccd7",
   "metadata": {
    "id": "5fa8ccd7"
   },
   "source": [
    "\n",
    "---\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "\n",
    "\"Scikit-learn extensions\" or \"sklearnex\" refers to additional modules or libraries that build upon the scikit-learn library, which is a machine learning library for Python. These extensions typically provide extra functionality, new algorithms, or improved features to enhance the capabilities of scikit-learn in various ways. The term \"sklearnex\" may encompass a range of third-party contributions aimed at extending and complementing the existing scikit-learn ecosystem.\n",
    "\n",
    "The function `sklearnex.patch_sklearn()` is a method within the scikit-learn extensions framework. Its primary purpose is to patch or modify the behavior of the scikit-learn library by incorporating additional functionalities or improvements provided by sklearnex.\n",
    "\n",
    "This function is typically employed to seamlessly integrate the extensions into the scikit-learn library, ensuring that the enhanced features or modifications become part of the standard scikit-learn functionality. By invoking `sklearnex.patch_sklearn()`, users can apply the necessary adjustments to the scikit-learn library, enabling the utilization of extended capabilities offered by the sklearnex framework within their machine learning workflows. Additional information can be found by referring to the documentation available [here](https://github.com/intel/scikit-learn-intelex).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a7299",
   "metadata": {
    "id": "c88a7299"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "try:\n",
    "  import sklearnex\n",
    "except ImportError:\n",
    "  !pip install pip install scikit-learn-intelex\n",
    "  import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "# Initialize and train the models\n",
    "# Initialize the GradientBoostingRegressor model\n",
    "random_state = 0\n",
    "reg_gb = GradientBoostingRegressor(random_state= random_state)\n",
    "reg_gb.fit(X_train, y_train)\n",
    "\n",
    "# Initialize the XGBRegressor model\n",
    "reg_xgb = xgb.XGBRegressor(random_state = random_state)\n",
    "reg_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Create DataFrames with feature importances\n",
    "# Create a DataFrame for feature importances using GradientBoostingRegressor\n",
    "importance_gb = pd.DataFrame({'Importance': reg_gb.feature_importances_ * 100}, index=X.columns)\n",
    "\n",
    "# Create a DataFrame for feature importances using XGBRegressor\n",
    "importance_xgb = pd.DataFrame({'Importance': reg_xgb.feature_importances_ * 100}, index=X.columns)\n",
    "\n",
    "# Create subplots\n",
    "# Create a figure with two vertically stacked subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Plot feature importance for GradientBoostingRegressor\n",
    "# Create a bar plot for feature importance in the first subplot\n",
    "axes[0].bar(importance_gb.index, importance_gb.Importance, color='#99f599', edgecolor='#006400', hatch=\"///\")\n",
    "axes[0].set_title('Feature Importance:\\nGradient Boosting Regressor', fontsize=13, weight='bold', color='DarkSlateGray')\n",
    "axes[0].set_ylim([0, 50])\n",
    "\n",
    "# Plot feature importance for XGBRegressor\n",
    "# Create a bar plot for feature importance in the second subplot\n",
    "axes[1].bar(importance_xgb.index, importance_xgb.Importance, color='#e9aaaa', edgecolor='#8B0000', hatch=\"\\\\\\\\\")\n",
    "axes[1].set_title('Feature Importance:\\nXGBoost Regressor', fontsize=12, weight='bold', color='DarkSlateGray')\n",
    "\n",
    "# Common settings for both subplots\n",
    "# Iterate through the axes and apply common settings\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Variable', weight='bold', color='MidnightBlue')\n",
    "    ax.tick_params(axis='x', rotation=45, color='DimGray')\n",
    "    ax.tick_params(axis='y', color='DimGray')\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "    ax.spines[['bottom', 'left']].set_color('DimGray')\n",
    "    ax.set_ylabel('Importance (%)', weight='bold', color='MidnightBlue')\n",
    "\n",
    "# Remove the ylabel for the right plot\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616618a",
   "metadata": {
    "id": "d616618a"
   },
   "outputs": [],
   "source": [
    "# Merge importance_gb and importance_xgb DataFrames\n",
    "merged_importance = pd.merge(importance_gb, importance_xgb, left_index=True, right_index=True,\n",
    "                              suffixes=('_GB', '_XGB'))\n",
    "\n",
    "# Display the merged DataFrame with background gradient\n",
    "display(merged_importance.style.format(precision=2).background_gradient(cmap='YlGnBu', axis=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b4f9f",
   "metadata": {
    "id": "f41b4f9f"
   },
   "source": [
    "---\n",
    "<font color='Red'><b>Note:</b></font>\n",
    "\n",
    "It's important to note that feature importance does not necessarily imply causation or directionality. Instead, it indicates the relative importance of each feature in predicting the target variable. The feature importance values can be used to identify the most relevant features for the task at hand and to gain insights into the underlying data. However, it's important to keep in mind that the feature importance values are based on the specific model and dataset used, and may not generalize to other models or datasets. Therefore, it's recommended to perform a thorough analysis of the models and data before drawing any conclusions based on the feature importance values [Géron, 2022].\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30679508",
   "metadata": {
    "id": "30679508"
   },
   "source": [
    "<font color='Blue'><b>Example:</b></font> The [digits dataset](https://archive.ics.uci.edu/dataset/80/optical+recognition+of+handwritten+digits) comprises a collection of 8x8 pixel images depicting various numerical digits. Within the dataset, the 'images' attribute holds 8x8 arrays representing grayscale values corresponding to each image. For illustrative purposes, we will leverage these arrays to visualize the initial four images. Notably, the 'target' attribute in the dataset retains information about the numerical digit portrayed by each image. This informative detail is seamlessly incorporated into the titles of the four plots showcased below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d791145e",
   "metadata": {
    "id": "d791145e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "# Create a dictionary to map labels to their word representations\n",
    "Labels_dict = dict(zip(np.arange(10), ['Zero','One','Two','Three','Four','Five','Six','Seven','Eight','Nine']))\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(9.5, 4.5), subplot_kw={'xticks': [], 'yticks': []})\n",
    "ax = ax.ravel()\n",
    "\n",
    "# Set custom font for titles\n",
    "font = FontProperties()\n",
    "font.set_family('fantasy')\n",
    "\n",
    "# Load the digits dataset\n",
    "digits = load_digits()\n",
    "\n",
    "# Iterate over images and labels\n",
    "for ax, (image, label) in zip(ax, zip(digits.images, digits.target)):\n",
    "    ax.imshow(image, cmap='Greys', interpolation='nearest')\n",
    "    ax.set_title(f'{Labels_dict[label]}', fontproperties=font, fontsize= 18)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "# Normalize images and set target labels\n",
    "X = digits.images / 256.0\n",
    "y = digits.target\n",
    "\n",
    "# Adjust layout and display the plots\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319acaa5",
   "metadata": {
    "id": "319acaa5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _dist_plot(ax, y, Labels_dict=Labels_dict, CM=plt.cm.tab20c.colors, title=False):\n",
    "    \"\"\"\n",
    "    Generate a pie chart illustrating the distribution of categories.\n",
    "\n",
    "    Parameters:\n",
    "    - ax: Axes object to plot on.\n",
    "    - y: Input data for which the distribution is to be visualized.\n",
    "    - Labels_dict: Dictionary mapping category indices to labels.\n",
    "    - CM: Color map for the pie chart.\n",
    "    - title: Title for the plot. Set to False to omit.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Prepare data for the pie chart\n",
    "    df = pd.Series(y).value_counts().to_frame('Count')\n",
    "\n",
    "    # Create the pie chart\n",
    "    wedges, texts, autotexts = ax.pie(df['Count'],\n",
    "                                      labels=[Labels_dict[i] for i in df.index],\n",
    "                                      autopct='%1.1f%%', startangle=140,\n",
    "                                      colors=CM,\n",
    "                                      explode=[0, 0, 0, 0, 0, 0.1, 0, 0, 0, 0],\n",
    "                                      shadow=True, wedgeprops={'edgecolor': 'whitesmoke'})\n",
    "    # Set title and ensure equal aspect ratio for a circular pie chart\n",
    "    if title:\n",
    "        _ = ax.set_title(title, fontsize=16, weight='bold')\n",
    "    _ = ax.axis('equal')\n",
    "\n",
    "    # Highlight the labels with annotations\n",
    "    for text, autotext in zip(texts, autotexts):\n",
    "        text.set_fontsize(12)\n",
    "        text.set_fontweight('bold')\n",
    "        autotext.set_fontsize(12)\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(5.5, 5.5))\n",
    "_dist_plot(ax, y, title='Distribution of Categories')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe1919",
   "metadata": {
    "id": "78fe1919"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(9, 5.5))\n",
    "_dist_plot(ax[0], y_train, CM = plt.cm.Pastel1.colors)\n",
    "_ = ax[0].set_title('Train Set', fontsize=12, weight='bold', color='Green')\n",
    "_dist_plot(ax[1], y_test, CM = plt.cm.Pastel2.colors)\n",
    "_ = ax[1].set_title('Test Set', fontsize=12, weight='bold', color='Blue')\n",
    "_ = fig.suptitle('Distribution of Categories', fontsize=16, weight='bold')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ec9ed",
   "metadata": {
    "id": "297ec9ed"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "try:\n",
    "  import sklearnex\n",
    "except ImportError:\n",
    "  !pip install pip install scikit-learn-intelex\n",
    "  import sklearnex\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "sklearnex.patch_sklearn()\n",
    "\n",
    "# ------------------------------\n",
    "# Gradient Boosting Classifier\n",
    "# ------------------------------\n",
    "\n",
    "# Create a GradientBoostingClassifier\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "gb_classifier.fit(X_train.reshape(-1, 64), y_train)\n",
    "\n",
    "# Evaluate the classifier on the test data\n",
    "gb_accuracy = gb_classifier.score(X_test.reshape(-1, 64), y_test)\n",
    "print(f\"Gradient Boosting Classifier Accuracy: {gb_accuracy:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# XGBoost Classifier\n",
    "# ------------------------------\n",
    "\n",
    "# Create an XGBClassifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "xgb_classifier.fit(X_train.reshape(-1, 64), y_train)\n",
    "\n",
    "# Predict using the classifier\n",
    "y_pred = xgb_classifier.predict(X_test.reshape(-1, 64))\n",
    "\n",
    "# Calculate accuracy using accuracy_score\n",
    "xgb_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGB Classifier Accuracy: {xgb_accuracy:.4f}\")\n",
    "sklearnex.unpatch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa21041",
   "metadata": {
    "id": "ffa21041",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Update the Matplotlib settings using the dictionary\n",
    "plt.rcParams.update({'axes.grid.axis': 'y'})\n",
    "\n",
    "# Choose indices from the test set to visualize\n",
    "test_indices = np.random.randint(1, len(y_test) + 1, size=12)\n",
    "\n",
    "# Create subplots grid\n",
    "num_rows = len(test_indices)\n",
    "fig, axs = plt.subplots(num_rows, 2, figsize=(9, 2 * num_rows))\n",
    "\n",
    "for i, test_index in enumerate(test_indices):\n",
    "    selected_sample = X_test[test_index]\n",
    "    true_label = y_test[test_index]\n",
    "\n",
    "    # Predict probabilities using the GradientBoostingClassifier\n",
    "    probs_gb = gb_classifier.predict_proba(selected_sample.reshape(1, -1))\n",
    "\n",
    "    # Predict probabilities using the XGBClassifier\n",
    "    probs_xgb = xgb_classifier.predict_proba(selected_sample.reshape(1, -1))\n",
    "\n",
    "    # Visualize the selected sample on the left side\n",
    "    axs[i, 0].imshow(selected_sample, cmap='Greys', interpolation='nearest')\n",
    "    axs[i, 0].set_title(f\"True Label: {true_label}\")\n",
    "    axs[i, 0].set(xticks=[], yticks=[])\n",
    "\n",
    "    # Plot probabilities on the right side\n",
    "    bar_width = 0.4  # Adjust this value for spacing\n",
    "    axs[i, 1].bar(np.arange(10) - bar_width/2, probs_gb[0], color = 'RoyalBlue',\n",
    "                  width=bar_width, label='Gradient Boosting', alpha=0.7, ec = 'k', lw = 0.5)\n",
    "    axs[i, 1].bar(np.arange(10) + bar_width/2, probs_xgb[0], color = 'OrangeRed',\n",
    "                  width=bar_width, label='XGBoost', alpha=0.7, ec = 'k', lw = 0.5)\n",
    "    axs[i, 1].set(xticks=range(10), xticklabels=[str(i) for i in range(10)],\n",
    "                  ylim=[0, 1], title = \"Class Probabilities\")\n",
    "    axs[i, 1].legend(loc='upper right', bbox_to_anchor=(1.55, 1))\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
